#!/usr/bin/env python3
"""
simple_eval.py - Enhanced evaluation script for Velora tasks with f2p/p2p support

This script:
1. Takes an output.jsonl from trajectory generation
2. Extracts the git patch
3. Applies it to the Docker container
4. Runs the FAIL_TO_PASS and PASS_TO_PASS tests
5. Generates report files with f2p_status and p2p_status

Usage:
    python simple_eval.py --input-file output.jsonl --dataset task.jsonl
"""

import argparse
import json
import os
import re
import subprocess
import sys
import time
from pathlib import Path


def load_jsonl(filepath):
    """Load a JSONL file and return list of dicts."""
    data = []
    with open(filepath, 'r') as f:
        for line in f:
            line = line.strip()
            if line:
                data.append(json.loads(line))
    return data


def extract_patch(output_data):
    """Extract git patch from output.jsonl entry."""
    patch = ''
    
    # Try test_result.git_patch first
    if 'test_result' in output_data and output_data['test_result']:
        patch = output_data['test_result'].get('git_patch', '')
    
    # Try model_patch as fallback
    if not patch and 'model_patch' in output_data:
        patch = output_data.get('model_patch', '')
    
    # Clean up patch
    if patch:
        patch = patch.replace('\r\n', '\n')
        # Find first diff line
        lines = patch.split('\n')
        for i, line in enumerate(lines):
            if line.startswith('diff --git'):
                patch = '\n'.join(lines[i:])
                break
        patch = patch.rstrip() + '\n'
    
    return patch


def parse_pytest_output(output: str, test_names: list) -> dict:
    """
    Parse pytest output to get status of each test.
    
    Returns dict mapping test_name -> 'PASSED' | 'FAILED' | 'ERROR' | 'SKIPPED'
    """
    status_map = {}
    
    for test_name in test_names:
        # Extract just the test function name for matching
        if '::' in test_name:
            test_func = test_name.split('::')[-1]
        else:
            test_func = test_name
        
        # Check for various pytest output patterns
        # Pattern 1: "test_name PASSED" or "PASSED test_name"
        if re.search(rf'{re.escape(test_func)}.*PASSED|PASSED.*{re.escape(test_func)}', output, re.IGNORECASE):
            status_map[test_name] = 'PASSED'
        # Pattern 2: "test_name FAILED" or "FAILED test_name"
        elif re.search(rf'{re.escape(test_func)}.*FAILED|FAILED.*{re.escape(test_func)}', output, re.IGNORECASE):
            status_map[test_name] = 'FAILED'
        # Pattern 3: pytest-style "::test_name PASSED"
        elif re.search(rf'::{re.escape(test_func)}[^\n]*PASSED', output):
            status_map[test_name] = 'PASSED'
        elif re.search(rf'::{re.escape(test_func)}[^\n]*FAILED', output):
            status_map[test_name] = 'FAILED'
        # Pattern 4: Check for "1 passed" at end (if only one test)
        elif len(test_names) == 1 and re.search(r'1 passed', output):
            status_map[test_name] = 'PASSED'
        elif len(test_names) == 1 and re.search(r'1 failed', output):
            status_map[test_name] = 'FAILED'
        # Pattern 5: Check exit code indicator in output
        elif 'error' in output.lower() and test_func in output:
            status_map[test_name] = 'ERROR'
        else:
            # Could not determine status
            status_map[test_name] = 'UNKNOWN'
    
    return status_map


def run_docker_eval(instance_id, image_name, patch, fail_to_pass, pass_to_pass, output_dir, timeout=600):
    """
    Run evaluation in Docker container with proper f2p/p2p tracking.
    
    Returns dict with:
        - resolved: bool
        - failed_apply_patch: bool
        - error_eval: bool
        - f2p_status: dict of test_name -> status
        - p2p_status: dict of test_name -> status
        - test_output: str
    """
    result = {
        'resolved': False,
        'failed_apply_patch': False,
        'error_eval': False,
        'test_output': '',
        'apply_output': '',
        'f2p_status': {},
        'p2p_status': {},
    }
    
    os.makedirs(output_dir, exist_ok=True)
    
    # Save patch file
    patch_file = os.path.join(output_dir, 'patch.diff')
    with open(patch_file, 'w') as f:
        f.write(patch)
    
    if not patch.strip():
        result['error_eval'] = True
        result['test_output'] = 'Empty patch - no changes generated by model'
        # Mark all tests as not run
        for test in fail_to_pass:
            result['f2p_status'][test] = 'NOT_RUN'
        for test in pass_to_pass:
            result['p2p_status'][test] = 'NOT_RUN'
        return result
    
    container_name = f"eval_{instance_id}_{int(time.time())}"
    
    try:
        # Start container
        print(f"Starting container from image: {image_name}")
        start_cmd = [
            'docker', 'run', '-d',
            '--name', container_name,
            '--entrypoint', '/bin/bash',
            image_name,
            '-c', 'sleep infinity'
        ]
        start_result = subprocess.run(start_cmd, capture_output=True, text=True)
        if start_result.returncode != 0:
            result['error_eval'] = True
            result['test_output'] = f"Failed to start container: {start_result.stderr}"
            return result
        
        # Copy patch to container
        subprocess.run(
            ['docker', 'cp', patch_file, f'{container_name}:/tmp/patch.diff'],
            check=True, capture_output=True
        )
        
        # Find repo directory
        find_repo_cmd = ['docker', 'exec', container_name, 'bash', '-c',
                        'if [ -d /app/expensify ]; then echo /app/expensify; elif [ -d /app/repo ]; then echo /app/repo; elif [ -d /testbed ]; then echo /testbed; elif [ -d /workspace ]; then echo /workspace; else find /app -maxdepth 1 -type d 2>/dev/null | head -2 | tail -1; fi']
        repo_result = subprocess.run(find_repo_cmd, capture_output=True, text=True)
        repo_dir = repo_result.stdout.strip() or '/app'
        print(f"Using repo directory: {repo_dir}")
        
        # Apply patch
        apply_cmd = f'''
cd {repo_dir} && git config --global --add safe.directory {repo_dir} && git apply -v /tmp/patch.diff 2>&1
'''
        apply_result = subprocess.run(
            ['docker', 'exec', container_name, 'bash', '-c', apply_cmd],
            capture_output=True, text=True, timeout=60
        )
        result['apply_output'] = apply_result.stdout + apply_result.stderr
        
        if apply_result.returncode != 0:
            print(f"git apply failed, trying patch command...")
            patch_cmd = f'''
cd {repo_dir} && patch --batch --fuzz=5 -p1 -i /tmp/patch.diff 2>&1
'''
            patch_result = subprocess.run(
                ['docker', 'exec', container_name, 'bash', '-c', patch_cmd],
                capture_output=True, text=True, timeout=60
            )
            result['apply_output'] += '\n--- Trying patch command ---\n' + patch_result.stdout + patch_result.stderr
            
            if patch_result.returncode != 0:
                result['failed_apply_patch'] = True
                result['test_output'] = f"Failed to apply patch:\n{result['apply_output']}"
                # Mark all tests as not run due to patch failure
                for test in fail_to_pass:
                    result['f2p_status'][test] = 'PATCH_FAILED'
                for test in pass_to_pass:
                    result['p2p_status'][test] = 'PATCH_FAILED'
                return result
        
        print("Patch applied successfully")
        
        # Run FAIL_TO_PASS tests
        all_f2p_passed = True
        if fail_to_pass:
            print(f"Running {len(fail_to_pass)} FAIL_TO_PASS tests...")
            f2p_output = ""
            
            for test in fail_to_pass:
                print(f"  Running: {test}")
                # Run pytest for this specific test
                test_cmd = f'cd {repo_dir} && python -m pytest {test} -v --tb=short 2>&1 || true'
                try:
                    test_result = subprocess.run(
                        ['docker', 'exec', container_name, 'bash', '-c', test_cmd],
                        capture_output=True, text=True, timeout=timeout
                    )
                    test_output = test_result.stdout + test_result.stderr
                    f2p_output += f"\n=== {test} ===\n{test_output}\n"
                    
                    # Parse the result
                    if 'passed' in test_output.lower() and 'failed' not in test_output.lower():
                        result['f2p_status'][test] = 'PASSED'
                        print(f"    Result: PASSED")
                    elif 'failed' in test_output.lower() or 'error' in test_output.lower():
                        result['f2p_status'][test] = 'FAILED'
                        all_f2p_passed = False
                        print(f"    Result: FAILED")
                    elif 'no tests ran' in test_output.lower() or 'collected 0 items' in test_output.lower():
                        result['f2p_status'][test] = 'NOT_FOUND'
                        all_f2p_passed = False
                        print(f"    Result: NOT_FOUND")
                    else:
                        # Check exit code
                        if test_result.returncode == 0:
                            result['f2p_status'][test] = 'PASSED'
                            print(f"    Result: PASSED (exit code 0)")
                        else:
                            result['f2p_status'][test] = 'FAILED'
                            all_f2p_passed = False
                            print(f"    Result: FAILED (exit code {test_result.returncode})")
                            
                except subprocess.TimeoutExpired:
                    result['f2p_status'][test] = 'TIMEOUT'
                    all_f2p_passed = False
                    f2p_output += f"\n=== {test} ===\nTIMEOUT after {timeout}s\n"
                    print(f"    Result: TIMEOUT")
            
            result['test_output'] += f"\n=== FAIL_TO_PASS Tests ===\n{f2p_output}"
        
        # Run PASS_TO_PASS tests
        all_p2p_passed = True
        if pass_to_pass:
            print(f"Running {len(pass_to_pass)} PASS_TO_PASS tests...")
            p2p_output = ""
            
            for test in pass_to_pass:
                print(f"  Running: {test}")
                test_cmd = f'cd {repo_dir} && python -m pytest {test} -v --tb=short 2>&1 || true'
                try:
                    test_result = subprocess.run(
                        ['docker', 'exec', container_name, 'bash', '-c', test_cmd],
                        capture_output=True, text=True, timeout=timeout
                    )
                    test_output = test_result.stdout + test_result.stderr
                    p2p_output += f"\n=== {test} ===\n{test_output}\n"
                    
                    # Parse the result
                    if 'passed' in test_output.lower() and 'failed' not in test_output.lower():
                        result['p2p_status'][test] = 'PASSED'
                        print(f"    Result: PASSED")
                    elif 'failed' in test_output.lower() or 'error' in test_output.lower():
                        result['p2p_status'][test] = 'FAILED'
                        all_p2p_passed = False
                        print(f"    Result: FAILED")
                    else:
                        if test_result.returncode == 0:
                            result['p2p_status'][test] = 'PASSED'
                            print(f"    Result: PASSED (exit code 0)")
                        else:
                            result['p2p_status'][test] = 'FAILED'
                            all_p2p_passed = False
                            print(f"    Result: FAILED (exit code {test_result.returncode})")
                            
                except subprocess.TimeoutExpired:
                    result['p2p_status'][test] = 'TIMEOUT'
                    all_p2p_passed = False
                    p2p_output += f"\n=== {test} ===\nTIMEOUT after {timeout}s\n"
                    print(f"    Result: TIMEOUT")
            
            result['test_output'] += f"\n=== PASS_TO_PASS Tests ===\n{p2p_output}"
        
        # Determine if resolved: all FAIL_TO_PASS must pass, all PASS_TO_PASS must pass
        if fail_to_pass:
            f2p_all_passed = all(status == 'PASSED' for status in result['f2p_status'].values())
        else:
            f2p_all_passed = True
            
        if pass_to_pass:
            p2p_all_passed = all(status == 'PASSED' for status in result['p2p_status'].values())
        else:
            p2p_all_passed = True
        
        result['resolved'] = f2p_all_passed and p2p_all_passed
        
        print(f"\nEvaluation Summary:")
        print(f"  FAIL_TO_PASS: {sum(1 for s in result['f2p_status'].values() if s == 'PASSED')}/{len(result['f2p_status'])} passed")
        print(f"  PASS_TO_PASS: {sum(1 for s in result['p2p_status'].values() if s == 'PASSED')}/{len(result['p2p_status'])} passed")
        print(f"  Resolved: {result['resolved']}")
            
    except subprocess.TimeoutExpired:
        result['error_eval'] = True
        result['test_output'] = f'Overall timeout after {timeout}s'
    except Exception as e:
        result['error_eval'] = True
        result['test_output'] = f'Error: {str(e)}'
    finally:
        # Cleanup container
        try:
            subprocess.run(['docker', 'stop', container_name], capture_output=True, timeout=30)
            subprocess.run(['docker', 'rm', container_name], capture_output=True, timeout=30)
        except:
            pass
    
    return result


def main():
    parser = argparse.ArgumentParser(description='Enhanced evaluation for Velora tasks with f2p/p2p support')
    parser.add_argument('--input-file', required=True, help='Path to output.jsonl from trajectory generation')
    parser.add_argument('--dataset', required=True, help='Path to dataset.jsonl with task info')
    parser.add_argument('--output-dir', default=None, help='Output directory for results (default: same as input)')
    parser.add_argument('--timeout', type=int, default=600, help='Test timeout in seconds per test')
    args = parser.parse_args()
    
    # Load files
    print(f"Loading output file: {args.input_file}")
    outputs = load_jsonl(args.input_file)
    
    print(f"Loading dataset: {args.dataset}")
    dataset = load_jsonl(args.dataset)
    
    # Create instance lookup
    dataset_map = {d['instance_id']: d for d in dataset}
    
    # Determine output directory
    if args.output_dir:
        base_output_dir = args.output_dir
    else:
        base_output_dir = os.path.dirname(args.input_file)
    
    eval_outputs_dir = os.path.join(base_output_dir, 'eval_outputs')
    os.makedirs(eval_outputs_dir, exist_ok=True)
    
    # Process each output
    results = []
    for output in outputs:
        instance_id = str(output.get('instance_id', 'unknown'))
        print(f"\n{'='*60}")
        print(f"Evaluating instance: {instance_id}")
        print('='*60)
        
        # Get dataset info
        if instance_id not in dataset_map:
            print(f"WARNING: Instance {instance_id} not found in dataset")
            continue
        
        task = dataset_map[instance_id]
        
        # Determine Docker image
        # Use image_storage_uri if available (for SWE-Lancer/custom images), otherwise use default
        storage_uri = task.get('image_storage_uri', '')
        if storage_uri and not storage_uri.startswith('s3://'):
            # Use direct docker image reference if not an S3 path
            image_name = storage_uri
            print(f"Using image_storage_uri from dataset: {image_name}")
        else:
            # Use default mswebench format for standard tasks
            image_name = f"mswebench/sweb.eval.x86_64.{instance_id}:latest"
            print(f"Using default image format: {image_name}")
        
        # Log the storage URI for reference
        print(f"Image storage URI: {storage_uri if storage_uri else 'N/A'}")
        
        # Extract patch
        patch = extract_patch(output)
        if not patch.strip():
            print("WARNING: No patch found in output")
        
        # Get FAIL_TO_PASS and PASS_TO_PASS tests
        fail_to_pass = task.get('FAIL_TO_PASS', [])
        if isinstance(fail_to_pass, str):
            fail_to_pass = [fail_to_pass]
        
        pass_to_pass = task.get('PASS_TO_PASS', [])
        if isinstance(pass_to_pass, str):
            pass_to_pass = [pass_to_pass]
        
        print(f"FAIL_TO_PASS tests: {fail_to_pass}")
        print(f"PASS_TO_PASS tests: {pass_to_pass}")
        
        # Create instance output directory
        instance_output_dir = os.path.join(eval_outputs_dir, instance_id)
        os.makedirs(instance_output_dir, exist_ok=True)
        
        # Run evaluation
        eval_result = run_docker_eval(
            instance_id=instance_id,
            image_name=image_name,
            patch=patch,
            fail_to_pass=fail_to_pass,
            pass_to_pass=pass_to_pass,
            output_dir=instance_output_dir,
            timeout=args.timeout
        )
        
        # Save results
        # patch.diff
        with open(os.path.join(instance_output_dir, 'patch.diff'), 'w') as f:
            f.write(patch)
        
        # test_output.txt
        with open(os.path.join(instance_output_dir, 'test_output.txt'), 'w') as f:
            f.write(eval_result['test_output'])
        
        # report.json - now includes f2p_status and p2p_status
        report = {
            'instance_id': instance_id,
            'resolved': eval_result['resolved'],
            'failed_apply_patch': eval_result['failed_apply_patch'],
            'error_eval': eval_result['error_eval'],
            'empty_generation': not bool(patch.strip()),
            'f2p_status': eval_result['f2p_status'],
            'p2p_status': eval_result['p2p_status'],
        }
        with open(os.path.join(instance_output_dir, 'report.json'), 'w') as f:
            json.dump(report, f, indent=2)
        
        # run_instance.log
        log_content = f"""Instance: {instance_id}
Image: {image_name}
FAIL_TO_PASS: {fail_to_pass}
PASS_TO_PASS: {pass_to_pass}

=== Apply Patch Output ===
{eval_result['apply_output']}

=== Test Output ===
{eval_result['test_output']}

=== f2p_status ===
{json.dumps(eval_result['f2p_status'], indent=2)}

=== p2p_status ===
{json.dumps(eval_result['p2p_status'], indent=2)}

=== Result ===
Resolved: {eval_result['resolved']}
Failed Apply Patch: {eval_result['failed_apply_patch']}
Error in Eval: {eval_result['error_eval']}
"""
        with open(os.path.join(instance_output_dir, 'run_instance.log'), 'w') as f:
            f.write(log_content)
        
        # eval.sh (the test commands)
        eval_sh = "#!/bin/bash\n# FAIL_TO_PASS tests:\n"
        for test in fail_to_pass:
            eval_sh += f"python -m pytest {test} -v\n"
        eval_sh += "\n# PASS_TO_PASS tests:\n"
        for test in pass_to_pass:
            eval_sh += f"python -m pytest {test} -v\n"
        with open(os.path.join(instance_output_dir, 'eval.sh'), 'w') as f:
            f.write(eval_sh)
        
        results.append(report)
        
        print(f"\nResult: {'RESOLVED' if eval_result['resolved'] else 'NOT RESOLVED'}")
        print(f"Output saved to: {instance_output_dir}")
    
    # Print summary
    print(f"\n{'='*60}")
    print("EVALUATION SUMMARY")
    print('='*60)
    total = len(results)
    resolved = sum(1 for r in results if r['resolved'])
    failed_apply = sum(1 for r in results if r['failed_apply_patch'])
    error = sum(1 for r in results if r['error_eval'])
    empty = sum(1 for r in results if r['empty_generation'])
    
    print(f"Total: {total}")
    print(f"Resolved: {resolved} ({100*resolved/total if total else 0:.1f}%)")
    print(f"Failed Apply Patch: {failed_apply}")
    print(f"Error in Eval: {error}")
    print(f"Empty Generation: {empty}")
    
    # Aggregate f2p/p2p stats
    total_f2p = sum(len(r['f2p_status']) for r in results)
    passed_f2p = sum(sum(1 for s in r['f2p_status'].values() if s == 'PASSED') for r in results)
    total_p2p = sum(len(r['p2p_status']) for r in results)
    passed_p2p = sum(sum(1 for s in r['p2p_status'].values() if s == 'PASSED') for r in results)
    
    print(f"\nTest Statistics:")
    print(f"  FAIL_TO_PASS: {passed_f2p}/{total_f2p} passed")
    print(f"  PASS_TO_PASS: {passed_p2p}/{total_p2p} passed")
    
    print(f"\nResults saved to: {eval_outputs_dir}")
    
    # Save summary
    summary = {
        'total': total,
        'resolved': resolved,
        'failed_apply_patch': failed_apply,
        'error_eval': error,
        'empty_generation': empty,
        'f2p_total': total_f2p,
        'f2p_passed': passed_f2p,
        'p2p_total': total_p2p,
        'p2p_passed': passed_p2p,
        'results': results,
    }
    with open(os.path.join(base_output_dir, 'eval_summary.json'), 'w') as f:
        json.dump(summary, f, indent=2)


if __name__ == '__main__':
    main()
