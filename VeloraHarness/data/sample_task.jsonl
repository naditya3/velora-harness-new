{"instance_id": "Project-MONAI__MONAI-2436", "hints_text": "", "patch": "diff --git a/monai/data/dataset.py b/monai/data/dataset.py\n--- a/monai/data/dataset.py\n+++ b/monai/data/dataset.py\n@@ -130,6 +130,8 @@ class PersistentDataset(Dataset):\n     Subsequent uses of a dataset directly read pre-processed results from `cache_dir`\n     followed by applying the random dependant parts of transform processing.\n \n+    During training call `set_data()` to update input data and recompute cache content.\n+\n     Note:\n         The input data must be a list of file paths and will hash them as cache keys.\n \n@@ -173,6 +175,16 @@ def __init__(\n             if not self.cache_dir.is_dir():\n                 raise ValueError(\"cache_dir must be a directory.\")\n \n+    def set_data(self, data: Sequence):\n+        \"\"\"\n+        Set the input data and delete all the out-dated cache content.\n+\n+        \"\"\"\n+        self.data = data\n+        if self.cache_dir is not None and self.cache_dir.exists():\n+            shutil.rmtree(self.cache_dir, ignore_errors=True)\n+            self.cache_dir.mkdir(parents=True, exist_ok=True)\n+\n     def _pre_transform(self, item_transformed):\n         \"\"\"\n         Process the data from original state up to the first random element.\n@@ -404,6 +416,14 @@ def __init__(\n         self._read_env = None\n         print(f\"Accessing lmdb file: {self.db_file.absolute()}.\")\n \n+    def set_data(self, data: Sequence):\n+        \"\"\"\n+        Set the input data and delete all the out-dated cache content.\n+\n+        \"\"\"\n+        super().set_data(data=data)\n+        self._read_env = None\n+\n     def _fill_cache_start_reader(self):\n         # create cache\n         self.lmdb_kwargs[\"readonly\"] = False\n@@ -515,6 +535,9 @@ class CacheDataset(Dataset):\n     ``RandCropByPosNegLabeld`` and ``ToTensord``, as ``RandCropByPosNegLabeld`` is a randomized transform\n     and the outcome not cached.\n \n+    During training call `set_data()` to update input data and recompute cache content, note that it requires\n+    `persistent_workers=False` in the PyTorch DataLoader.\n+\n     Note:\n         `CacheDataset` executes non-random transforms and prepares cache content in the main process before\n         the first epoch, then all the subprocesses of DataLoader will read the same cache content in the main process\n@@ -555,6 +578,18 @@ def __init__(\n             self.num_workers = max(int(self.num_workers), 1)\n         self._cache: List = self._fill_cache()\n \n+    def set_data(self, data: Sequence):\n+        \"\"\"\n+        Set the input data and run deterministic transforms to generate cache content.\n+\n+        Note: should call this func after an entire epoch and must set `persisten_workers=False`\n+        in PyTorch DataLoader, because it needs to create new worker processes based on new\n+        generated cache content.\n+\n+        \"\"\"\n+        self.data = data\n+        self._cache = self._fill_cache()\n+\n     def _fill_cache(self) -> List:\n         if self.cache_num <= 0:\n             return []\n@@ -639,6 +674,9 @@ class SmartCacheDataset(Randomizable, CacheDataset):\n         3. Call `update_cache()` before every epoch to replace training items.\n         4. Call `shutdown()` when training ends.\n \n+    During training call `set_data()` to update input data and recompute cache content, note to call\n+    `shutdown()` to stop first, then update data and call `start()` to restart.\n+\n     Note:\n         This replacement will not work for below cases:\n         1. Set the `multiprocessing_context` of DataLoader to `spawn`.\n@@ -683,6 +721,7 @@ def __init__(\n             self.set_random_state(seed=seed)\n             data = copy(data)\n             self.randomize(data)\n+        self.shuffle = shuffle\n \n         super().__init__(data, transform, cache_num, cache_rate, num_init_workers, progress)\n         if self._cache is None:\n@@ -711,6 +750,22 @@ def __init__(\n \n         self._compute_data_idx()\n \n+    def set_data(self, data: Sequence):\n+        \"\"\"\n+        Set the input data and run deterministic transforms to generate cache content.\n+\n+        Note: should call `shutdown()` before calling this func.\n+\n+        \"\"\"\n+        if self.is_started():\n+            warnings.warn(\"SmartCacheDataset is not shutdown yet, shutdown it directly.\")\n+            self.shutdown()\n+\n+        if self.shuffle:\n+            data = copy(data)\n+            self.randomize(data)\n+        super().set_data(data)\n+\n     def randomize(self, data: Sequence) -> None:\n         try:\n             self.R.shuffle(data)\n@@ -798,6 +853,8 @@ def _try_shutdown(self):\n         with self._update_lock:\n             if self._replace_done:\n                 self._round = 0\n+                self._start_pos = 0\n+                self._compute_data_idx()\n                 self._replace_done = False\n                 return True\n             return False\n", "test_patch": "diff --git a/tests/test_cachedataset.py b/tests/test_cachedataset.py\n--- a/tests/test_cachedataset.py\n+++ b/tests/test_cachedataset.py\n@@ -19,7 +19,7 @@\n from parameterized import parameterized\n \n from monai.data import CacheDataset, DataLoader, PersistentDataset, SmartCacheDataset\n-from monai.transforms import Compose, LoadImaged, ThreadUnsafe, Transform\n+from monai.transforms import Compose, Lambda, LoadImaged, ThreadUnsafe, Transform\n from monai.utils import get_torch_version_tuple\n \n TEST_CASE_1 = [Compose([LoadImaged(keys=[\"image\", \"label\", \"extra\"])]), (128, 128, 128)]\n@@ -81,6 +81,31 @@ def test_shape(self, transform, expected_shape):\n             for d in data3:\n                 self.assertTupleEqual(d[\"image\"].shape, expected_shape)\n \n+    def test_set_data(self):\n+        data_list1 = list(range(10))\n+\n+        transform = Lambda(func=lambda x: np.array([x * 10]))\n+\n+        dataset = CacheDataset(\n+            data=data_list1,\n+            transform=transform,\n+            cache_rate=1.0,\n+            num_workers=4,\n+            progress=True,\n+        )\n+\n+        num_workers = 2 if sys.platform == \"linux\" else 0\n+        dataloader = DataLoader(dataset=dataset, num_workers=num_workers, batch_size=1)\n+        for i, d in enumerate(dataloader):\n+            np.testing.assert_allclose([[data_list1[i] * 10]], d)\n+\n+        # update the datalist and fill the cache content\n+        data_list2 = list(range(-10, 0))\n+        dataset.set_data(data=data_list2)\n+        # rerun with updated cache content\n+        for i, d in enumerate(dataloader):\n+            np.testing.assert_allclose([[data_list2[i] * 10]], d)\n+\n \n class _StatefulTransform(Transform, ThreadUnsafe):\n     \"\"\"\ndiff --git a/tests/test_lmdbdataset.py b/tests/test_lmdbdataset.py\n--- a/tests/test_lmdbdataset.py\n+++ b/tests/test_lmdbdataset.py\n@@ -158,25 +158,45 @@ def test_shape(self, transform, expected_shape, kwargs=None):\n             data1_postcached = dataset_postcached[0]\n             data2_postcached = dataset_postcached[1]\n \n-        if transform is None:\n-            self.assertEqual(data1_precached[\"image\"], os.path.join(tempdir, \"test_image1.nii.gz\"))\n-            self.assertEqual(data2_precached[\"label\"], os.path.join(tempdir, \"test_label2.nii.gz\"))\n-            self.assertEqual(data1_postcached[\"image\"], os.path.join(tempdir, \"test_image1.nii.gz\"))\n-            self.assertEqual(data2_postcached[\"extra\"], os.path.join(tempdir, \"test_extra2.nii.gz\"))\n-        else:\n-            self.assertTupleEqual(data1_precached[\"image\"].shape, expected_shape)\n-            self.assertTupleEqual(data1_precached[\"label\"].shape, expected_shape)\n-            self.assertTupleEqual(data1_precached[\"extra\"].shape, expected_shape)\n-            self.assertTupleEqual(data2_precached[\"image\"].shape, expected_shape)\n-            self.assertTupleEqual(data2_precached[\"label\"].shape, expected_shape)\n-            self.assertTupleEqual(data2_precached[\"extra\"].shape, expected_shape)\n-\n-            self.assertTupleEqual(data1_postcached[\"image\"].shape, expected_shape)\n-            self.assertTupleEqual(data1_postcached[\"label\"].shape, expected_shape)\n-            self.assertTupleEqual(data1_postcached[\"extra\"].shape, expected_shape)\n-            self.assertTupleEqual(data2_postcached[\"image\"].shape, expected_shape)\n-            self.assertTupleEqual(data2_postcached[\"label\"].shape, expected_shape)\n-            self.assertTupleEqual(data2_postcached[\"extra\"].shape, expected_shape)\n+            if transform is None:\n+                self.assertEqual(data1_precached[\"image\"], os.path.join(tempdir, \"test_image1.nii.gz\"))\n+                self.assertEqual(data2_precached[\"label\"], os.path.join(tempdir, \"test_label2.nii.gz\"))\n+                self.assertEqual(data1_postcached[\"image\"], os.path.join(tempdir, \"test_image1.nii.gz\"))\n+                self.assertEqual(data2_postcached[\"extra\"], os.path.join(tempdir, \"test_extra2.nii.gz\"))\n+            else:\n+                self.assertTupleEqual(data1_precached[\"image\"].shape, expected_shape)\n+                self.assertTupleEqual(data1_precached[\"label\"].shape, expected_shape)\n+                self.assertTupleEqual(data1_precached[\"extra\"].shape, expected_shape)\n+                self.assertTupleEqual(data2_precached[\"image\"].shape, expected_shape)\n+                self.assertTupleEqual(data2_precached[\"label\"].shape, expected_shape)\n+                self.assertTupleEqual(data2_precached[\"extra\"].shape, expected_shape)\n+\n+                self.assertTupleEqual(data1_postcached[\"image\"].shape, expected_shape)\n+                self.assertTupleEqual(data1_postcached[\"label\"].shape, expected_shape)\n+                self.assertTupleEqual(data1_postcached[\"extra\"].shape, expected_shape)\n+                self.assertTupleEqual(data2_postcached[\"image\"].shape, expected_shape)\n+                self.assertTupleEqual(data2_postcached[\"label\"].shape, expected_shape)\n+                self.assertTupleEqual(data2_postcached[\"extra\"].shape, expected_shape)\n+\n+            # update the data to cache\n+            test_data_new = [\n+                {\n+                    \"image\": os.path.join(tempdir, \"test_image1_new.nii.gz\"),\n+                    \"label\": os.path.join(tempdir, \"test_label1_new.nii.gz\"),\n+                    \"extra\": os.path.join(tempdir, \"test_extra1_new.nii.gz\"),\n+                },\n+                {\n+                    \"image\": os.path.join(tempdir, \"test_image2_new.nii.gz\"),\n+                    \"label\": os.path.join(tempdir, \"test_label2_new.nii.gz\"),\n+                    \"extra\": os.path.join(tempdir, \"test_extra2_new.nii.gz\"),\n+                },\n+            ]\n+            dataset_postcached.set_data(data=test_data_new)\n+            # test new exchanged cache content\n+            if transform is None:\n+                self.assertEqual(dataset_postcached[0][\"image\"], os.path.join(tempdir, \"test_image1_new.nii.gz\"))\n+                self.assertEqual(dataset_postcached[0][\"label\"], os.path.join(tempdir, \"test_label1_new.nii.gz\"))\n+                self.assertEqual(dataset_postcached[1][\"extra\"], os.path.join(tempdir, \"test_extra2_new.nii.gz\"))\n \n \n @skip_if_windows\ndiff --git a/tests/test_persistentdataset.py b/tests/test_persistentdataset.py\n--- a/tests/test_persistentdataset.py\n+++ b/tests/test_persistentdataset.py\n@@ -123,6 +123,26 @@ def test_shape(self, transform, expected_shape):\n                 for d in data3_postcached:\n                     self.assertTupleEqual(d[\"image\"].shape, expected_shape)\n \n+            # update the data to cache\n+            test_data_new = [\n+                {\n+                    \"image\": os.path.join(tempdir, \"test_image1_new.nii.gz\"),\n+                    \"label\": os.path.join(tempdir, \"test_label1_new.nii.gz\"),\n+                    \"extra\": os.path.join(tempdir, \"test_extra1_new.nii.gz\"),\n+                },\n+                {\n+                    \"image\": os.path.join(tempdir, \"test_image2_new.nii.gz\"),\n+                    \"label\": os.path.join(tempdir, \"test_label2_new.nii.gz\"),\n+                    \"extra\": os.path.join(tempdir, \"test_extra2_new.nii.gz\"),\n+                },\n+            ]\n+            dataset_postcached.set_data(data=test_data_new)\n+            # test new exchanged cache content\n+            if transform is None:\n+                self.assertEqual(dataset_postcached[0][\"image\"], os.path.join(tempdir, \"test_image1_new.nii.gz\"))\n+                self.assertEqual(dataset_postcached[0][\"label\"], os.path.join(tempdir, \"test_label1_new.nii.gz\"))\n+                self.assertEqual(dataset_postcached[1][\"extra\"], os.path.join(tempdir, \"test_extra2_new.nii.gz\"))\n+\n \n if __name__ == \"__main__\":\n     unittest.main()\ndiff --git a/tests/test_smartcachedataset.py b/tests/test_smartcachedataset.py\n--- a/tests/test_smartcachedataset.py\n+++ b/tests/test_smartcachedataset.py\n@@ -11,6 +11,7 @@\n \n import copy\n import os\n+import sys\n import tempfile\n import unittest\n \n@@ -18,8 +19,8 @@\n import numpy as np\n from parameterized import parameterized\n \n-from monai.data import SmartCacheDataset\n-from monai.transforms import Compose, LoadImaged\n+from monai.data import DataLoader, SmartCacheDataset\n+from monai.transforms import Compose, Lambda, LoadImaged\n \n TEST_CASE_1 = [0.1, 0, Compose([LoadImaged(keys=[\"image\", \"label\", \"extra\"])])]\n \n@@ -126,6 +127,49 @@ def test_shuffle(self):\n \n         dataset.shutdown()\n \n+    def test_set_data(self):\n+        data_list1 = list(range(10))\n+\n+        transform = Lambda(func=lambda x: np.array([x * 10]))\n+\n+        dataset = SmartCacheDataset(\n+            data=data_list1,\n+            transform=transform,\n+            cache_rate=0.5,\n+            replace_rate=0.4,\n+            num_init_workers=4,\n+            num_replace_workers=2,\n+            shuffle=False,\n+            progress=True,\n+        )\n+\n+        num_workers = 2 if sys.platform == \"linux\" else 0\n+        dataloader = DataLoader(dataset=dataset, num_workers=num_workers, batch_size=1)\n+\n+        dataset.start()\n+        for i, d in enumerate(dataloader):\n+            np.testing.assert_allclose([[data_list1[i] * 10]], d)\n+        # replace cache content, move forward 2(5 * 0.4) items\n+        dataset.update_cache()\n+        for i, d in enumerate(dataloader):\n+            np.testing.assert_allclose([[data_list1[i + 2] * 10]], d)\n+        # shutdown to update data\n+        dataset.shutdown()\n+        # update the datalist and fill the cache content\n+        data_list2 = list(range(-10, 0))\n+        dataset.set_data(data=data_list2)\n+        # restart the dataset\n+        dataset.start()\n+        # rerun with updated cache content\n+        for i, d in enumerate(dataloader):\n+            np.testing.assert_allclose([[data_list2[i] * 10]], d)\n+        # replace cache content, move forward 2(5 * 0.4) items\n+        dataset.update_cache()\n+        for i, d in enumerate(dataloader):\n+            np.testing.assert_allclose([[data_list2[i + 2] * 10]], d)\n+        # finally shutdown the dataset\n+        dataset.shutdown()\n+\n     def test_datalist(self):\n         data_list = [np.array([i]) for i in range(5)]\n         data_list_backup = copy.copy(data_list)\n", "created_at": "2021-06-24T09:00:28Z", "problem_statement": "support dynamic data list for the dataset APIs (2/July)\n**Is your feature request related to a problem? Please describe.**\r\nAt the workflow level, would be great to have the capability of changing the input training dataset on the fly, without restarting the running engine. This would require some extra works for the cache-based datasets.\r\n\r\ncc @SachidanandAlle @diazandr3s \n", "repo": "Project-MONAI/MONAI", "base_commit": "91ea1b676a8fda066fd07ba6b55e87151da16dde", "version": "0.5", "PASS_TO_PASS": ["tests/test_cachedataset.py::TestCacheThread::test_thread_safe_07", "tests/test_cachedataset.py::TestCacheThread::test_thread_safe_08", "tests/test_lmdbdataset.py::TestMPLMDBDataset::test_mp_cache", "tests/test_cachedataset.py::TestCacheThread::test_thread_safe_06", "tests/test_smartcachedataset.py::TestSmartCacheDataset::test_datalist", "tests/test_cachedataset.py::TestCacheThread::test_thread_safe_03", "tests/test_cachedataset.py::TestCacheThread::test_thread_safe_14", "tests/test_cachedataset.py::TestCacheThread::test_thread_safe_05", "tests/test_cachedataset.py::TestCacheThread::test_thread_safe_13", "tests/test_smartcachedataset.py::TestSmartCacheDataset::test_shuffle", "tests/test_cachedataset.py::TestCacheThread::test_thread_safe_11", "tests/test_cachedataset.py::TestCacheThread::test_thread_safe_04", "tests/test_cachedataset.py::TestCacheThread::test_thread_safe_00", "tests/test_cachedataset.py::TestCacheThread::test_thread_safe_10", "tests/test_smartcachedataset.py::TestSmartCacheDataset::test_update_cache", "tests/test_cachedataset.py::TestCacheThread::test_thread_safe_01", "tests/test_persistentdataset.py::TestDataset::test_cache", "tests/test_lmdbdataset.py::TestLMDBDataset::test_cache", "tests/test_cachedataset.py::TestCacheThread::test_thread_safe_02", "tests/test_cachedataset.py::TestCacheThread::test_thread_safe_12", "tests/test_cachedataset.py::TestCacheThread::test_thread_safe_09"], "FAIL_TO_PASS": ["tests/test_smartcachedataset.py::TestSmartCacheDataset::test_set_data", "tests/test_cachedataset.py::TestCacheDataset::test_set_data"]}
