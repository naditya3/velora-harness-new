###################### OpenHands Configuration Example ######################
#
# All settings have default values, so you only need to uncomment and
# modify what you want to change
# The fields within each section are sorted in alphabetical order.
#
# SETUP: Copy this file to config.toml and fill in your API keys
#   cp config.toml.example config.toml
#
##############################################################################

#################################### Core ####################################
# General core configurations
##############################################################################
[core]
# API keys and configuration for core services

# Base path for the workspace
#workspace_base = "./workspace"

# Cache directory path
#cache_dir = "/tmp/cache"

# Debugging enabled
#debug = false

# Disable color in terminal output
#disable_color = false

# Path to store trajectories, can be a folder or a file
# If it's a folder, the session id will be used as the file name
#save_trajectory_path="./trajectories"

# Maximum number of iterations
#max_iterations = 500

# Runtime environment
#runtime = "docker"

# Name of the default agent
#default_agent = "CodeActAgent"

#################################### LLM #####################################
# Configuration for LLM models (group name starts with 'llm')
# use 'llm' for the default LLM config
# NOTE: Set your API keys as environment variables or directly here
##############################################################################
[llm]
# Default LLM - uses OpenAI GPT-4o
model = "gpt-4o"
api_key = "${OPENAI_API_KEY}"  # Or set directly: "sk-..."
temperature = 0.2
max_input_tokens = 120000
max_output_tokens = 65536

[llm.draft_editor]
correct_num = 5


#################################### Agent ###################################
# Configuration for agents (group name starts with 'agent')
##############################################################################
[agent]
# Whether the browsing tool is enabled
enable_browsing = true

# Whether the LLM draft editor is enabled
enable_llm_editor = false

# Whether the standard editor tool (str_replace_editor) is enabled
enable_editor = true

# Whether the IPython tool is enabled
enable_jupyter = true

# Whether the command tool is enabled
enable_cmd = true

# Whether the think tool is enabled
enable_think = true

# Whether the finish tool is enabled
enable_finish = true

# Whether history should be truncated when hitting context length limit
enable_history_truncation = true

# Whether the condensation request tool is enabled
enable_condensation_request = false

[agent.RepoExplorerAgent]
llm_config = 'gpt3'

#################################### Sandbox ###################################
# Configuration for the sandbox
##############################################################################
[sandbox]
# Sandbox timeout in seconds
#timeout = 120

# Container image to use for the sandbox
#base_container_image = "nikolaik/python-nodejs:python3.12-nodejs22"

# Runtime container image (recommended: use pre-built image)
runtime_container_image = "ghcr.io/openhands/runtime:velora_ready"

# Use host network
#use_host_network = false

# Enable auto linting after editing
#enable_auto_lint = false

#################################### Security ###################################
# Configuration for security features
##############################################################################
[security]
# Enable confirmation mode
#confirmation_mode = false

# The security analyzer to use
#security_analyzer = "llm"

#################################### Condenser #################################
# Condensers control how conversation history is managed
##############################################################################
[condenser]
type = "noop"

# Named condenser for evaluation - use via EVAL_CONDENSER=obs_mask
[condenser.obs_mask]
type = "observation_masking"
attention_window = 50

#################################### Custom LLM Configs ####################################
# Add your LLM configurations below
##############################################################################

# GPT-4 / GPT-5 for coding tasks
[llm.gpt]
model = "gpt-4o"
api_key = ""
temperature = 0.2
max_input_tokens = 120000
max_output_tokens = 65536

# Google Gemini
[llm.gemini]
model = "gemini/gemini-2.0-flash"
api_key = "YOUR_GOOGLE_API_KEY"
temperature = 0.0
top_p = 1.0
max_input_tokens = 1000000
max_output_tokens = 8192
timeout = 600
custom_llm_provider = "gemini"
num_retries = 5
retry_min_wait = 10
retry_max_wait = 60

# Claude via direct Anthropic API
[llm.claude]
model = "claude-sonnet-4-5-20250929"
api_key = "YOUR_ANTHROPIC_API_KEY"
temperature = 0.2
max_input_tokens = 200000
max_output_tokens = 64000

# Qwen via custom endpoint (e.g., Bedrock Proxy)
[llm.qwen]
model = "qwen.qwen3-coder-480b-a35b-v1:0"
api_key = "YOUR_QWEN_API_KEY"
temperature = 0.0
max_input_tokens = 120000
max_output_tokens = 8192
base_url = "YOUR_QWEN_ENDPOINT"
custom_llm_provider = "openai"
drop_params = true
disable_stop_word = true
num_retries = 5
retry_min_wait = 10
retry_max_wait = 60

# Kimi K2 via Moonshot API
[llm.kimi]
model = "kimi-k2-thinking-turbo"
api_key = "YOUR_MOONSHOT_API_KEY"
base_url = "https://api.moonshot.ai/v1"
temperature = 0.2
max_input_tokens = 200000
max_output_tokens = 65536
drop_params = true
native_tool_calling = true
custom_llm_provider = "openai"
# IMPORTANT: Forces Kimi to use tool calls instead of XML <finish> tags
completion_kwargs = { "tool_choice" = "required" }

# Qwen via OpenRouter (alternative)
[llm.qwenOP]
model = "qwen/qwen3-coder-plus"
api_key = "YOUR_OPENROUTER_API_KEY"
base_url = "https://openrouter.ai/api/v1"
temperature = 0.2
max_input_tokens = 120000
max_output_tokens = 8000
custom_llm_provider = "openai"
drop_params = true
native_tool_calling = true
num_retries = 5
retry_min_wait = 10
retry_max_wait = 60

#################################### Condensers for Long Tasks ####################################
[condenser.llm_qwen]
type = "llm"
llm_config = "qwen"
max_size = 100
keep_first = 2
max_event_length = 10000

[condenser.llm_gpt_long]
type = "llm"
llm_config = "gpt"
max_size = 120
keep_first = 2
max_event_length = 10000

