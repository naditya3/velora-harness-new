# Evaluation Pipeline - Added to VeloraHarness

## ğŸ“‹ What Was Added

### âœ… Complete Evaluation Scripts

Four new files have been added to provide a complete end-to-end evaluation pipeline:

```
evaluation/benchmarks/multi_swe_bench/scripts/
â”œâ”€â”€ run_full_eval.sh        â­ Complete pipeline (trajectory + evaluation)
â”œâ”€â”€ run_velora_infer.sh     ğŸ“ Trajectory generation only
â”œâ”€â”€ simple_eval.py          ğŸ”§ Standalone evaluation script
â””â”€â”€ README_EVALUATION.md    ğŸ“š Complete documentation
```

---

## ğŸ¯ Purpose

These scripts enable you to:

1. **Generate AI trajectories** - OpenHands agent solves coding tasks
2. **Evaluate patches** - Apply generated fixes and run tests
3. **Get detailed results** - JSON reports with resolution status

**All in one command!**

---

## ğŸš€ Quick Start Example

### Single Task Test

```bash
# Navigate to VeloraHarness
cd /Users/macbookpro/Documents/SWE_Bench/Velora_SWE_Harness/VeloraHarness

# Activate Poetry environment
poetry shell

# Set critical environment variables
export DOCKER_BUILDKIT=0
export EVAL_DOCKER_IMAGE_PREFIX=mswebench
export USE_INSTANCE_IMAGE=true

# Run full evaluation on 1 task
./evaluation/benchmarks/multi_swe_bench/scripts/run_full_eval.sh \
    llm.gpt \
    ~/datasets/task_1319603449576684.jsonl \
    1 \
    200 \
    1
```

**Expected Output:**
```
============================================
VELORA FULL EVALUATION PIPELINE
============================================
PHASE 1: TRAJECTORY GENERATION
  â†’ Generating git patch...
  âœ… Patch generated: 67.5 KB

PHASE 2: PATCH EVALUATION
  â†’ Applying patch...
  â†’ Running tests...
  âœ… RESOLVED!

=== EVALUATION SUMMARY ===
Total: 1
Resolved: 1
Failed Apply: 0
Errors: 0
```

---

## ğŸ“Š Output Structure

```
evaluation/evaluation_outputs/outputs/<hash>/
â”œâ”€â”€ output.jsonl              â† Trajectory + git patch
â”œâ”€â”€ metadata.json             â† Run metadata
â”œâ”€â”€ llm_completions/          â† LLM response history
â”œâ”€â”€ logs/                     â† Agent logs
â”œâ”€â”€ eval_summary.json         â† Overall evaluation summary
â””â”€â”€ eval_outputs/             â† Per-task evaluation results
    â””â”€â”€ 1319603449576684/
        â”œâ”€â”€ patch.diff        â† Extracted git patch
        â”œâ”€â”€ report.json       â† Resolution status â­
        â”œâ”€â”€ test_output.txt   â† Full test output
        â”œâ”€â”€ run_instance.log  â† Detailed execution log
        â””â”€â”€ eval.sh           â† Test command used
```

### Key File: `report.json`

```json
{
  "resolved": true,              â† âœ… Task successfully resolved!
  "failed_apply_patch": false,   â† Patch applied cleanly
  "error_eval": false,           â† No evaluation errors
  "f2p_status": {
    "tests/test_ThemeSelect.py::TestThemeSelect::test_init": "PASSED"
  },
  "p2p_status": {
    "tests/test_Alert.py::TestAlert::test_init": "PASSED"
  },
  "total_tests": 2,
  "passed_tests": 2,
  "failed_tests": 0
}
```

---

## ğŸ”§ Three Ways to Use

### 1. Full Evaluation (Recommended)

**Use:** Complete pipeline - generates trajectory AND evaluates patch

```bash
./evaluation/benchmarks/multi_swe_bench/scripts/run_full_eval.sh \
    llm.gpt \
    ~/datasets/task.jsonl \
    1 \      # Number of tasks
    200 \    # Max iterations
    1        # Workers
```

**When to use:**
- You want complete results
- You need to know if the fix actually works
- Production runs

---

### 2. Trajectory Only (Faster)

**Use:** Just generate patches, skip evaluation

```bash
./evaluation/benchmarks/multi_swe_bench/scripts/run_velora_infer.sh \
    llm.gpt \
    ~/datasets/task.jsonl \
    10 \     # Can do more tasks faster
    200 \
    2        # Can use more workers
```

**When to use:**
- Batch processing many tasks
- You'll evaluate later
- Testing different models quickly

---

### 3. Evaluate Existing Trajectories

**Use:** Re-evaluate previously generated trajectories

```bash
python3 ./evaluation/benchmarks/multi_swe_bench/scripts/simple_eval.py \
    --input-file evaluation_outputs/.../output.jsonl \
    --dataset ~/datasets/task.jsonl \
    --timeout 600
```

**When to use:**
- You already have trajectory outputs
- Debugging evaluation issues
- Testing different evaluation settings

---

## ğŸ“ Real-World Example

### Scenario: Test the sepal_ui task

**Prerequisites:**
```bash
# 1. Docker image already loaded and tagged
docker images | grep "12rambau_m_sepal_ui"

# 2. Dataset exists
ls ~/datasets/task_1319603449576684.jsonl

# 3. Environment variables set
export DOCKER_BUILDKIT=0
export EVAL_DOCKER_IMAGE_PREFIX=mswebench
export USE_INSTANCE_IMAGE=true
```

**Run:**
```bash
cd /Users/macbookpro/Documents/SWE_Bench/Velora_SWE_Harness/VeloraHarness

poetry shell

./evaluation/benchmarks/multi_swe_bench/scripts/run_full_eval.sh \
    llm.gpt \
    ~/datasets/task_1319603449576684.jsonl \
    1 \
    200 \
    1
```

**Expected:**
- âœ… **Cost:** ~$0.30
- âœ… **Time:** ~3-5 minutes
- âœ… **Patch Size:** ~67 KB
- âœ… **Resolution:** `resolved: true`

---

## ğŸ”— Integration with Existing Workflow

### Before (Manual Process)

```bash
# Step 1: Generate trajectory manually
poetry run python evaluation/benchmarks/multi_swe_bench/run_infer.py ...

# Step 2: Extract patch manually
cat output.jsonl | jq '.test_result.git_patch' > patch.diff

# Step 3: Apply patch manually in Docker
docker run ... git apply patch.diff

# Step 4: Run tests manually
docker exec ... pytest

# Step 5: Parse results manually
# (painful!)
```

### After (Automated)

```bash
# One command does everything!
./evaluation/benchmarks/multi_swe_bench/scripts/run_full_eval.sh \
    llm.gpt ~/datasets/task.jsonl 1 200 1

# Results in standardized JSON format
cat eval_outputs/*/report.json
```

---

## ğŸŒŸ Key Features

1. **âœ… Automated End-to-End**
   - No manual steps required
   - Consistent output format
   - Error handling built-in

2. **âœ… Detailed Results**
   - Per-test status (PASSED/FAILED)
   - Full test output saved
   - Execution logs for debugging

3. **âœ… Production-Ready**
   - Handles errors gracefully
   - Timeout protection
   - Clean Docker lifecycle

4. **âœ… Flexible Usage**
   - Run full pipeline or individual steps
   - Support for all LLM models
   - Configurable workers and iterations

5. **âœ… Comprehensive Documentation**
   - README with examples
   - Troubleshooting guide
   - Expected outputs documented

---

## ğŸ“ Git Status

### Files to Commit

```bash
git add evaluation/benchmarks/multi_swe_bench/scripts/run_full_eval.sh
git add evaluation/benchmarks/multi_swe_bench/scripts/run_velora_infer.sh
git add evaluation/benchmarks/multi_swe_bench/scripts/simple_eval.py
git add evaluation/benchmarks/multi_swe_bench/scripts/README_EVALUATION.md
git add EVALUATION_PIPELINE_ADDED.md

git commit -m "feat: Add complete evaluation pipeline

- run_full_eval.sh: End-to-end trajectory + evaluation
- run_velora_infer.sh: Trajectory generation only
- simple_eval.py: Standalone evaluation script
- README_EVALUATION.md: Complete documentation

Enables automated testing of AI-generated code fixes with
detailed JSON reports and standardized output format."

git push origin main
```

---

## âœ… Verification Checklist

Before pushing to GitHub, verify:

- [ ] All 4 files are staged
- [ ] Scripts are executable (`chmod +x *.sh`)
- [ ] No secrets in scripts (API keys, etc.)
- [ ] README is comprehensive
- [ ] Scripts reference correct paths
- [ ] Environment variables documented

---

## ğŸ¯ What Your Teammate Gets

When your teammate clones the repo, they'll have:

1. **Complete evaluation pipeline** ready to use
2. **Clear documentation** with examples
3. **Three flexible approaches** depending on needs
4. **Standardized output format** for easy parsing
5. **Production-ready scripts** with error handling

They just need to:
1. Follow `SETUP.md` to install dependencies
2. Load Docker images
3. Run `run_full_eval.sh`
4. Get results in `report.json`

---

## ğŸš€ Next Steps

### For Local Testing

```bash
# 1. Test with a single task
./evaluation/benchmarks/multi_swe_bench/scripts/run_full_eval.sh \
    llm.gpt ~/datasets/test_task.jsonl 1 200 1

# 2. Review output
cat evaluation_outputs/.../eval_outputs/*/report.json

# 3. If successful, scale up!
```

### For AWS Deployment

```bash
# 1. Copy scripts to AWS instances
rsync -avz evaluation/benchmarks/multi_swe_bench/scripts/ \
    ubuntu@aws-instance:~/VeloraHarness/evaluation/benchmarks/multi_swe_bench/scripts/

# 2. Run remotely
ssh aws-instance './VeloraHarness/evaluation/benchmarks/multi_swe_bench/scripts/run_full_eval.sh llm.gpt ~/datasets/task.jsonl 1 200 1'

# 3. Download results
scp -r ubuntu@aws-instance:~/VeloraHarness/evaluation_outputs/ ./
```

### For Batch Processing (200 Tasks)

```bash
# Use master pipeline (if available) or run in batches
for batch in task_{1..200}.jsonl; do
    ./evaluation/benchmarks/multi_swe_bench/scripts/run_full_eval.sh \
        llm.gpt ~/datasets/$batch 1 200 1
done
```

---

## ğŸ‰ Summary

You now have a **complete, production-ready evaluation pipeline** that:

- âœ… Works with the Dockerfile.j2 fix
- âœ… Generates detailed JSON reports
- âœ… Handles errors gracefully
- âœ… Supports all LLM models
- âœ… Scales from 1 to 200+ tasks
- âœ… Is fully documented

**Ready to commit and share with your team!** ğŸš€

