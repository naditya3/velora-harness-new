# VeloraHarness Output Directory Structure

This document defines the standard output directory structure for all evaluation results across RCT, LST, and SWE benchmarks.

## Overview

All evaluation outputs follow a consistent hierarchical structure:

```
evaluation/evaluation_outputs/
└── Trajectory_results/
    └── {Benchmark}/
        └── {Instance_ID}/
            └── {Model}/
                └── Run{N}/
                    └── [output files]
```

---

## Directory Hierarchy

### Level 1: Root

```
evaluation/evaluation_outputs/
└── Trajectory_results/
```

All trajectory and evaluation results are stored under `Trajectory_results/`.

### Level 2: Benchmark Type

```
Trajectory_results/
├── RCT/          # Expensify/SWE-Lancer JavaScript tasks
├── SWE-hard/     # Standard SWE-Bench (Python, PHP, Ruby, Go, Java)
├── LST/          # Large-Scale SWE Tasks
└── Lite/         # SWE-Bench Lite subset
```

| Benchmark | Description | Languages |
|-----------|-------------|-----------|
| **RCT** | Real Coding Tasks (Expensify/SWE-Lancer) | JavaScript, TypeScript |
| **SWE-hard** | Standard Multi-SWE-Bench | Python, PHP, Ruby, Go, Java |
| **LST** | Large-Scale SWE Tasks | Python |
| **Lite** | SWE-Bench Lite (300 tasks) | Python |

### Level 3: Instance ID

```
RCT/
├── 1769880766122899/
├── 1769880766144488/
└── ...

SWE-hard/
├── iterative__dvc_0.92.0_0.92.1/
├── psf__requests_2.4.0_2.4.1/
└── ...
```

The instance ID is extracted from the dataset file and uniquely identifies each task.

### Level 4: Model

```
1769880766122899/
├── Gemini/
├── Claude/
└── GPT/
```

| Model Key | Full Model Name | Config Key |
|-----------|-----------------|------------|
| **Gemini** | gemini-3-pro-preview | `llm.gemini3` |
| **Claude** | claude-sonnet-4-20250514 | `llm.claude` |
| **GPT** | gpt-5.2-codex | `llm.gpt` |

### Level 5: Run Number (Pass@K)

```
Gemini/
├── Run1/
├── Run2/
├── Run3/
├── Run4/
├── Run5/
├── Run6/
├── Run7/
└── Run8/
```

For pass@k evaluation (default k=8), each run is stored separately.

---

## Run Directory Contents

Each `Run{N}/` directory contains:

```
Run1/
├── output.jsonl              # Trajectory (agent actions + git_patch)
├── metadata.json             # Run configuration and metadata
├── llm_completions/          # LLM API request/response logs
│   └── {instance_id}/
│       └── *.json            # Individual completion files
├── logs/                     # Execution logs
├── eval_pilot2_output.jsonl  # Raw evaluation result with details
└── eval_outputs/             # OpenHands-compatible format
    ├── report.json           # Aggregate report
    └── {instance_id}/
        ├── report.json       # Instance-level report
        ├── patch.diff        # Git patch applied
        ├── test_output.txt   # Full test output
        ├── run_instance.log  # Evaluation execution log
        └── eval.sh           # Test command used
```

### File Descriptions

| File | Description | Size |
|------|-------------|------|
| `output.jsonl` | Complete trajectory with agent actions, thoughts, and git_patch | 400KB+ |
| `metadata.json` | Run configuration (model, iterations, timestamps) | ~2KB |
| `llm_completions/` | Raw LLM API logs for debugging | 3MB+ |
| `eval_pilot2_output.jsonl` | Detailed evaluation result with F2P/P2P breakdown | 400KB+ |
| `eval_outputs/report.json` | Aggregate report in OpenHands format | ~8KB |
| `eval_outputs/{id}/patch.diff` | The git patch generated by the model | varies |
| `eval_outputs/{id}/test_output.txt` | Complete test execution output | varies |

---

## Complete Example

### RCT (Expensify) Task

```
evaluation/evaluation_outputs/
└── Trajectory_results/
    └── RCT/
        └── 1769880766122899/
            ├── Gemini/
            │   ├── Run1/
            │   │   ├── output.jsonl
            │   │   ├── metadata.json
            │   │   ├── llm_completions/
            │   │   │   └── 1769880766122899/
            │   │   │       ├── 0001_request.json
            │   │   │       ├── 0001_response.json
            │   │   │       └── ...
            │   │   ├── logs/
            │   │   ├── eval_pilot2_output.jsonl
            │   │   └── eval_outputs/
            │   │       ├── report.json
            │   │       └── 1769880766122899/
            │   │           ├── report.json
            │   │           ├── patch.diff
            │   │           ├── test_output.txt
            │   │           ├── run_instance.log
            │   │           └── eval.sh
            │   ├── Run2/
            │   │   └── ...
            │   └── Run8/
            │       └── ...
            ├── Claude/
            │   ├── Run1/
            │   └── ...
            └── GPT/
                └── ...
```

### SWE-hard (Standard SWE-Bench) Task

```
evaluation/evaluation_outputs/
└── Trajectory_results/
    └── SWE-hard/
        └── iterative__dvc_0.92.0_0.92.1/
            ├── Gemini/
            │   ├── Run1/
            │   │   ├── output.jsonl
            │   │   ├── metadata.json
            │   │   ├── llm_completions/
            │   │   ├── eval_pilot2_output.jsonl
            │   │   └── eval_outputs/
            │   │       ├── report.json
            │   │       └── iterative__dvc_0.92.0_0.92.1/
            │   │           ├── report.json
            │   │           ├── patch.diff
            │   │           └── test_output.txt
            │   └── Run8/
            └── Claude/
                └── ...
```

---

## Environment Variables

Scripts use these environment variables for path generation:

| Variable | Description | Default |
|----------|-------------|---------|
| `BENCHMARK_TYPE` | RCT, SWE-hard, LST, Lite | Auto-detected from dataset |
| `INSTANCE_ID` | Task instance identifier | From dataset |
| `MODEL_NAME` | Gemini, Claude, GPT | From `--models` flag |
| `RUN_NUMBER` | 1-8 for pass@8 | From iteration |

---

## Path Generation Logic

### In `rct.sh`

```bash
# Generate output path for a run
get_run_output_dir() {
    local benchmark="$1"      # RCT, SWE-hard, LST, Lite
    local instance_id="$2"    # e.g., 1769880766122899
    local model="$3"          # Gemini, Claude, GPT
    local run_number="$4"     # 1-8
    
    echo "$OUTPUT_DIR/Trajectory_results/$benchmark/$instance_id/$model/Run$run_number"
}
```

### In `run_full_eval_with_s3.sh`

```bash
# Determine benchmark type from dataset
detect_benchmark_type() {
    local dataset="$1"
    
    if [[ "$dataset" == *"swelancer"* ]] || [[ "$dataset" == *"expensify"* ]]; then
        echo "RCT"
    elif [[ "$dataset" == *"swe_lite"* ]]; then
        echo "Lite"
    elif [[ "$dataset" == *"lst"* ]]; then
        echo "LST"
    else
        echo "SWE-hard"
    fi
}
```

---

## Report JSON Format

### Aggregate Report (`eval_outputs/report.json`)

```json
{
    "1769880766122899": {
        "patch_is_None": false,
        "patch_exists": true,
        "patch_successfully_applied": true,
        "resolved": true,
        "tests_status": {
            "FAIL_TO_PASS": {
                "success": ["issues/15925/test.py::test_expensify"],
                "failure": []
            },
            "PASS_TO_PASS": {
                "success": [],
                "failure": []
            }
        }
    }
}
```

### Instance Report (`eval_outputs/{id}/report.json`)

Same format as aggregate, but for single instance.

---

## Usage Examples

### Running Pass@8 Evaluation

```bash
./rct.sh --models gemini --runs 8 --instances 1769880766122899
```

This creates:
```
Trajectory_results/RCT/1769880766122899/Gemini/Run1/
Trajectory_results/RCT/1769880766122899/Gemini/Run2/
...
Trajectory_results/RCT/1769880766122899/Gemini/Run8/
```

### Running Multiple Models

```bash
./rct.sh --models gemini,claude,gpt --runs 8 --instances 1769880766122899
```

This creates:
```
Trajectory_results/RCT/1769880766122899/Gemini/Run1-8/
Trajectory_results/RCT/1769880766122899/Claude/Run1-8/
Trajectory_results/RCT/1769880766122899/GPT/Run1-8/
```

---

## Migration from Old Structure

### Old Structure

```
outputs/{dataset}-train/CodeActAgent/{model}_maxiter_{N}_v1.1.0-no-hint-run_{K}/
```

### New Structure

```
Trajectory_results/{Benchmark}/{Instance_ID}/{Model}/Run{K}/
```

### Mapping

| Old Component | New Component |
|---------------|---------------|
| `{dataset}-train` | Auto-detected to `{Benchmark}` |
| `{model}_maxiter_{N}_...` | Split to `{Model}/Run{K}` |
| `eval_outputs/` | Unchanged (inside Run folder) |

---

## See Also

- [Evaluation Scripts README](../evaluation/benchmarks/multi_swe_bench/scripts/README.md)
- [rct.sh Configuration](../evaluation/benchmarks/multi_swe_bench/scripts/rct_config.toml)
- [Troubleshooting Guide](./TROUBLESHOOTING_GUIDE.md)
