{"instance_id": "721209483834310", "repo": "craylabs/smartsim", "base_commit": "14f6d5038636b29d3bb103010f2d78cccf72bddd", "problem_statement": "Experiment.create_run_settings:\\n# Description\\r\\nWhen writing a SmartSim driver script, ``RunSettings`` are often created to define how an entity is to be executed. These are largely convenience wrappers around parallel run commands.  \\r\\n\\r\\n### Currently Supported\\r\\n - ``MpirunSettings`` for ``mpirun``\\r\\n - ``AprunSettings`` for ``aprun``\\r\\n - ``JsrunSettings`` for ``jsrun``\\r\\n - ``SrunSettings`` for ``srun``\\r\\n - ``RunSettings`` for custom and serial launches\\r\\n\\r\\n# Justification\\r\\nUsers that wish to make their SmartSim scripts more portable to various systems that use different workload managers will benefit from these because it will perform a similar switch case that we have seen implemented in a few cases.\\r\\n\\r\\n\\r\\n# Implementation Strategy\\r\\n \\r\\nA single function should be created with abstract arguments that apply generally to each of the various run command types.\\r\\n\\r\\n```python\\r\\nclass Experiment:\\r\\n   ...\\r\\n\\r\\n   def create_run_settings(self, exe, exe_args, run_args, env_vars, **kwargs):\\r\\n     \\r\\n```\\r\\nThis function will use the fact that the Experiment knows the launcher type and will:\\r\\n - determine which ``RunSettings`` child class to use based on the launcher type\\r\\n - initialize a new instance of that class\\r\\n - return it.\\r\\n\\r\\n\\r\\n## to think about\\r\\n- functions like ``RunSettings.set_cpus_per_task`` will need to be unified accross all classes if not already.\\r\\n", "FAIL_TO_PASS": ["tests/test_local_restart.py::test_restart", "tests/test_local_launch.py::test_models", "tests/test_local_multi_run.py::test_models", "tests/test_local_restart.py::test_ensemble", "tests/test_local_launch.py::test_ensemble"], "PASS_TO_PASS": ["tests/test_shell_util.py::test_errors", "tests/test_alps_settings.py::test_aprun_add_mpmd", "tests/test_generator.py::test_ensemble_overwrite", "tests/test_lsf_parser.py::test_parse_jslist", "tests/test_slurm_parser.py::test_parse_salloc", "tests/test_lsf_settings.py::test_jsrun_mpmd", "tests/test_slurm_parser.py::test_parse_sstat_nodes_3", "tests/test_model.py::test_disable_key_prefixing", "tests/test_slurm_parser.py::test_parse_salloc_error", "tests/test_slurm_parser.py::test_parse_sstat_nodes", "tests/test_launch_errors.py::test_model_failure", "tests/test_controller_errors.py::test_no_launcher", "tests/test_ensemble.py::test_ensemble_type", "tests/test_modelwriter.py::test_write_new_tag_configs", "tests/test_slurm_settings.py::test_sbatch_settings", "tests/test_controller_errors.py::test_finished_entity_wrong_type", "tests/test_ensemble.py::test_models_property", "tests/test_lsf_parser.py::test_parse_bsub", "tests/test_slurm_parser.py::test_parse_sacct_step_id", "tests/test_experiment.py::test_type_exp_path", "tests/test_model.py::test_repr", "tests/test_pbs_parser.py::test_parse_qsub", "tests/test_ensemble.py::test_incorrect_param_type", "tests/test_lsf_settings.py::test_jsrun_settings", "tests/test_step_info.py::test_default", "tests/test_experiment.py::test_bad_ensemble_init_no_params", "tests/test_model.py::test_register_incoming_entity_preexists", "tests/test_modelwriter.py::test_write_easy_configs", "tests/test_config.py::test_launcher_log_interval_attributes", "tests/test_slurm_parser.py::test_parse_salloc_error_2", "tests/test_lsf_parser.py::test_parse_bsub_error", "tests/test_ensemble.py::test_user_strategy", "tests/test_shell_util.py::test_execute_async_cmd", "tests/test_run_settings.py::test_addto_existing_exe_args", "tests/test_modelwriter.py::test_mw_error_1", "tests/test_experiment.py::test_bad_ensemble_init_no_rs_bs", "tests/test_slurm_parser.py::test_parse_sstat_nodes_4", "tests/test_experiment.py::test_status_pre_launch", "tests/test_lsf_parser.py::test_parse_bsub_nodes", "tests/test_entitylist.py::test_entity_list_repr", "tests/test_experiment.py::test_stop_entity", "tests/test_ensemble.py::test_add_existing_model", "tests/test_openmpi_settings.py::test_mpirun_settings", "tests/test_cobalt_parser.py::test_parse_qsub_out", "tests/test_lsf_settings.py::test_jsrun_args", "tests/test_slurm_parser.py::test_parse_sacct_status_1", "tests/test_slurm_parser.py::test_parse_salloc_error_3", "tests/test_alps_settings.py::test_aprun_args", "tests/test_slurm_parser.py::test_parse_sacct_status", "tests/test_slurm_settings.py::test_srun_args", "tests/test_generator.py::test_ensemble_overwrite_error", "tests/test_slurm_parser.py::test_parse_sacct_status_2", "tests/test_cobalt_parser.py::test_parse_step_status", "tests/test_config.py::test_redis_exe", "tests/test_openmpi_settings.py::test_mpirun_hostlist_errors", "tests/test_orchestrator.py::test_catch_local_db_errors", "tests/test_experiment.py::test_bad_exp_path", "tests/test_model.py::test_str", "tests/test_run_settings.py::test_bad_exe_args_2", "tests/test_ensemble.py::test_key_prefixing", "tests/test_run_settings.py::test_bad_exe_args", "tests/test_shell_util.py::test_execute_cmd", "tests/test_lsf_parser.py::test_parse_max_step_id", "tests/test_lsf_settings.py::test_bsub_batch_settings", "tests/test_slurm_settings.py::test_change_batch_cmd", "tests/test_controller_errors.py::test_bad_orc_checkpoint", "tests/test_controller_errors.py::test_entity_status_wrong_type", "tests/test_experiment.py::test_poll", "tests/test_controller_errors.py::test_catch_empty_ensemble", "tests/test_run_settings.py::test_str", "tests/test_ensemble.py::test_unknown_perm_strat", "tests/test_slurm_settings.py::test_update_env", "tests/test_ensemble.py::test_bad_perm_strat", "tests/test_openmpi_settings.py::test_mpirun_args", "tests/test_pbs_parser.py::test_parse_qstat_status", "tests/test_experiment.py::test_status_type", "tests/test_ensemble.py::test_bad_perm_strat_2", "tests/test_config.py::test_redisai", "tests/test_openmpi_settings.py::test_format_env", "tests/test_slurm_get_alloc.py::test_get_alloc_format", "tests/test_modelwriter.py::test_write_med_configs", "tests/test_pbs_parser.py::test_parse_qsub_error", "tests/test_ensemble.py::test_add_model_type", "tests/test_slurm_parser.py::test_parse_sstat_nodes_1", "tests/test_slurm_settings.py::test_format_env", "tests/test_alps_settings.py::test_aprun_settings", "tests/test_controller_errors.py::test_finished_not_found", "tests/test_config.py::test_redis_conf", "tests/test_experiment.py::test_bad_ensemble_init_no_rs", "tests/test_controller_errors.py::test_entity_list_status_wrong_type", "tests/test_slurm_parser.py::test_parse_sstat_nodes_5", "tests/test_experiment.py::test_finished_type", "tests/with_ray/test_ray.py::test_ray_errors", "tests/test_cobalt_parser.py::test_parse_step_id", "tests/test_experiment.py::test_stop_type", "tests/test_slurm_settings.py::test_sbatch_manual", "tests/test_slurm_parser.py::test_parse_salloc_extra", "tests/test_modelwriter.py::test_mw_error_2", "tests/test_lsf_settings.py::test_bsub_batch_manual", "tests/test_ensemble.py::test_step", "tests/test_lsf_settings.py::test_jsrun_format_env", "tests/test_controller_errors.py::test_unsupported_launcher", "tests/test_pbs_parser.py::test_parse_qstat_nodes", "tests/test_slurm_get_alloc.py::test_get_alloc_format_overlap", "tests/test_slurm_parser.py::test_parse_sacct_step_id_2", "tests/test_lsf_settings.py::test_jsrun_update_env", "tests/test_run_settings.py::test_format_run_args", "tests/test_slurm_parser.py::test_parse_salloc_high", "tests/test_entitylist.py::test_entity_list_init", "tests/test_slurm_parser.py::test_parse_sstat_nodes_2", "tests/test_entitylist.py::test_entity_list_getitem", "tests/test_init.py::test_import_ss", "tests/test_slurm_settings.py::test_srun_settings", "tests/test_slurm_parser.py::test_parse_salloc_error_4", "tests/test_experiment.py::test_model_prefix", "tests/test_ensemble.py::test_random", "tests/test_generator.py::test_ensemble", "tests/test_openmpi_settings.py::test_mpirun_add_mpmd", "tests/test_run_settings.py::test_add_exe_args", "tests/test_step_info.py::test_str", "tests/test_alps_settings.py::test_format_env", "tests/test_openmpi_settings.py::test_mpirun_hostlist_errors_1", "tests/test_ensemble.py::test_all_perm", "tests/test_generator.py::test_dir_files"], "language": "python", "test_command": "source /saved/ENV || source /saved/*/ENV && pytest --no-header -rA --tb=no -p no:cacheprovider --continue-on-collection-errors", "test_output_parser": "python/parse_log_pytest_v3", "image_storage_uri": "vmvm-registry.fbinfra.net/repomate_image_activ_pytest/craylabs_smartsim:14f6d5038636b29d3bb103010f2d78cccf72bddd", "patch": "", "test_patch": "[\"diff --git a/conftest.py b/conftest.py\\nindex 9d132d537..81be26853 100644\\n--- a/conftest.py\\n+++ b/conftest.py\\n@@ -142,7 +142,7 @@ def get_base_run_settings(exe, args, nodes=1, ntasks=1, **kwargs):\\n             raise SSConfigError(f\\\"Base run settings are available for Slurm, PBS, Cobalt, and LSF, but launcher was {test_launcher}\\\")\\n         # TODO allow user to pick aprun vs MPIrun\\n         return RunSettings(exe, args)\\n-        \\n+\\n \\n     @staticmethod\\n     def get_run_settings(exe, args, nodes=1, ntasks=1, **kwargs):\\n\",\"diff --git a/tests/test_local_launch.py b/tests/test_local_launch.py\\nindex 8f32fd618..fc63f77a8 100644\\n--- a/tests/test_local_launch.py\\n+++ b/tests/test_local_launch.py\\n@@ -1,5 +1,4 @@\\n from smartsim import Experiment, constants\\n-from smartsim.settings import RunSettings\\n \\n \\\"\\\"\\\"\\n Test the launch of simple entity types with local launcher\\n@@ -12,7 +11,7 @@ def test_models(fileutils):\\n     test_dir = fileutils.make_test_dir(exp_name)\\n \\n     script = fileutils.get_test_conf_path(\\\"sleep.py\\\")\\n-    settings = RunSettings(\\\"python\\\", f\\\"{script} --time=3\\\")\\n+    settings = exp.create_run_settings(\\\"python\\\", f\\\"{script} --time=3\\\")\\n \\n     M1 = exp.create_model(\\\"m1\\\", path=test_dir, run_settings=settings)\\n     M2 = exp.create_model(\\\"m2\\\", path=test_dir, run_settings=settings)\\n@@ -28,7 +27,7 @@ def test_ensemble(fileutils):\\n     test_dir = fileutils.make_test_dir(exp_name)\\n \\n     script = fileutils.get_test_conf_path(\\\"sleep.py\\\")\\n-    settings = RunSettings(\\\"python\\\", f\\\"{script} --time=3\\\")\\n+    settings = exp.create_run_settings(\\\"python\\\", f\\\"{script} --time=3\\\")\\n \\n     ensemble = exp.create_ensemble(\\\"e1\\\", run_settings=settings, replicas=2)\\n     ensemble.set_path(test_dir)\\n\",\"diff --git a/tests/test_local_multi_run.py b/tests/test_local_multi_run.py\\nindex 589040d5f..167f32a63 100644\\n--- a/tests/test_local_multi_run.py\\n+++ b/tests/test_local_multi_run.py\\n@@ -12,7 +12,7 @@ def test_models(fileutils):\\n     test_dir = fileutils.make_test_dir(exp_name)\\n \\n     script = fileutils.get_test_conf_path(\\\"sleep.py\\\")\\n-    settings = RunSettings(\\\"python\\\", f\\\"{script} --time=5\\\")\\n+    settings = exp.create_run_settings(\\\"python\\\", f\\\"{script} --time=5\\\")\\n \\n     M1 = exp.create_model(\\\"m1\\\", path=test_dir, run_settings=settings)\\n     M2 = exp.create_model(\\\"m2\\\", path=test_dir, run_settings=settings)\\n\",\"diff --git a/tests/test_local_restart.py b/tests/test_local_restart.py\\nindex 39791e24f..1f09ad1f5 100644\\n--- a/tests/test_local_restart.py\\n+++ b/tests/test_local_restart.py\\n@@ -13,7 +13,7 @@ def test_restart(fileutils):\\n     test_dir = fileutils.make_test_dir(exp_name)\\n \\n     script = fileutils.get_test_conf_path(\\\"sleep.py\\\")\\n-    settings = RunSettings(\\\"python\\\", f\\\"{script} --time=3\\\")\\n+    settings = exp.create_run_settings(\\\"python\\\", f\\\"{script} --time=3\\\")\\n \\n     M1 = exp.create_model(\\\"m1\\\", path=test_dir, run_settings=settings)\\n \\n@@ -33,7 +33,7 @@ def test_ensemble(fileutils):\\n     test_dir = fileutils.make_test_dir(exp_name)\\n \\n     script = fileutils.get_test_conf_path(\\\"sleep.py\\\")\\n-    settings = RunSettings(\\\"python\\\", f\\\"{script} --time=3\\\")\\n+    settings = exp.create_run_settings(\\\"python\\\", f\\\"{script} --time=3\\\")\\n \\n     ensemble = exp.create_ensemble(\\\"e1\\\", run_settings=settings, replicas=2)\\n     ensemble.set_path(test_dir)\\n\",\"diff --git a/tests/test_smartredis.py b/tests/test_smartredis.py\\nindex 3c65455ce..eaf4f0397 100644\\n--- a/tests/test_smartredis.py\\n+++ b/tests/test_smartredis.py\\n@@ -50,7 +50,7 @@ def test_exchange(fileutils):\\n     exp.generate(orc)\\n     exp.start(orc, block=False)\\n \\n-    rs = RunSettings(\\\"python\\\", \\\"producer.py --exchange\\\")\\n+    rs = exp.create_run_settings(\\\"python\\\", \\\"producer.py --exchange\\\")\\n     params = {\\\"mult\\\": [1, -10]}\\n     ensemble = Ensemble(\\n         name=\\\"producer\\\",\\n@@ -99,8 +99,8 @@ def test_consumer(fileutils):\\n     exp.generate(orc)\\n     exp.start(orc, block=False)\\n \\n-    rs_prod = RunSettings(\\\"python\\\", \\\"producer.py\\\")\\n-    rs_consumer = RunSettings(\\\"python\\\", \\\"consumer.py\\\")\\n+    rs_prod = exp.create_run_settings(\\\"python\\\", \\\"producer.py\\\")\\n+    rs_consumer = exp.create_run_settings(\\\"python\\\", \\\"consumer.py\\\")\\n     params = {\\\"mult\\\": [1, -10]}\\n     ensemble = Ensemble(\\n         name=\\\"producer\\\", params=params, run_settings=rs_prod, perm_strat=\\\"step\\\"\\n\",\"diff --git a/tests/backends/test_onnx.py b/tests/backends/test_onnx.py\\nindex ccc02e44d..0be6bee02 100644\\n--- a/tests/backends/test_onnx.py\\n+++ b/tests/backends/test_onnx.py\\n@@ -51,7 +51,7 @@ def test_sklearn_onnx(fileutils, mlutils, wlmutils):\\n     db.set_path(test_dir)\\n     exp.start(db)\\n \\n-    run_settings = wlmutils.get_run_settings(\\n+    run_settings = exp.create_run_settings(\\n         \\\"python\\\", f\\\"run_sklearn_onnx.py --device={test_device}\\\"\\n     )\\n     model = exp.create_model(\\\"onnx_models\\\", run_settings)\\n\",\"diff --git a/tests/backends/test_tf.py b/tests/backends/test_tf.py\\nindex 47aae9652..06355be43 100644\\n--- a/tests/backends/test_tf.py\\n+++ b/tests/backends/test_tf.py\\n@@ -40,7 +40,7 @@ def test_keras_model(fileutils, mlutils, wlmutils):\\n     db.set_path(test_dir)\\n     exp.start(db)\\n \\n-    run_settings = wlmutils.get_run_settings(\\n+    run_settings = exp.create_run_settings(\\n         \\\"python\\\", f\\\"run_tf.py --device={test_device}\\\"\\n     )\\n     model = exp.create_model(\\\"tf_script\\\", run_settings)\\n\",\"diff --git a/tests/backends/test_torch.py b/tests/backends/test_torch.py\\nindex 2db0cf410..98bc67d99 100644\\n--- a/tests/backends/test_torch.py\\n+++ b/tests/backends/test_torch.py\\n@@ -37,7 +37,7 @@ def test_torch_model_and_script(fileutils, mlutils, wlmutils):\\n     db.set_path(test_dir)\\n     exp.start(db)\\n \\n-    run_settings = wlmutils.get_run_settings(\\n+    run_settings = exp.create_run_settings(\\n         \\\"python\\\", f\\\"run_torch.py --device={test_device}\\\"\\n     )\\n     model = exp.create_model(\\\"torch_script\\\", run_settings)\\n\",\"diff --git a/tests/on_wlm/test_base_settings_on_wlm.py b/tests/on_wlm/test_base_settings_on_wlm.py\\nindex 21cb49ee2..9ae02bb7b 100644\\n--- a/tests/on_wlm/test_base_settings_on_wlm.py\\n+++ b/tests/on_wlm/test_base_settings_on_wlm.py\\n@@ -15,10 +15,6 @@\\n \\n \\n def test_model_on_wlm(fileutils, wlmutils):\\n-    launcher = wlmutils.get_test_launcher()\\n-    if launcher not in [\\\"pbs\\\", \\\"slurm\\\", \\\"cobalt\\\", \\\"lsf\\\"]:\\n-        pytest.skip(\\\"Test only runs on systems with PBSPro, Slurm, LSF, or Cobalt as WLM\\\")\\n-\\n     exp_name = \\\"test-base-settings-model-launch\\\"\\n     exp = Experiment(exp_name, launcher=wlmutils.get_test_launcher())\\n     test_dir = fileutils.make_test_dir(exp_name)\\n@@ -37,10 +33,6 @@ def test_model_on_wlm(fileutils, wlmutils):\\n \\n \\n def test_model_stop_on_wlm(fileutils, wlmutils):\\n-    launcher = wlmutils.get_test_launcher()\\n-    if launcher not in [\\\"pbs\\\", \\\"slurm\\\", \\\"cobalt\\\", \\\"lsf\\\"]:\\n-        pytest.skip(\\\"Test only runs on systems with PBSPro, Slurm, LSF, or Cobalt as WLM\\\")\\n-\\n     exp_name = \\\"test-base-settings-model-stop\\\"\\n     exp = Experiment(exp_name, launcher=wlmutils.get_test_launcher())\\n     test_dir = fileutils.make_test_dir(exp_name)\\n@@ -59,47 +51,3 @@ def test_model_stop_on_wlm(fileutils, wlmutils):\\n     assert M2.name in exp._control._jobs.completed\\n     statuses = exp.get_status(M1, M2)\\n     assert all([stat == constants.STATUS_CANCELLED for stat in statuses])\\n-\\n-\\n-def test_ensemble_on_wlm(fileutils, wlmutils):\\n-    launcher = wlmutils.get_test_launcher()\\n-    if launcher not in [\\\"pbs\\\", \\\"slurm\\\", \\\"cobalt\\\", \\\"lsf\\\"]:\\n-        pytest.skip(\\\"Test only runs on systems with PBSPro, Slurm, LSF, or Cobalt as WLM\\\")\\n-\\n-    exp_name = \\\"test-base-settings-ensemble-launch\\\"\\n-    exp = Experiment(exp_name, launcher=wlmutils.get_test_launcher())\\n-    test_dir = fileutils.make_test_dir(exp_name)\\n-\\n-    script = fileutils.get_test_conf_path(\\\"sleep.py\\\")\\n-    settings = wlmutils.get_base_run_settings(\\\"python\\\", f\\\"{script} --time=5\\\")\\n-    ensemble = exp.create_ensemble(\\\"ensemble\\\", run_settings=settings, replicas=2)\\n-    ensemble.set_path(test_dir)\\n-\\n-    # launch ensemble twice to show that it can also be restarted\\n-    for _ in range(2):\\n-        exp.start(ensemble, block=True)\\n-        statuses = exp.get_status(ensemble)\\n-        assert all([stat == constants.STATUS_COMPLETED for stat in statuses])\\n-\\n-\\n-def test_ensemble_stop_on_wlm(fileutils, wlmutils):\\n-    launcher = wlmutils.get_test_launcher()\\n-    if launcher not in [\\\"pbs\\\", \\\"slurm\\\", \\\"cobalt\\\", \\\"lsf\\\"]:\\n-        pytest.skip(\\\"Test only runs on systems with PBSPro, Slurm, LSF, or Cobalt as WLM\\\")\\n-\\n-    exp_name = \\\"test-base-settings-ensemble-launch\\\"\\n-    exp = Experiment(exp_name, launcher=wlmutils.get_test_launcher())\\n-    test_dir = fileutils.make_test_dir(exp_name)\\n-\\n-    script = fileutils.get_test_conf_path(\\\"sleep.py\\\")\\n-    settings = wlmutils.get_base_run_settings(\\\"python\\\", f\\\"{script} --time=5\\\")\\n-    ensemble = exp.create_ensemble(\\\"ensemble\\\", run_settings=settings, replicas=2)\\n-    ensemble.set_path(test_dir)\\n-\\n-    # stop launched ensemble\\n-    exp.start(ensemble, block=False)\\n-    time.sleep(2)\\n-    exp.stop(ensemble)\\n-    statuses = exp.get_status(ensemble)\\n-    assert all([stat == constants.STATUS_CANCELLED for stat in statuses])\\n-    assert all([m.name in exp._control._jobs.completed for m in ensemble])\\n\",\"diff --git a/tests/on_wlm/test_launch_errors.py b/tests/on_wlm/test_launch_errors.py\\nindex 5057dffe1..57077e4ce 100644\\n--- a/tests/on_wlm/test_launch_errors.py\\n+++ b/tests/on_wlm/test_launch_errors.py\\n@@ -3,7 +3,7 @@\\n import pytest\\n \\n from smartsim import Experiment, constants\\n-from smartsim.error import LauncherError, SmartSimError, SSUnsupportedError\\n+from smartsim.error import SmartSimError\\n \\n # retrieved from pytest fixtures\\n if pytest.test_launcher not in pytest.wlm_options:\\n@@ -18,7 +18,7 @@ def test_failed_status(fileutils, wlmutils):\\n     test_dir = fileutils.make_test_dir(exp_name)\\n \\n     script = fileutils.get_test_conf_path(\\\"bad.py\\\")\\n-    settings = wlmutils.get_run_settings(\\\"python\\\", f\\\"{script} --time=7\\\")\\n+    settings = exp.create_run_settings(\\\"python\\\", f\\\"{script} --time=7\\\", run_comamnd=\\\"auto\\\")\\n \\n     model = exp.create_model(\\\"bad-model\\\", path=test_dir, run_settings=settings)\\n \\n@@ -47,44 +47,11 @@ def test_bad_run_command_args(fileutils, wlmutils):\\n \\n     # this argument will get turned into an argument for the run command\\n     # of the specific WLM of the system.\\n-    settings = wlmutils.get_run_settings(\\n-        \\\"python\\\", f\\\"{script} --time=5\\\", badarg=\\\"bad-arg\\\"\\n+    settings = exp.create_run_settings(\\n+        \\\"python\\\", f\\\"{script} --time=5\\\", run_args={\\\"badarg\\\": \\\"badvalue\\\"}\\n     )\\n \\n     model = exp.create_model(\\\"bad-model\\\", path=test_dir, run_settings=settings)\\n \\n     with pytest.raises(SmartSimError):\\n         exp.start(model)\\n-\\n-\\n-def test_unsupported_run_settings_on_wlm(fileutils, wlmutils):\\n-    \\\"\\\"\\\"Fails at launch because the model has\\n-    an unsupported run settings\\n-\\n-    This test ensures that the launcher does not try to launch an\\n-    instance of a child class of RunSettings as a LocalStep\\n-    \\\"\\\"\\\"\\n-    launcher = wlmutils.get_test_launcher()\\n-\\n-    exp_name = \\\"test-unsupported-run-settings-on-wlm\\\"\\n-    exp = Experiment(exp_name, launcher=launcher)\\n-    test_dir = fileutils.make_test_dir(exp_name)\\n-\\n-    # temporarily change the test launcher in order to easily get an instance of an unsupported run settings\\n-    if launcher == \\\"slurm\\\":\\n-        wlmutils.set_test_launcher(\\\"cobalt\\\")\\n-    else:\\n-        wlmutils.set_test_launcher(\\\"slurm\\\")\\n-\\n-    script = fileutils.get_test_conf_path(\\\"sleep.py\\\")\\n-    settings = wlmutils.get_run_settings(\\\"python\\\", f\\\"{script} --time=5\\\")\\n-\\n-    # change test launcher back to its original state\\n-    wlmutils.set_test_launcher(launcher)\\n-\\n-    model = exp.create_model(\\n-        \\\"unsupported-rs-model\\\", path=test_dir, run_settings=settings\\n-    )\\n-\\n-    with pytest.raises(SSUnsupportedError):\\n-        exp.start(model)\\n\",\"diff --git a/tests/on_wlm/test_launch_ompi_lsf.py b/tests/on_wlm/test_launch_ompi_lsf.py\\nindex 402d22374..e4d5390b3 100644\\n--- a/tests/on_wlm/test_launch_ompi_lsf.py\\n+++ b/tests/on_wlm/test_launch_ompi_lsf.py\\n@@ -12,15 +12,15 @@ def test_launch_openmpi_lsf(wlmutils, fileutils):\\n     launcher = wlmutils.get_test_launcher()\\n     if launcher != \\\"lsf\\\":\\n         pytest.skip(\\\"Test only runs on systems with LSF as WLM\\\")\\n+    exp_name = \\\"test-launch-openmpi-lsf\\\"\\n+    exp = Experiment(exp_name, launcher=launcher)\\n+    test_dir = fileutils.make_test_dir(exp_name)\\n \\n     script = fileutils.get_test_conf_path(\\\"sleep.py\\\")\\n-    settings = MpirunSettings(\\\"python\\\", script)\\n+    settings = exp.create_run_settings(\\\"python\\\", script, \\\"mpirun\\\")\\n     settings.set_cpus_per_task(1)\\n-    settings.set_tasks(2)\\n+    settings.set_tasks(1)\\n \\n-    exp_name = \\\"test-launch-openmpi-lsf\\\"\\n-    exp = Experiment(exp_name, launcher=launcher)\\n-    test_dir = fileutils.make_test_dir(exp_name)\\n \\n     model = exp.create_model(\\\"ompi-model\\\", path=test_dir, run_settings=settings)\\n     exp.start(model, block=True)\\n\",\"diff --git a/tests/on_wlm/test_launch_orc_slurm.py b/tests/on_wlm/test_launch_orc_slurm.py\\nindex 567c75129..b95375edd 100644\\n--- a/tests/on_wlm/test_launch_orc_slurm.py\\n+++ b/tests/on_wlm/test_launch_orc_slurm.py\\n@@ -78,14 +78,17 @@ def test_incoming_entities(fileutils, wlmutils):\\n     orc.set_path(test_dir)\\n \\n     sleep = fileutils.get_test_conf_path(\\\"sleep.py\\\")\\n-    sleep_settings = wlmutils.get_run_settings(\\\"python\\\", f\\\"{sleep} --time=3\\\")\\n+    sleep_settings = exp.create_run_settings(\\\"python\\\", f\\\"{sleep} --time=3\\\")\\n+    sleep_settings.set_tasks(1)\\n \\n     sleep_ensemble = exp.create_ensemble(\\\"sleep-ensemble\\\",\\n                                          run_settings=sleep_settings,\\n                                          replicas=2)\\n \\n     sskeyin_reader = fileutils.get_test_conf_path(\\\"incoming_entities_reader.py\\\")\\n-    sskeyin_reader_settings = wlmutils.get_run_settings(\\\"python\\\", f\\\"{sskeyin_reader}\\\")\\n+    sskeyin_reader_settings = exp.create_run_settings(\\\"python\\\", f\\\"{sskeyin_reader}\\\")\\n+    sskeyin_reader_settings.set_tasks(1)\\n+\\n     sskeyin_reader_settings.env_vars[\\\"NAME_0\\\"] = sleep_ensemble.entities[0].name\\n     sskeyin_reader_settings.env_vars[\\\"NAME_1\\\"] = sleep_ensemble.entities[1].name\\n     sskeyin_reader = exp.create_model(\\\"sskeyin_reader\\\", path=test_dir, run_settings=sskeyin_reader_settings)\\n\",\"diff --git a/tests/on_wlm/test_restart.py b/tests/on_wlm/test_restart.py\\nindex dbf3734c0..2e12fc5bd 100644\\n--- a/tests/on_wlm/test_restart.py\\n+++ b/tests/on_wlm/test_restart.py\\n@@ -1,5 +1,5 @@\\n import pytest\\n-\\n+from copy import deepcopy\\n from smartsim import Experiment, constants\\n \\n # retrieved from pytest fixtures\\n@@ -14,9 +14,11 @@ def test_restart(fileutils, wlmutils):\\n     test_dir = fileutils.make_test_dir(exp_name)\\n \\n     script = fileutils.get_test_conf_path(\\\"sleep.py\\\")\\n-    settings = wlmutils.get_run_settings(\\\"python\\\", f\\\"{script} --time=5\\\")\\n+    settings = exp.create_run_settings(\\\"python\\\", f\\\"{script} --time=5\\\")\\n+    settings.set_tasks(1)\\n+\\n     M1 = exp.create_model(\\\"m1\\\", path=test_dir, run_settings=settings)\\n-    M2 = exp.create_model(\\\"m2\\\", path=test_dir, run_settings=settings)\\n+    M2 = exp.create_model(\\\"m2\\\", path=test_dir, run_settings=deepcopy(settings))\\n \\n     exp.start(M1, M2, block=True)\\n     statuses = exp.get_status(M1, M2)\\n\",\"diff --git a/tests/on_wlm/test_simple_base_settings_on_wlm.py b/tests/on_wlm/test_simple_base_settings_on_wlm.py\\nindex 0fe1e7ab3..28afd45f3 100644\\n--- a/tests/on_wlm/test_simple_base_settings_on_wlm.py\\n+++ b/tests/on_wlm/test_simple_base_settings_on_wlm.py\\n@@ -7,7 +7,14 @@\\n \\n \\\"\\\"\\\"\\n Test the launch and stop of simple models and ensembles that use base\\n-RunSettings while on WLM.\\n+RunSettings while on WLM that do not include a run command\\n+\\n+These tests will execute code (very light scripts) on the head node\\n+as no run command will exec them out onto the compute nodes\\n+\\n+the reason we test this is because we want to make sure that each\\n+WLM launcher also supports the ability to run scripts on the head node\\n+that also contain an invoking run command\\n \\\"\\\"\\\"\\n \\n # retrieved from pytest fixtures\\n@@ -54,44 +61,3 @@ def test_simple_model_stop_on_wlm(fileutils, wlmutils):\\n     assert M.name in exp._control._jobs.completed\\n     assert exp.get_status(M)[0] == constants.STATUS_CANCELLED\\n \\n-\\n-def test_simple_ensemble_on_wlm(fileutils, wlmutils):\\n-    launcher = wlmutils.get_test_launcher()\\n-    if launcher not in [\\\"pbs\\\", \\\"slurm\\\", \\\"cobalt\\\"]:\\n-        pytest.skip(\\\"Test only runs on systems with PBSPro, Slurm, or Cobalt as WLM\\\")\\n-\\n-    exp_name = \\\"test-simple-base-settings-ensemble-launch\\\"\\n-    exp = Experiment(exp_name, launcher=wlmutils.get_test_launcher())\\n-    test_dir = fileutils.make_test_dir(exp_name)\\n-\\n-    script = fileutils.get_test_conf_path(\\\"sleep.py\\\")\\n-    settings = RunSettings(\\\"python\\\", exe_args=f\\\"{script} --time=5\\\")\\n-    ensemble = exp.create_ensemble(\\\"ensemble\\\", run_settings=settings, replicas=1)\\n-    ensemble.set_path(test_dir)\\n-\\n-    # launch ensemble twice to show that it can also be restarted\\n-    for _ in range(2):\\n-        exp.start(ensemble, block=True)\\n-        assert exp.get_status(ensemble)[0] == constants.STATUS_COMPLETED\\n-\\n-\\n-def test_simple_ensemble_stop_on_wlm(fileutils, wlmutils):\\n-    launcher = wlmutils.get_test_launcher()\\n-    if launcher not in [\\\"pbs\\\", \\\"slurm\\\", \\\"cobalt\\\"]:\\n-        pytest.skip(\\\"Test only runs on systems with PBSPro, Slurm, or Cobalt as WLM\\\")\\n-\\n-    exp_name = \\\"test-simple-base-settings-ensemble-stop\\\"\\n-    exp = Experiment(exp_name, launcher=wlmutils.get_test_launcher())\\n-    test_dir = fileutils.make_test_dir(exp_name)\\n-\\n-    script = fileutils.get_test_conf_path(\\\"sleep.py\\\")\\n-    settings = RunSettings(\\\"python\\\", exe_args=f\\\"{script} --time=5\\\")\\n-    ensemble = exp.create_ensemble(\\\"ensemble\\\", run_settings=settings, replicas=1)\\n-    ensemble.set_path(test_dir)\\n-\\n-    # stop launched ensemble\\n-    exp.start(ensemble, block=False)\\n-    time.sleep(2)\\n-    exp.stop(ensemble)\\n-    assert exp.get_status(ensemble)[0] == constants.STATUS_CANCELLED\\n-    assert ensemble.models[0].name in exp._control._jobs.completed\\n\",\"diff --git a/tests/on_wlm/test_simple_entity_launch.py b/tests/on_wlm/test_simple_entity_launch.py\\nindex b1cc41a6c..61f8a78f4 100644\\n--- a/tests/on_wlm/test_simple_entity_launch.py\\n+++ b/tests/on_wlm/test_simple_entity_launch.py\\n@@ -1,5 +1,5 @@\\n import pytest\\n-\\n+from copy import deepcopy\\n from smartsim import Experiment, constants\\n \\n \\\"\\\"\\\"\\n@@ -7,6 +7,10 @@\\n \\n All entities will obtain the allocation from the environment of the\\n user\\n+\\n+Each of the tests below will have their RunSettings automatically\\n+created which means that these tests may vary in run command that\\n+is used.\\n \\\"\\\"\\\"\\n \\n # retrieved from pytest fixtures\\n@@ -20,9 +24,11 @@ def test_models(fileutils, wlmutils):\\n     test_dir = fileutils.make_test_dir(exp_name)\\n \\n     script = fileutils.get_test_conf_path(\\\"sleep.py\\\")\\n-    settings = wlmutils.get_run_settings(\\\"python\\\", f\\\"{script} --time=5\\\")\\n+    settings = exp.create_run_settings(\\\"python\\\", f\\\"{script} --time=5\\\")\\n+    settings.set_tasks(1)\\n+\\n     M1 = exp.create_model(\\\"m1\\\", path=test_dir, run_settings=settings)\\n-    M2 = exp.create_model(\\\"m2\\\", path=test_dir, run_settings=settings)\\n+    M2 = exp.create_model(\\\"m2\\\", path=test_dir, run_settings=deepcopy(settings))\\n \\n     exp.start(M1, M2, block=True)\\n     statuses = exp.get_status(M1, M2)\\n@@ -35,7 +41,9 @@ def test_ensemble(fileutils, wlmutils):\\n     test_dir = fileutils.make_test_dir(exp_name)\\n \\n     script = fileutils.get_test_conf_path(\\\"sleep.py\\\")\\n-    settings = wlmutils.get_run_settings(\\\"python\\\", f\\\"{script} --time=5\\\")\\n+    settings = exp.create_run_settings(\\\"python\\\", f\\\"{script} --time=5\\\")\\n+    settings.set_tasks(1)\\n+\\n     ensemble = exp.create_ensemble(\\\"e1\\\", run_settings=settings, replicas=2)\\n     ensemble.set_path(test_dir)\\n \\n@@ -53,8 +61,11 @@ def test_summary(fileutils, wlmutils):\\n \\n     sleep = fileutils.get_test_conf_path(\\\"sleep.py\\\")\\n     bad = fileutils.get_test_conf_path(\\\"bad.py\\\")\\n-    sleep_settings = wlmutils.get_run_settings(\\\"python\\\", f\\\"{sleep} --time=3\\\")\\n-    bad_settings = wlmutils.get_run_settings(\\\"python\\\", f\\\"{bad} --time=6\\\")\\n+\\n+    sleep_settings = exp.create_run_settings(\\\"python\\\", f\\\"{sleep} --time=3\\\")\\n+    sleep_settings.set_tasks(1)\\n+    bad_settings = exp.create_run_settings(\\\"python\\\", f\\\"{bad} --time=6\\\")\\n+    bad_settings.set_tasks(1)\\n \\n     sleep = exp.create_model(\\\"sleep\\\", path=test_dir, run_settings=sleep_settings)\\n     bad = exp.create_model(\\\"bad\\\", path=test_dir, run_settings=bad_settings)\\n\",\"diff --git a/tests/on_wlm/test_stop.py b/tests/on_wlm/test_stop.py\\nindex a0fc530fe..d598cb2b0 100644\\n--- a/tests/on_wlm/test_stop.py\\n+++ b/tests/on_wlm/test_stop.py\\n@@ -4,6 +4,13 @@\\n \\n from smartsim import Experiment, constants\\n \\n+\\\"\\\"\\\"\\n+Test Stopping launched entities.\\n+\\n+These tests will have their run settings automatically created\\n+by the experiment which will choose the run_command so runtime may vary.\\n+\\\"\\\"\\\"\\n+\\n # retrieved from pytest fixtures\\n if pytest.test_launcher not in pytest.wlm_options:\\n     pytestmark = pytest.mark.skip(reason=\\\"Not testing WLM integrations\\\")\\n@@ -15,7 +22,8 @@ def test_stop_entity(fileutils, wlmutils):\\n     test_dir = fileutils.make_test_dir(exp_name)\\n \\n     script = fileutils.get_test_conf_path(\\\"sleep.py\\\")\\n-    settings = wlmutils.get_run_settings(\\\"python\\\", f\\\"{script} --time=10\\\")\\n+    settings = exp.create_run_settings(\\\"python\\\", f\\\"{script} --time=10\\\")\\n+    settings.set_tasks(1)\\n     M1 = exp.create_model(\\\"m1\\\", path=test_dir, run_settings=settings)\\n \\n     exp.start(M1, block=False)\\n@@ -32,7 +40,9 @@ def test_stop_entity_list(fileutils, wlmutils):\\n     test_dir = fileutils.make_test_dir(exp_name)\\n \\n     script = fileutils.get_test_conf_path(\\\"sleep.py\\\")\\n-    settings = wlmutils.get_run_settings(\\\"python\\\", f\\\"{script} --time=10\\\")\\n+    settings = exp.create_run_settings(\\\"python\\\", f\\\"{script} --time=10\\\")\\n+    settings.set_tasks(1)\\n+\\n     ensemble = exp.create_ensemble(\\\"e1\\\", run_settings=settings, replicas=2)\\n     ensemble.set_path(test_dir)\\n \\n\",\"diff --git a/tests/test_launch_errors.py b/tests/test_launch_errors.py\\nindex 31d740d6b..f17f317b4 100644\\n--- a/tests/test_launch_errors.py\\n+++ b/tests/test_launch_errors.py\\n@@ -2,12 +2,19 @@\\n \\n from smartsim import Experiment, constants\\n from smartsim.database import Orchestrator\\n-from smartsim.error import SmartSimError\\n-from smartsim.settings import RunSettings\\n+from smartsim.error import SmartSimError, SSUnsupportedError\\n+from smartsim.settings import RunSettings, JsrunSettings\\n \\n-\\\"\\\"\\\"\\n-Test the launch errors with local launcher\\n-\\\"\\\"\\\"\\n+\\n+def test_unsupported_run_settings(fileutils):\\n+    exp_name = \\\"test-unsupported-run-settings\\\"\\n+    exp = Experiment(exp_name, launcher=\\\"slurm\\\")\\n+    test_dir = fileutils.make_test_dir(exp_name)\\n+    bad_settings = JsrunSettings(\\\"echo\\\", \\\"hello\\\")\\n+    model = exp.create_model(\\\"bad_rs\\\", bad_settings)\\n+\\n+    with pytest.raises(SSUnsupportedError):\\n+        exp.start(model)\\n \\n \\n def test_model_failure(fileutils):\\n\",\"diff --git a/tests/test_run_settings.py b/tests/test_run_settings.py\\nindex 5f22523aa..c3813aebb 100644\\n--- a/tests/test_run_settings.py\\n+++ b/tests/test_run_settings.py\\n@@ -3,20 +3,39 @@\\n \\n import pytest\\n \\n-from smartsim.settings import RunSettings\\n-\\n-\\n-def test_run_command_exists():\\n-    # you wouldn't actually do this, but we know python will be installed\\n-    settings = RunSettings(\\\"python\\\", run_command=\\\"python\\\")\\n-    python = which(\\\"python\\\")\\n-    assert settings.run_command == python\\n-\\n-\\n-def test_run_command_not_exists():\\n-    settings = RunSettings(\\\"python\\\", run_command=\\\"not-on-system\\\")\\n-    assert settings.run_command == \\\"not-on-system\\\"\\n-\\n+from smartsim.settings import RunSettings, MpirunSettings\\n+from smartsim.settings.settings import create_run_settings\\n+\\n+\\n+def test_create_run_settings_local():\\n+    # no run command provided\\n+    rs = create_run_settings(\\\"local\\\", \\\"echo\\\", \\\"hello\\\", run_command=None)\\n+    assert rs.run_command == None\\n+    assert type(rs) == RunSettings\\n+\\n+    # auto should never return a run_command when\\n+    # the user has specified the local launcher\\n+    auto = create_run_settings(\\\"local\\\", \\\"echo\\\", \\\"hello\\\", run_command=\\\"auto\\\")\\n+    assert auto.run_command == None\\n+    assert type(auto) == RunSettings\\n+\\n+    # Test when a run_command is provided that we do not currently have a helper\\n+    # implementation for it.\\n+    # NOTE: we allow for the command to be invalid if it's user specified in the\\n+    # case where a head node may not have the same installed binaries as the MOM\\n+    # or compute nodes.\\n+    specific = create_run_settings(\\\"local\\\", \\\"echo\\\", \\\"hello\\\", run_command=\\\"specific\\\")\\n+    assert specific.run_command == \\\"specific\\\"\\n+    assert type(specific) == RunSettings\\n+\\n+    # make it return MpirunSettings\\n+    _mpirun = which(\\\"mpirun\\\")\\n+    if _mpirun:\\n+        mpirun = create_run_settings(\\\"local\\\", \\\"echo\\\", \\\"hello\\\", run_command=\\\"mpirun\\\")\\n+        assert mpirun.run_command == _mpirun\\n+        assert type(mpirun) == MpirunSettings\\n+\\n+####### Base Run Settings tests #######\\n \\n def test_add_exe_args():\\n     settings = RunSettings(\\\"python\\\")\\n\",\"diff --git a/tests/with_ray/on_wlm/test_ray_pbs.py b/tests/with_ray/on_wlm/test_ray_pbs.py\\nindex 3858d0241..9ed669e7a 100644\\n--- a/tests/with_ray/on_wlm/test_ray_pbs.py\\n+++ b/tests/with_ray/on_wlm/test_ray_pbs.py\\n@@ -1,8 +1,6 @@\\n import logging\\n-import sys\\n import time\\n from os import environ\\n-from shutil import which\\n \\n import pytest\\n \\n\",\"diff --git a/tests/with_ray/on_wlm/test_ray_slurm.py b/tests/with_ray/on_wlm/test_ray_slurm.py\\nindex 143d34b1f..166888c00 100644\\n--- a/tests/with_ray/on_wlm/test_ray_slurm.py\\n+++ b/tests/with_ray/on_wlm/test_ray_slurm.py\\n@@ -1,5 +1,4 @@\\n import logging\\n-import sys\\n import time\\n from os import environ\\n \\n\",\"diff --git a/tests/on_wlm/test_launch_errors.py b/tests/on_wlm/test_launch_errors.py\\nindex 57077e4ce..a079f5aaf 100644\\n--- a/tests/on_wlm/test_launch_errors.py\\n+++ b/tests/on_wlm/test_launch_errors.py\\n@@ -18,7 +18,9 @@ def test_failed_status(fileutils, wlmutils):\\n     test_dir = fileutils.make_test_dir(exp_name)\\n \\n     script = fileutils.get_test_conf_path(\\\"bad.py\\\")\\n-    settings = exp.create_run_settings(\\\"python\\\", f\\\"{script} --time=7\\\", run_comamnd=\\\"auto\\\")\\n+    settings = exp.create_run_settings(\\n+        \\\"python\\\", f\\\"{script} --time=7\\\", run_comamnd=\\\"auto\\\"\\n+    )\\n \\n     model = exp.create_model(\\\"bad-model\\\", path=test_dir, run_settings=settings)\\n \\n\",\"diff --git a/tests/on_wlm/test_launch_ompi_lsf.py b/tests/on_wlm/test_launch_ompi_lsf.py\\nindex e4d5390b3..02cdb4e5f 100644\\n--- a/tests/on_wlm/test_launch_ompi_lsf.py\\n+++ b/tests/on_wlm/test_launch_ompi_lsf.py\\n@@ -21,7 +21,6 @@ def test_launch_openmpi_lsf(wlmutils, fileutils):\\n     settings.set_cpus_per_task(1)\\n     settings.set_tasks(1)\\n \\n-\\n     model = exp.create_model(\\\"ompi-model\\\", path=test_dir, run_settings=settings)\\n     exp.start(model, block=True)\\n     statuses = exp.get_status(model)\\n\",\"diff --git a/tests/on_wlm/test_launch_orc_slurm.py b/tests/on_wlm/test_launch_orc_slurm.py\\nindex b95375edd..8c7c2f758 100644\\n--- a/tests/on_wlm/test_launch_orc_slurm.py\\n+++ b/tests/on_wlm/test_launch_orc_slurm.py\\n@@ -81,9 +81,9 @@ def test_incoming_entities(fileutils, wlmutils):\\n     sleep_settings = exp.create_run_settings(\\\"python\\\", f\\\"{sleep} --time=3\\\")\\n     sleep_settings.set_tasks(1)\\n \\n-    sleep_ensemble = exp.create_ensemble(\\\"sleep-ensemble\\\",\\n-                                         run_settings=sleep_settings,\\n-                                         replicas=2)\\n+    sleep_ensemble = exp.create_ensemble(\\n+        \\\"sleep-ensemble\\\", run_settings=sleep_settings, replicas=2\\n+    )\\n \\n     sskeyin_reader = fileutils.get_test_conf_path(\\\"incoming_entities_reader.py\\\")\\n     sskeyin_reader_settings = exp.create_run_settings(\\\"python\\\", f\\\"{sskeyin_reader}\\\")\\n@@ -91,7 +91,9 @@ def test_incoming_entities(fileutils, wlmutils):\\n \\n     sskeyin_reader_settings.env_vars[\\\"NAME_0\\\"] = sleep_ensemble.entities[0].name\\n     sskeyin_reader_settings.env_vars[\\\"NAME_1\\\"] = sleep_ensemble.entities[1].name\\n-    sskeyin_reader = exp.create_model(\\\"sskeyin_reader\\\", path=test_dir, run_settings=sskeyin_reader_settings)\\n+    sskeyin_reader = exp.create_model(\\n+        \\\"sskeyin_reader\\\", path=test_dir, run_settings=sskeyin_reader_settings\\n+    )\\n     sskeyin_reader.register_incoming_entity(sleep_ensemble.entities[0])\\n     sskeyin_reader.register_incoming_entity(sleep_ensemble.entities[1])\\n \\n\",\"diff --git a/tests/on_wlm/test_restart.py b/tests/on_wlm/test_restart.py\\nindex 2e12fc5bd..fd78ab2f0 100644\\n--- a/tests/on_wlm/test_restart.py\\n+++ b/tests/on_wlm/test_restart.py\\n@@ -1,5 +1,7 @@\\n-import pytest\\n from copy import deepcopy\\n+\\n+import pytest\\n+\\n from smartsim import Experiment, constants\\n \\n # retrieved from pytest fixtures\\n\",\"diff --git a/tests/on_wlm/test_simple_base_settings_on_wlm.py b/tests/on_wlm/test_simple_base_settings_on_wlm.py\\nindex 28afd45f3..4a0ba4da3 100644\\n--- a/tests/on_wlm/test_simple_base_settings_on_wlm.py\\n+++ b/tests/on_wlm/test_simple_base_settings_on_wlm.py\\n@@ -60,4 +60,3 @@ def test_simple_model_stop_on_wlm(fileutils, wlmutils):\\n     exp.stop(M)\\n     assert M.name in exp._control._jobs.completed\\n     assert exp.get_status(M)[0] == constants.STATUS_CANCELLED\\n-\\n\",\"diff --git a/tests/on_wlm/test_simple_entity_launch.py b/tests/on_wlm/test_simple_entity_launch.py\\nindex 61f8a78f4..8e28f7fc5 100644\\n--- a/tests/on_wlm/test_simple_entity_launch.py\\n+++ b/tests/on_wlm/test_simple_entity_launch.py\\n@@ -1,5 +1,7 @@\\n-import pytest\\n from copy import deepcopy\\n+\\n+import pytest\\n+\\n from smartsim import Experiment, constants\\n \\n \\\"\\\"\\\"\\n\",\"diff --git a/tests/test_configs/incoming_entities_reader.py b/tests/test_configs/incoming_entities_reader.py\\nindex 674f9fa7a..0471ff182 100644\\n--- a/tests/test_configs/incoming_entities_reader.py\\n+++ b/tests/test_configs/incoming_entities_reader.py\\n@@ -21,5 +21,6 @@\\n expected_sskeyin = smartsim_separator.join((name_0, name_1))\\n \\n if sskeyin != expected_sskeyin:\\n-    raise ValueError(f\\\"SSKEYIN expected to be {expected_sskeyin}, \\\" \\n-                     f\\\"but was {sskeyin}\\\")\\n\\\\ No newline at end of file\\n+    raise ValueError(\\n+        f\\\"SSKEYIN expected to be {expected_sskeyin}, \\\" f\\\"but was {sskeyin}\\\"\\n+    )\\n\",\"diff --git a/tests/test_launch_errors.py b/tests/test_launch_errors.py\\nindex f17f317b4..919c80ff3 100644\\n--- a/tests/test_launch_errors.py\\n+++ b/tests/test_launch_errors.py\\n@@ -3,7 +3,7 @@\\n from smartsim import Experiment, constants\\n from smartsim.database import Orchestrator\\n from smartsim.error import SmartSimError, SSUnsupportedError\\n-from smartsim.settings import RunSettings, JsrunSettings\\n+from smartsim.settings import JsrunSettings, RunSettings\\n \\n \\n def test_unsupported_run_settings(fileutils):\\n\",\"diff --git a/tests/test_run_settings.py b/tests/test_run_settings.py\\nindex c3813aebb..6788094fc 100644\\n--- a/tests/test_run_settings.py\\n+++ b/tests/test_run_settings.py\\n@@ -3,7 +3,7 @@\\n \\n import pytest\\n \\n-from smartsim.settings import RunSettings, MpirunSettings\\n+from smartsim.settings import MpirunSettings, RunSettings\\n from smartsim.settings.settings import create_run_settings\\n \\n \\n@@ -35,8 +35,10 @@ def test_create_run_settings_local():\\n         assert mpirun.run_command == _mpirun\\n         assert type(mpirun) == MpirunSettings\\n \\n+\\n ####### Base Run Settings tests #######\\n \\n+\\n def test_add_exe_args():\\n     settings = RunSettings(\\\"python\\\")\\n     settings.add_exe_args(\\\"--time 5\\\")\"]", "hints_text": ""}
