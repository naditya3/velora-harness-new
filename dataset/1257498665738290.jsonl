{"instance_id": "1257498665738290", "repo": "aistairc/aiaccel", "base_commit": "0dfc6be2dbd15101a5d700adb54c827eab814882", "problem_statement": "The probrem regarding the execution speed of aiaccel.:\\n**Bug??**\\r\\nThe problem regarding the execution speed of aiaccel. The aiaccel seems to be a little slow.\\r\\nApplying aiaccel to a Sphere function (dim=5) by using RandomOptimizer, it consumes more than 1sec / trial, as shown in the table below.\\r\\nIs this due to sleep time with respect to optimizer, scheduler, and master?\\r\\n\\r\\n|  Trial Number |  Total Time [sec]  | \\r\\n| --------------- | ------------------  |\\r\\n|  100                |  160                     |\\r\\n|  2000              |  3682                   |\\r\\n\\r\\n**Desktop**\\r\\n - OS: Ubuntu 22.04.1 LTS on Windows 11 x86_64\\r\\n - Kernel: 5.15.68.1-microsoft-standard-WSL2\\r\\n - CPU: Intel i7-8700K (12) @ 3.696GHz\\r\\n", "FAIL_TO_PASS": ["tests/unit/abci_test/test_abci_batch.py::TestCreateAbciBatchFile::test_create_abci_batch_file", "tests/unit/test_wrapper_tools.py::TestCeaterRunnerComand::test_create_runner_command", "tests/unit/storage/db/test_storage.py::test_get_best_trial_dict", "tests/unit/optimizer_test/test_abstract_optimizer.py::TestAbstractOptimizer::test_inner_loop_main_process", "tests/unit/optimizer_test/test_nelder_mead.py::TestNelderMead::test_pop_result"], "PASS_TO_PASS": ["tests/unit/optimizer_test/test_nelder_mead.py::TestNelderMead::test_wait_outside_contract", "tests/unit/storage/db/test_trial.py::test_get_all_trial_id", "tests/unit/test_workspace.py::test_exists", "tests/unit/util_test/test_buffer.py::test_Duplicate", "tests/unit/optimizer_test/test_sobol_search.py::TestSobolOptimizer::test_generate_parameter", "tests/unit/scheduler_test/algorithm_test/test_random_sampling.py::test_random_sampling", "tests/unit/master_test/test_abstract_master.py::TestAbstractMaster::test_post_process", "tests/unit/util_test/test_filesystem.py::test_load_yaml", "tests/unit/optimizer_test/test_nelder_mead.py::TestNelderMead::test_add_result_parameters", "tests/unit/optimizer_test/test_nelder_mead.py::TestNelderMead::test_wait_reflect", "tests/unit/storage/db/test_trial.py::test_get_any_state_list", "tests/unit/storage/db/test_timestamp.py::test_all_delete", "tests/unit/optimizer_test/test_nelder_mead_search.py::TestNelderMeadOptimizer::test_pre_process", "tests/unit/storage/db/test_storage.py::test_get_result_and_error", "tests/unit/util_test/test_filesystem.py::test_make_directories", "tests/unit/optimizer_test/test_nelder_mead.py::TestNelderMead::test_reflect", "tests/unit/optimizer_test/test_nelder_mead_search.py::TestNelderMeadOptimizer::test_update_ready_parameter_name", "tests/unit/abci_test/test_abci_qstat.py::test_parse_job_list", "tests/unit/util_test/test_filesystem.py::test_make_directory", "tests/newly_test/random_generation/benchmark/test_benchmark_random_generation.py::TestBenchmarkRandomGeneration::test_run", "tests/unit/abci_test/test_abci_qsub.py::test_create_qsub_command", "tests/unit/storage/db/test_timestamp.py::test_set_any_trial_end_time", "tests/unit/master_test/evaluator_test/test_abstract_evaluator.py::TestAbstractEvaluator::test_save", "tests/unit/optimizer_test/test_nelder_mead.py::test_nelder_mead_parameters", "tests/unit/optimizer_test/test_nelder_mead.py::TestNelderMead::test_add_y_history", "tests/unit/storage/db/test_result.py::test_delete_any_trial_objective", "tests/unit/util_test/test_easy_visualizer.py::test_caption", "tests/unit/util_test/test_buffer.py::test_point_diff", "tests/unit/optimizer_test/test_abstract_optimizer.py::TestAbstractOptimizer::test_register_new_parameters", "tests/unit/storage/db/test_variable.py::test_all_delete_exception", "tests/unit/util_test/test_buffer.py::test_Is_Empty", "tests/unit/util_test/test_buffer.py::test_has_difference", "tests/unit/optimizer_test/test_nelder_mead_search.py::TestNelderMeadOptimizer::test_get_ready_parameters", "tests/unit/storage/db/test_timestamp.py::test_all_delete_exception", "tests/unit/storage/db/test_trial.py::test_set_any_trial_state_exception", "tests/unit/storage/db/test_variable.py::test_set_exception", "tests/unit/optimizer_test/test_sobol_search.py::TestSobolOptimizer::test_generate_initial_parameter", "tests/unit/util_test/test_easy_visualizer.py::test_easy_visualizer_init", "tests/unit/storage/db/test_hp.py::test_set_any_trial_params_exception", "tests/unit/storage/db/test_hp.py::test_get_any_trial_params", "tests/unit/util_test/test_config.py::test_value", "tests/unit/util_test/test_filesystem.py::test_file_read", "tests/unit/master_test/test_abstract_master.py::TestAbstractMaster::test_inner_loop_main_process", "tests/unit/util_test/test_filesystem.py::test_create_yaml", "tests/unit/storage/db/test_storage.py::test_get_finished", "tests/unit/test_workspace.py::test_move_completed_data", "tests/unit/storage/db/test_trial.py::test_get_ready", "tests/unit/abci_test/test_abci_qstat.py::test_parse_qstat", "tests/unit/storage/db/test_result.py::test_set_any_trial_objective", "tests/unit/util_test/test_name.py::test_generate_random_name", "tests/unit/util_test/test_buffer.py::test_replace", "tests/unit/optimizer_test/test_nelder_mead.py::TestNelderMead::test_is_out_of_boundary", "tests/unit/optimizer_test/test_nelder_mead.py::TestNelderMead::test_wait_expand", "tests/unit/util_test/test_easy_visualizer.py::test_set_height", "tests/unit/util_test/test_config.py::TestJsonOrYamlObjectConfig::test_to_dict", "tests/unit/storage/db/test_result.py::test_get_objectives", "tests/unit/master_test/test_abstract_master.py::TestAbstractMaster::test_print_dict_state", "tests/unit/storage/db/test_result.py::test_set_any_trial_objective_exception", "tests/unit/master_test/test_local_master.py::TestLocalMaster::test_init", "tests/unit/storage/db/test_hp.py::test_delete_any_trial_params_exception", "tests/unit/util_test/test_trial_id.py::test_trial_id_init", "tests/unit/storage/db/test_result.py::test_get_any_trial_objective", "tests/unit/storage/db/test_trial.py::test_delete_any_trial_state", "tests/unit/util_test/test_buffer.py::test_delta", "tests/unit/storage/db/test_timestamp.py::test_get_any_trial_start_time", "tests/unit/util_test/test_buffer.py::test_Del", "tests/unit/optimizer_test/test_nelder_mead.py::TestNelderMead::test_reflect_branch", "tests/unit/optimizer_test/test_nelder_mead.py::TestNelderMead::test_expand", "tests/unit/storage/db/test_result.py::test_get_bests", "tests/unit/cli/test_report.py::test_report", "tests/unit/storage/db/test_jobstate.py::test_set_any_trial_jobstates_exception", "tests/unit/optimizer_test/test_abstract_optimizer.py::TestAbstractOptimizer::test__serialize", "tests/unit/util_test/test_buffer.py::test_set_max_len", "tests/unit/util_test/test_buffer.py::test_Pre", "tests/unit/storage/db/test_timestamp.py::test_set_any_trial_end_time_assersion", "tests/unit/util_test/test_config.py::test_config_set", "tests/unit/optimizer_test/test_tpe_search.py::TestTpeOptimizer::test_is_startup_trials", "tests/unit/util_test/test_logger.py::test_str_to_logging_level", "tests/unit/optimizer_test/test_nelder_mead_search.py::TestNelderMeadOptimizer::test_generate_initial_parameter", "tests/unit/optimizer_test/test_abstract_optimizer.py::TestAbstractOptimizer::test_check_error", "tests/unit/optimizer_test/test_abstract_optimizer.py::TestAbstractOptimizer::test_post_process", "tests/unit/storage/db/test_storage.py::test_get_running", "tests/unit/util_test/test_config.py::test_config", "tests/unit/master_test/test_abstract_master.py::TestAbstractMaster::test_pre_process_2", "tests/unit/optimizer_test/test_tpe_search.py::TestTpeOptimizer::test_check_result", "tests/unit/storage/db/test_result.py::test_get_result_trial_id_list", "tests/unit/util_test/test_config.py::TestBaseConfig::test_base_config", "tests/unit/storage/db/test_hp.py::test_set_any_trial_params", "tests/unit/storage/db/test_result.py::test_all_delete_exception", "tests/unit/optimizer_test/test_nelder_mead.py::TestNelderMead::test_init", "tests/unit/optimizer_test/test_tpe_search.py::TestTpeOptimizer::test_serialize", "tests/unit/optimizer_test/test_nelder_mead.py::TestNelderMead::test_order_by", "tests/unit/optimizer_test/test_nelder_mead.py::TestNelderMead::test_initialize", "tests/unit/util_test/test_config.py::test_load_config", "tests/unit/util_test/test_trial_id.py::test_integer", "tests/unit/master_test/test_create.py::test_create", "tests/unit/util_test/test_config.py::TestJsonOrYamlObjectConfig::test_get_property", "tests/unit/storage/db/test_jobstate.py::test_set_any_trial_jobstates", "tests/unit/optimizer_test/test_tpe_search.py::TestTpeOptimizer::test_pre_process", "tests/unit/util_test/test_config.py::TestConfileWrapper::test_get", "tests/unit/optimizer_test/test_sobol_search.py::TestSobolOptimizer::test_pre_process", "tests/unit/storage/db/test_storage.py::test_get_num_finished", "tests/unit/optimizer_test/test_nelder_mead.py::TestNelderMead::test_add_executing", "tests/unit/test_parameter.py::TestParameter::test_load_parameter", "tests/unit/master_test/verification_test/test_abstract_verification.py::TestAbstractVerification::test_make_verification", "tests/unit/storage/db/test_timestamp.py::test_set_any_trial_start_time", "tests/unit/storage/db/test_error.py::test_all_delete", "tests/unit/test_parameter.py::TestParameter::test_get_hyperparameter", "tests/unit/util_test/test_config.py::TestConfileWrapper::test_init", "tests/unit/optimizer_test/test_nelder_mead_search.py::TestNelderMeadOptimizer::test_nelder_mead_main", "tests/unit/storage/db/test_storage.py::test_delete_trial", "tests/unit/optimizer_test/test_abstract_optimizer.py::TestAbstractOptimizer::test_pre_process", "tests/unit/optimizer_test/test_nelder_mead.py::TestNelderMead::test_wait_inside_contract", "tests/unit/storage/db/test_error.py::test_set_any_trial_error_exception", "tests/unit/storage/db/test_storage.py::test_get_hp_dict_int", "tests/unit/storage/db/test_jobstate.py::test_set_any_trial_jobstate_exception", "tests/unit/storage/db/test_storage.py::test_get_best_trial", "tests/unit/master_test/evaluator_test/test_minimize_evaluator.py::TestMinimizeEvaluator::test_maximize_evaluator", "tests/unit/cli/view/test_view.py::test_view", "tests/unit/storage/db/test_storage.py::test_rollback_to_ready", "tests/unit/storage/db/test_timestamp.py::test_delete_any_trial_timestamp_excedption", "tests/unit/storage/db/test_trial.py::test_set_any_trial_state", "tests/unit/master_test/verification_test/test_abstract_verification.py::TestAbstractVerification::test_verify", "tests/unit/optimizer_test/test_tpe_search.py::TestTPESamplerWrapper::test_get_startup_trials", "tests/unit/optimizer_test/test_tpe_search.py::TestTpeOptimizer::test_create_study", "tests/unit/optimizer_test/test_abstract_optimizer.py::TestAbstractOptimizer::test_generate_parameter", "tests/unit/test_workspace.py::test_check_consists", "tests/unit/optimizer_test/test_nelder_mead.py::TestNelderMead::test_shrink", "tests/unit/util_test/test_filesystem.py::test_get_file_result", "tests/unit/storage/db/test_trial.py::test_get_finished", "tests/unit/master_test/verification_test/test_abstract_verification.py::TestAbstractVerification::test_print", "tests/unit/optimizer_test/test_tpe_search.py::TestTpeOptimizer::test_generate_initial_parameter", "tests/unit/optimizer_test/test_nelder_mead_search.py::TestNelderMeadOptimizer::test__add_result", "tests/unit/master_test/test_abstract_master.py::TestAbstractMaster::test_pre_process", "tests/unit/storage/db/test_error.py::test_get_any_trial_error", "tests/unit/master_test/test_abci_master.py::TestAbciMaster::test_pre_process", "tests/unit/test_workspace.py::test_clean", "tests/unit/storage/db/test_error.py::test_delete_any_trial_error", "tests/unit/storage/db/test_storage.py::test_delete_trial_data_after_this", "tests/unit/storage/db/test_timestamp.py::test_delete_any_trial_timestamp", "tests/unit/util_test/test_filesystem.py::test_file_delete", "tests/unit/storage/db/test_result.py::test_get_all_result", "tests/unit/util_test/test_buffer.py::test_Value", "tests/unit/optimizer_test/test_nelder_mead.py::TestNelderMead::test_expand_branch", "tests/unit/storage/db/test_variable.py::test_all_delete", "tests/unit/master_test/verification_test/test_abstract_verification.py::TestAbstractVerification::test_init", "tests/unit/optimizer_test/test_grid_search.py::TestGridOptimizer::test_pre_process", "tests/unit/util_test/test_filesystem.py::test_file_create", "tests/unit/optimizer_test/test_nelder_mead_search.py::TestNelderMeadOptimizer::test_generate_parameter", "tests/unit/test_parameter.py::TestParameter::test_sample", "tests/unit/optimizer_test/test_nelder_mead.py::TestNelderMead::test_change_state", "tests/newly_test/additional_resumption/benchmark/test_benchmark_nelder_mead_resumption.py::TestBenchmarkNelderMeadResumption::test_run", "tests/unit/util_test/test_suffix.py::test_suffix", "tests/unit/optimizer_test/test_nelder_mead.py::TestNelderMead::test_inside_contract", "tests/unit/test_parameter.py::TestParameter::test_get_best_parameters", "tests/unit/optimizer_test/test_grid_search.py::test_get_grid_options", "tests/unit/storage/db/test_trial.py::test_get_running", "tests/unit/storage/db/test_trial.py::test_all_delete", "tests/unit/optimizer_test/test_grid_search.py::test_generate_grid_points", "tests/unit/master_test/evaluator_test/test_abstract_evaluator.py::TestAbstractEvaluator::test_init", "tests/unit/optimizer_test/test_abstract_optimizer.py::TestAbstractOptimizer::test__deserialize", "tests/unit/optimizer_test/test_random_search.py::TestRandomOptimizer::test_generate_parameter", "tests/unit/util_test/test_buffer.py::test_Add_lengthover", "tests/unit/storage/db/test_timestamp.py::test_set_any_trial_start_time_exception", "tests/unit/util_test/test_buffer.py::test_Add", "tests/unit/storage/db/test_trial.py::test_all_delete_exception", "tests/unit/storage/db/test_hp.py::test_all_delete", "tests/unit/optimizer_test/test_nelder_mead.py::TestNelderMead::test__create_initial_values", "tests/unit/master_test/evaluator_test/test_maximize_evaluator.py::TestMaximizeEvaluator::test_maximize_evaluator", "tests/unit/util_test/test_trial_id.py::test_string", "tests/unit/storage/db/test_timestamp.py::test_set_any_trial_end_time_exception", "tests/unit/master_test/test_abstract_master.py::TestAbstractMaster::test_pre_process_3", "tests/unit/storage/db/test_hp.py::test_delete_any_trial_params", "tests/unit/util_test/test_buffer.py::test_Clear", "tests/unit/storage/db/test_storage.py::test_is_ready", "tests/unit/storage/db/test_error.py::test_delete_any_trial_error_exception", "tests/unit/optimizer_test/test_grid_search.py::TestGridOptimizer::test_generate_initial_parameter", "tests/unit/storage/db/test_storage.py::test_get_hp_dict_categorical", "tests/unit/test_workspace.py::test_create", "tests/unit/storage/db/test_error.py::test_all_delete_exception", "tests/unit/util_test/test_time.py::test_time", "tests/unit/util_test/test_trial_id.py::test_increment_1", "tests/unit/util_test/test_filesystem.py::test_get_file_result_hp", "tests/unit/storage/db/test_trial.py::test_delete_any_trial_state_exception", "tests/unit/optimizer_test/test_nelder_mead_search.py::TestNelderMeadOptimizer::test_check_result", "tests/unit/optimizer_test/test_nelder_mead.py::TestNelderMead::test_inside_contract_branch", "tests/unit/storage/db/test_variable.py::test_delete_any_trial_variable_exception", "tests/unit/storage/db/test_jobstate.py::test_is_failure", "tests/unit/test_parameter.py::TestParameter::test_get_type", "tests/unit/util_test/test_easy_visualizer.py::test_set_colors", "tests/unit/test_parameter.py::TestParameter::test_get_parameter_list", "tests/unit/optimizer_test/test_nelder_mead.py::TestNelderMead::test_centroid", "tests/unit/storage/db/test_storage.py::test_is_running", "tests/unit/master_test/evaluator_test/test_abstract_evaluator.py::TestAbstractEvaluator::test_evaluate", "tests/unit/optimizer_test/test_tpe_search.py::TestTpeOptimizer::test_post_process", "tests/unit/storage/db/test_storage.py::test_get_ready", "tests/unit/optimizer_test/test_nelder_mead.py::TestNelderMead::test_finalize", "tests/unit/cli/test_report.py::test_report_", "tests/unit/optimizer_test/test_nelder_mead.py::TestNelderMead::test_search", "tests/unit/storage/db/test_jobstate.py::test_delete_any_trial_jobstate", "tests/unit/storage/db/test_hp.py::test_all_delete_exception", "tests/unit/storage/db/test_error.py::test_set_any_trial_error", "tests/unit/storage/db/test_error.py::test_get_error_trial_id", "tests/unit/storage/db/test_hp.py::test_set_any_trial_param", "tests/unit/storage/db/test_jobstate.py::test_get_any_trial_jobstate", "tests/unit/optimizer_test/test_nelder_mead_search.py::TestNelderMeadOptimizer::test_generate_parameter2", "tests/unit/util_test/test_config.py::test_config_not_exists", "tests/unit/storage/db/test_storage.py::test_is_finished", "tests/unit/scheduler_test/algorithm_test/test_abstract_scheduling_algorithm.py::test_abstract_scheduling_algorithm", "tests/unit/storage/db/test_trial.py::test_get_any_trial_state", "tests/unit/util_test/test_retry.py::test_retry", "tests/unit/optimizer_test/test_nelder_mead_search.py::TestNelderMeadOptimizer::test_get_nm_results", "tests/unit/optimizer_test/test_tpe_search.py::test_create_distributions", "tests/unit/master_test/evaluator_test/test_abstract_evaluator.py::TestAbstractEvaluator::test_print", "tests/unit/storage/db/test_variable.py::test_get", "tests/unit/storage/db/test_storage.py::test_get_hp_dict_invalid_type", "tests/unit/util_test/test_filesystem.py::test_get_dict_files", "tests/unit/optimizer_test/test_nelder_mead.py::TestNelderMead::test_outside_contract_branch", "tests/unit/optimizer_test/test_abstract_optimizer.py::TestAbstractOptimizer::test_generate_initial_parameter", "tests/unit/master_test/verification_test/test_abstract_verification.py::TestAbstractVerification::test_save", "tests/unit/optimizer_test/test_nelder_mead.py::TestNelderMead::test_wait_initialize", "tests/unit/util_test/test_trial_id.py::test_initial", "tests/unit/optimizer_test/test_tpe_search.py::TestTpeOptimizer::test_generate_parameter", "tests/unit/storage/db/test_result.py::test_delete_any_trial_objective_exception", "tests/unit/util_test/test_config.py::TestJsonOrYamlObjectConfig::test_init", "tests/unit/optimizer_test/test_tpe_search.py::TestTpeOptimizer::test_deserialize", "tests/unit/storage/db/test_variable.py::test_serialize", "tests/unit/storage/db/test_variable.py::test_delete_any_trial_variable", "tests/unit/util_test/test_config.py::test_empty_if_error", "tests/unit/util_test/test_trial_id.py::test_zero_padding_any_trial_id", "tests/unit/optimizer_test/test_grid_search.py::TestGridOptimizer::test_generate_parameter", "tests/unit/util_test/test_buffer.py::test_Now", "tests/unit/storage/db/test_result.py::test_all_delete", "tests/unit/storage/db/test_jobstate.py::test_delete_any_trial_jobstate_exception", "tests/unit/storage/db/test_jobstate.py::test_set_any_trial_jobstate", "tests/unit/util_test/test_trial_id.py::test_get", "tests/newly_test/additional_resumption/benchmark/test_benchmark_tpe_resumption.py::TestBenchmarkTpeResumption::test_run", "tests/unit/storage/db/test_variable.py::test_delete", "tests/unit/optimizer_test/test_create.py::test_create", "tests/unit/optimizer_test/test_grid_search.py::TestGridOptimizer::test_get_parameter_index", "tests/unit/storage/db/test_jobstate.py::test_get_all_trial_jobstate", "tests/unit/optimizer_test/test_nelder_mead_search.py::TestNelderMeadOptimizer::test__get_all_trial_id", "tests/unit/optimizer_test/test_abstract_optimizer.py::TestAbstractOptimizer::test_cast", "tests/unit/storage/db/test_storage.py::test_get_hp_dict", "tests/unit/storage/db/test_timestamp.py::test_get_any_trial_end_time", "tests/unit/util_test/test_easy_visualizer.py::test_sort", "tests/unit/storage/db/test_storage.py::test_get_num_running", "tests/unit/optimizer_test/test_nelder_mead.py::TestNelderMead::test_wait_shrink", "tests/unit/storage/db/test_storage.py::test_current_max_trial_number", "tests/unit/storage/db/test_storage.py::test_get_num_ready"], "language": "python", "test_command": "source /saved/ENV || source /saved/*/ENV && pytest --no-header -rA --tb=no -p no:cacheprovider --continue-on-collection-errors", "test_output_parser": "python/parse_log_pytest_v3", "image_storage_uri": "vmvm-registry.fbinfra.net/repomate_image_activ_pytest/aistairc_aiaccel:0dfc6be2dbd15101a5d700adb54c827eab814882", "patch": "[\"diff --git a/aiaccel/cli/start.py b/aiaccel/cli/start.py\\nindex f1fef4423..a067c5760 100644\\n--- a/aiaccel/cli/start.py\\n+++ b/aiaccel/cli/start.py\\n@@ -53,12 +53,10 @@ def main() -> None:  # pragma: no cover\\n \\n     logger.info(f\\\"config: {str(pathlib.Path(args.config).resolve())}\\\")\\n \\n-    time_s = time.time()\\n-\\n     Master = create_master(args.config)\\n     Optimizer = create_optimizer(args.config)\\n     Scheduler = create_scheduler(args.config)\\n-    modules = [Master(vars(args)), Scheduler(vars(args)), Optimizer(vars(args))]\\n+    modules = [Master(vars(args)), Optimizer(vars(args)), Scheduler(vars(args))]\\n \\n     sleep_time = config.sleep_time.get()\\n     time_s = time.time()\\n\",\"diff --git a/aiaccel/config.py b/aiaccel/config.py\\nindex 8e86257cc..dd0b79714 100644\\n--- a/aiaccel/config.py\\n+++ b/aiaccel/config.py\\n@@ -330,8 +330,8 @@ def Value(self):\\n _DEFAULT_IS_VERIFIED = False\\n _DEFAULT_VERIFI_CONDITION = []\\n _DEFAULT_RANDOM_SCHESULING = True\\n-_RESOURCE_TYPES = ['abci', 'local']\\n-_GOALS = ['minimize', 'maximize']\\n+_DEFAULT_PYTHON_FILE = \\\"\\\"\\n+_DEFAULT_FUNCTION = \\\"\\\"\\n \\n \\n class Config:\\n@@ -404,6 +404,22 @@ def define_items(self, config: ConfileWrapper, warn: bool):\\n             group=\\\"generic\\\",\\n             keys=(\\\"sleep_time\\\")\\n         )\\n+        self.python_file = ConfigEntry(\\n+            config=config,\\n+            type=[str],\\n+            default=_DEFAULT_PYTHON_FILE,\\n+            warning=False,\\n+            group=\\\"generic\\\",\\n+            keys=(\\\"python_file\\\")\\n+        )\\n+        self.function = ConfigEntry(\\n+            config=config,\\n+            type=[str],\\n+            default=_DEFAULT_FUNCTION,\\n+            warning=False,\\n+            group=\\\"generic\\\",\\n+            keys=(\\\"function\\\")\\n+        )\\n \\n         # === scheduler defalt config===\\n         self.cancel_retry = ConfigEntry(\\n\",\"diff --git a/aiaccel/master/create.py b/aiaccel/master/create.py\\nindex df9a82ca1..27d8cdfda 100644\\n--- a/aiaccel/master/create.py\\n+++ b/aiaccel/master/create.py\\n@@ -3,6 +3,7 @@\\n from aiaccel.config import Config\\n from aiaccel.master.abci_master import AbciMaster\\n from aiaccel.master.local_master import LocalMaster\\n+from aiaccel.master.pylocal_master import PylocalMaster\\n \\n \\n def create_master(config_path: str) -> Any:\\n@@ -15,7 +16,11 @@ def create_master(config_path: str) -> Any:\\n     if resource.lower() == \\\"local\\\":\\n         return LocalMaster\\n \\n+    elif resource.lower() == \\\"python_local\\\":\\n+        return PylocalMaster\\n+\\n     elif resource.lower() == \\\"abci\\\":\\n         return AbciMaster\\n+\\n     else:\\n         return None\\n\",\"diff --git a/aiaccel/master/evaluator/abstract_evaluator.py b/aiaccel/master/evaluator/abstract_evaluator.py\\nindex 8a60143a0..a722c9485 100644\\n--- a/aiaccel/master/evaluator/abstract_evaluator.py\\n+++ b/aiaccel/master/evaluator/abstract_evaluator.py\\n@@ -39,8 +39,7 @@ def evaluate(self) -> None:\\n             None\\n         \\\"\\\"\\\"\\n         best_trial_id, _ = self.storage.get_best_trial(self.goal)\\n-        best_trial_id_str = self.get_zero_padding_any_trial_id(best_trial_id)\\n-        self.hp_result = self.storage.get_hp_dict(best_trial_id_str)\\n+        self.hp_result = self.storage.get_hp_dict(best_trial_id)\\n \\n     def print(self) -> None:\\n         \\\"\\\"\\\"Print current results.\\n\",\"diff --git a/aiaccel/master/pylocal_master.py b/aiaccel/master/pylocal_master.py\\nnew file mode 100644\\nindex 000000000..f3adaf4a0\\n--- /dev/null\\n+++ b/aiaccel/master/pylocal_master.py\\n@@ -0,0 +1,8 @@\\n+from aiaccel.master.abstract_master import AbstractMaster\\n+\\n+\\n+class PylocalMaster(AbstractMaster):\\n+    \\\"\\\"\\\"A master class running on a local computer.\\n+\\n+    \\\"\\\"\\\"\\n+    pass\\n\",\"diff --git a/aiaccel/optimizer/abstract_optimizer.py b/aiaccel/optimizer/abstract_optimizer.py\\nindex 7490e8bc3..d9e9fcca1 100644\\n--- a/aiaccel/optimizer/abstract_optimizer.py\\n+++ b/aiaccel/optimizer/abstract_optimizer.py\\n@@ -111,6 +111,32 @@ def generate_parameter(self) -> list:\\n         \\\"\\\"\\\"\\n         raise NotImplementedError\\n \\n+    def get_pool_size(self) -> int:\\n+\\n+        hp_ready = self.storage.get_num_ready()\\n+        hp_running = self.storage.get_num_running()\\n+        hp_finished = self.storage.get_num_finished()\\n+\\n+        max_pool_size = self.config.num_node.get()\\n+        max_trial_number = self.config.trial_number.get()\\n+\\n+        n1 = max_pool_size - hp_running - hp_ready\\n+        n2 = max_trial_number - hp_finished - hp_running - hp_ready\\n+        pool_size = min(n1, n2)\\n+\\n+        if (pool_size <= 0 or hp_ready >= max_pool_size):\\n+            return 0\\n+\\n+        return pool_size\\n+\\n+    def generate_new_parameter(self) -> list:\\n+        if self.num_of_generated_parameter == 0:\\n+            new_params = self.cast(self.generate_initial_parameter())\\n+        else:\\n+            new_params = self.cast(self.generate_parameter())\\n+\\n+        return new_params\\n+\\n     def pre_process(self) -> None:\\n         \\\"\\\"\\\"Pre-procedure before executing processes.\\n \\n@@ -141,38 +167,29 @@ def inner_loop_main_process(self) -> bool:\\n \\n         self.get_each_state_count()\\n \\n-        _max_pool_size = self.config.num_node.get()\\n-        _max_trial_number = self.config.trial_number.get()\\n-\\n-        n1 = _max_pool_size - self.hp_running - self.hp_ready\\n-        n2 = _max_trial_number - self.hp_finished - self.hp_running - self.hp_ready\\n-        pool_size = min(n1, n2)\\n-\\n-        if (pool_size <= 0 or self.hp_ready >= _max_pool_size):\\n+        pool_size = self.get_pool_size()\\n+        if pool_size <= 0:\\n             return True\\n \\n         self.logger.info(\\n             f'hp_ready: {self.hp_ready}, '\\n             f'hp_running: {self.hp_running}, '\\n             f'hp_finished: {self.hp_finished}, '\\n-            f'total: {_max_trial_number}, '\\n+            f'total: {self.config.trial_number.get()}, '\\n             f'pool_size: {pool_size}'\\n         )\\n \\n-        if self.num_of_generated_parameter == 0:\\n-            new_params = self.cast(self.generate_initial_parameter())\\n-        else:\\n-            new_params = self.cast(self.generate_parameter())\\n+        for _ in range(pool_size):\\n+            new_params = self.generate_new_parameter()\\n+            if new_params is not None and len(new_params) > 0:\\n+                self.register_new_parameters(new_params)\\n \\n-        if new_params is not None and len(new_params) > 0:\\n-            self.register_new_parameters(new_params)\\n+                self.trial_id.increment()\\n+                self._serialize(self.trial_id.integer)\\n \\n-            self.trial_id.increment()\\n-            self._serialize(self.trial_id.integer)\\n-\\n-            if self.all_parameter_generated is True:\\n-                self.logger.info(\\\"All parameter was generated.\\\")\\n-                return False\\n+        if self.all_parameter_generated is True:\\n+            self.logger.info(\\\"All parameter was generated.\\\")\\n+            return False\\n \\n         self.print_dict_state()\\n \\n\",\"diff --git a/aiaccel/optimizer/nelder_mead_optimizer.py b/aiaccel/optimizer/nelder_mead_optimizer.py\\nindex 825f0dcb9..b5545d6ae 100644\\n--- a/aiaccel/optimizer/nelder_mead_optimizer.py\\n+++ b/aiaccel/optimizer/nelder_mead_optimizer.py\\n@@ -219,7 +219,6 @@ def generate_parameter(self) -> None:\\n         new_params = []\\n \\n         if len(self.parameter_pool) == 0:\\n-            self.logger.info('All parameters in pool has been generated.')\\n             return new_params\\n \\n         pool_p = self.parameter_pool.pop(0)\\n\",\"diff --git a/aiaccel/scheduler/abstract_scheduler.py b/aiaccel/scheduler/abstract_scheduler.py\\nindex d78f416f7..ac45934a6 100644\\n--- a/aiaccel/scheduler/abstract_scheduler.py\\n+++ b/aiaccel/scheduler/abstract_scheduler.py\\n@@ -6,6 +6,8 @@\\n from aiaccel.scheduler.algorithm import schedule_sampling\\n from aiaccel.scheduler.job.job_thread import Job\\n from aiaccel.util.logger import str_to_logging_level\\n+from aiaccel.util.filesystem import create_yaml\\n+from aiaccel import dict_result\\n \\n \\n class AbstractScheduler(AbstractModule):\\n@@ -50,6 +52,7 @@ def __init__(self, options: dict) -> None:\\n         self.job_status = {}\\n         self.algorithm = None\\n         self.sleep_time = self.config.sleep_time.get()\\n+        self.num_node = self.config.num_node.get()\\n \\n     def change_state_finished_trials(self) -> None:\\n         \\\"\\\"\\\"Create finished hyper parameter files if result files can be found\\n@@ -133,7 +136,6 @@ def pre_process(self) -> None:\\n         Returns:\\n             None\\n         \\\"\\\"\\\"\\n-        self.trial_id.initial(num=0)\\n         self.set_numpy_random_seed()\\n         self.resume()\\n \\n@@ -295,6 +297,31 @@ def resume(self) -> None:\\n         ):\\n             self._deserialize(self.options['resume'])\\n \\n+    def create_result_file(self, trial_id: int) -> None:\\n+        \\\"\\\"\\\" Save the results in yaml format.\\n+\\n+        Args:\\n+            trial_id (int): Any trial od\\n+\\n+        Returns:\\n+            None\\n+        \\\"\\\"\\\"\\n+\\n+        file_hp_count_fmt = f'%0{self.config.name_length.get()}d'\\n+        file_name = file_hp_count_fmt % trial_id + '.hp'\\n+\\n+        content = self.storage.get_hp_dict(trial_id)\\n+        result = self.storage.result.get_any_trial_objective(trial_id=trial_id)\\n+        error = self.storage.error.get_any_trial_error(trial_id=trial_id)\\n+\\n+        content['result'] = str(result)\\n+\\n+        if error is not None:\\n+            content['error'] = error\\n+\\n+        result_file_path = self.ws / dict_result / file_name\\n+        create_yaml(result_file_path, content)\\n+\\n     def __getstate__(self):\\n         obj = super().__getstate__()\\n         del obj['jobs']\\n\",\"diff --git a/aiaccel/scheduler/create.py b/aiaccel/scheduler/create.py\\nindex 6f7f9fda3..1c892ee91 100644\\n--- a/aiaccel/scheduler/create.py\\n+++ b/aiaccel/scheduler/create.py\\n@@ -3,6 +3,7 @@\\n from aiaccel.config import Config\\n from aiaccel.scheduler.abci_scheduler import AbciScheduler\\n from aiaccel.scheduler.local_scheduler import LocalScheduler\\n+from aiaccel.scheduler.pylocal_scheduler import PylocalScheduler\\n \\n \\n def create_scheduler(config_path: str) -> Any:\\n@@ -12,6 +13,9 @@ def create_scheduler(config_path: str) -> Any:\\n     if resource.lower() == \\\"local\\\":\\n         return LocalScheduler\\n \\n+    elif resource.lower() == \\\"python_local\\\":\\n+        return PylocalScheduler\\n+\\n     elif resource.lower() == \\\"abci\\\":\\n         return AbciScheduler\\n \\n\",\"diff --git a/aiaccel/scheduler/job/job_thread.py b/aiaccel/scheduler/job/job_thread.py\\nindex c70d39a68..2a9bd3bfd 100644\\n--- a/aiaccel/scheduler/job/job_thread.py\\n+++ b/aiaccel/scheduler/job/job_thread.py\\n@@ -511,9 +511,8 @@ def before_runner_create(self, obj: 'Job') -> None:\\n         commands = create_runner_command(\\n             obj.config.job_command.get(),\\n             obj.content,\\n-            str(obj.trial_id),\\n-            obj.config_path,\\n-            obj.options\\n+            obj.trial_id,\\n+            obj.config_path\\n         )\\n \\n         with obj.lock:\\n@@ -727,7 +726,7 @@ def before_finished(self, obj: 'Job') -> None:\\n         with obj.lock:\\n             result = obj.storage.result.get_any_trial_objective(trial_id=obj.trial_id)\\n             error = obj.storage.error.get_any_trial_error(trial_id=obj.trial_id)\\n-            content = obj.storage.get_hp_dict(trial_id_str=obj.trial_id_str)\\n+            content = obj.storage.get_hp_dict(trial_id=obj.trial_id)\\n             content['result'] = result\\n \\n             if error is not None:\\n\",\"diff --git a/aiaccel/scheduler/local_scheduler.py b/aiaccel/scheduler/local_scheduler.py\\nindex ea3aca5de..6f6486d93 100644\\n--- a/aiaccel/scheduler/local_scheduler.py\\n+++ b/aiaccel/scheduler/local_scheduler.py\\n@@ -1,8 +1,12 @@\\n import re\\n+import subprocess\\n+import threading\\n+\\n from typing import Union\\n \\n from aiaccel.scheduler.abstract_scheduler import AbstractScheduler\\n from aiaccel.util.process import ps2joblist\\n+from aiaccel.wrapper_tools import create_runner_command\\n \\n \\n class LocalScheduler(AbstractScheduler):\\n@@ -54,3 +58,48 @@ def parse_trial_id(self, command: str) -> Union[None, str]:\\n             return None\\n \\n         return args[trial_id_index + index_offset]\\n+\\n+    def inner_loop_main_process(self) -> bool:\\n+        \\\"\\\"\\\"A main loop process. This process is repeated every main loop.\\n+\\n+        Returns:\\n+            bool: The process succeeds or not. The main loop exits if failed.\\n+        \\\"\\\"\\\"\\n+\\n+        trial_ids = self.storage.trial.get_ready()\\n+        if trial_ids is None or len(trial_ids) == 0:\\n+            return True\\n+\\n+        for trial_id in trial_ids:\\n+            self._serialize(trial_id)\\n+            if self.num_node > 1:\\n+                th = threading.Thread(target=self.execute, args=(trial_id,))\\n+                th.start()\\n+            else:\\n+                self.execute(trial_id)\\n+\\n+        return True\\n+\\n+    def execute(self, trial_id: int) -> None:\\n+        \\\"\\\"\\\" Generates and executes commands to run user programs.\\n+\\n+        Args:\\n+            trial_id (int): Any trial od\\n+\\n+        Returns:\\n+            None\\n+        \\\"\\\"\\\"\\n+        self.storage.trial.set_any_trial_state(trial_id=trial_id, state='running')\\n+\\n+        runner_command = create_runner_command(\\n+            self.config.job_command.get(),\\n+            self.storage.get_hp_dict(trial_id),\\n+            trial_id,\\n+            self.config_path\\n+        )\\n+        subprocess.run(runner_command)\\n+\\n+        self.storage.trial.set_any_trial_state(trial_id=trial_id, state='finished')\\n+        self.create_result_file(trial_id)\\n+\\n+        return\\n\",\"diff --git a/aiaccel/scheduler/pylocal_scheduler.py b/aiaccel/scheduler/pylocal_scheduler.py\\nnew file mode 100644\\nindex 000000000..f0de072f2\\n--- /dev/null\\n+++ b/aiaccel/scheduler/pylocal_scheduler.py\\n@@ -0,0 +1,91 @@\\n+import threading\\n+import importlib\\n+\\n+from typing import Union\\n+from pathlib import Path\\n+\\n+from aiaccel.scheduler.abstract_scheduler import AbstractScheduler\\n+from aiaccel.util.aiaccel import Run\\n+from aiaccel.util.time_tools import get_time_now\\n+\\n+\\n+class PylocalScheduler(AbstractScheduler):\\n+    \\\"\\\"\\\"A scheduler class running on a local computer.\\n+\\n+    \\\"\\\"\\\"\\n+\\n+    def __init__(self, options: dict) -> None:\\n+        super().__init__(options)\\n+\\n+        self.run = None\\n+        self.user_func = None\\n+\\n+        self.user_func = self.get_callable_object(\\n+            self.config.python_file.get(),\\n+            self.config.function.get()\\n+        )\\n+        self.run = Run(self.config_path)\\n+\\n+    def get_callable_object(self, file_path: Union[str, Path], attr_name: str) -> callable:\\n+        \\\"\\\"\\\" Loads the specified module from the specified python program.\\n+\\n+        Args:\\n+            file_path (str, pathlib.Path): A user program file path.(python file only)\\n+            attr_name (str): A name of objective function in user program.\\n+\\n+        Returns:\\n+            callable\\n+        \\\"\\\"\\\"\\n+        spec = importlib.util.spec_from_file_location(\\\"user_module\\\", file_path)\\n+        module = importlib.util.module_from_spec(spec)\\n+        spec.loader.exec_module(module)\\n+\\n+        return getattr(module, attr_name)\\n+\\n+    def inner_loop_main_process(self) -> bool:\\n+        \\\"\\\"\\\"A main loop process. This process is repeated every main loop.\\n+\\n+        Returns:\\n+            bool: The process succeeds or not. The main loop exits if failed.\\n+        \\\"\\\"\\\"\\n+\\n+        trial_ids = self.storage.trial.get_ready()\\n+        if trial_ids is None or len(trial_ids) == 0:\\n+            return True\\n+\\n+        for trial_id in trial_ids:\\n+            self._serialize(trial_id)\\n+            if self.num_node > 1:\\n+                th = threading.Thread(target=self.execute, args=(trial_id,))\\n+                th.start()\\n+            else:\\n+                self.execute(trial_id)\\n+\\n+        return True\\n+\\n+    def execute(self, trial_id: int) -> None:\\n+        \\\"\\\"\\\" Executes the loaded callable object.\\n+\\n+        Args:\\n+            trial_id (int): Any trial od\\n+\\n+        Returns:\\n+            None\\n+        \\\"\\\"\\\"\\n+        self.storage.trial.set_any_trial_state(trial_id=trial_id, state='running')\\n+\\n+        start_time = get_time_now()\\n+        xs, y, err = self.run.execute(self.user_func, trial_id, y_data_type=None)\\n+        end_time = get_time_now()\\n+        self.run.report(trial_id, xs, y, err, start_time, end_time)\\n+\\n+        self.storage.trial.set_any_trial_state(trial_id=trial_id, state='finished')\\n+        self.create_result_file(trial_id)\\n+\\n+        return\\n+\\n+    def __getstate__(self):\\n+        obj = super().__getstate__()\\n+        del obj['run']\\n+        del obj['user_func']\\n+        return obj\\n\",\"diff --git a/aiaccel/storage/result.py b/aiaccel/storage/result.py\\nindex 67a0f7a1d..1501864c8 100644\\n--- a/aiaccel/storage/result.py\\n+++ b/aiaccel/storage/result.py\\n@@ -33,6 +33,7 @@ def set_any_trial_objective(self, trial_id: int, objective: Any) -> None:\\n                 if data is None:\\n                     new_row = ResultTable(\\n                         trial_id=trial_id,\\n+                        data_type=str(type(objective)),\\n                         objective=objective\\n                     )\\n                     session.add(new_row)\\n\",\"diff --git a/aiaccel/storage/storage.py b/aiaccel/storage/storage.py\\nindex 30eec38f5..29a4ac308 100644\\n--- a/aiaccel/storage/storage.py\\n+++ b/aiaccel/storage/storage.py\\n@@ -120,7 +120,7 @@ def is_finished(self, trial_id: int) -> bool:\\n         \\\"\\\"\\\"\\n         return trial_id in self.trial.get_finished()\\n \\n-    def get_hp_dict(self, trial_id_str: str) -> Union[None, dict]:\\n+    def get_hp_dict(self, trial_id) -> Union[None, dict]:\\n         \\\"\\\"\\\"Obtain information on a specified trial in dict.\\n \\n         Args:\\n@@ -129,7 +129,7 @@ def get_hp_dict(self, trial_id_str: str) -> Union[None, dict]:\\n         Returns:\\n             content(dict): Any trials information\\n         \\\"\\\"\\\"\\n-        trial_id = int(trial_id_str)\\n+\\n         data = self.hp.get_any_trial_params(trial_id=trial_id)\\n         if data is None:\\n             return None\\n@@ -162,7 +162,7 @@ def get_hp_dict(self, trial_id_str: str) -> Union[None, dict]:\\n         error = self.error.get_any_trial_error(trial_id=trial_id)\\n \\n         content = {}\\n-        content['trial_id'] = trial_id_str\\n+        content['trial_id'] = trial_id\\n         content['parameters'] = hp\\n         content['result'] = result\\n         content['start_time'] = start_time\\n@@ -220,7 +220,7 @@ def get_best_trial_dict(self, goal: str) -> dict:\\n             -(dict): Any trials information\\n         \\\"\\\"\\\"\\n         best_trial_id, _ = self.get_best_trial(goal)\\n-        return self.get_hp_dict(str(best_trial_id))\\n+        return self.get_hp_dict(best_trial_id)\\n \\n     def get_result_and_error(self, trial_id: int) -> tuple:\\n         \\\"\\\"\\\"Get results and errors for a given trial number.\\n\",\"diff --git a/aiaccel/util/report.py b/aiaccel/util/report.py\\nindex 3deaacaca..53d242f1d 100644\\n--- a/aiaccel/util/report.py\\n+++ b/aiaccel/util/report.py\\n@@ -38,7 +38,7 @@ def create(self):\\n             return\\n \\n         # write header\\n-        example = self.storage.get_hp_dict(self.get_zero_padding_trial_id(finished[0]))\\n+        example = self.storage.get_hp_dict(finished[0])\\n         header.append('trial_id')\\n         for param in example['parameters']:\\n             header.append(param['parameter_name'])\\n\",\"diff --git a/aiaccel/wrapper_tools.py b/aiaccel/wrapper_tools.py\\nindex ff44d428e..362a13ba7 100644\\n--- a/aiaccel/wrapper_tools.py\\n+++ b/aiaccel/wrapper_tools.py\\n@@ -7,29 +7,25 @@\\n \\n def create_runner_command(\\n     command: str,\\n-    param_content:\\n-    dict,\\n+    param_content: dict,\\n     trial_id: int,\\n-    config_path: str,\\n-    options: dict\\n+    config_path: str\\n ) -> list:\\n \\n     \\\"\\\"\\\"Create a list of command strings to run a hyper parameter.\\n-\\n     Args:\\n         command (str): A string command.\\n         param_content (dict): A hyper parameter content.\\n         trial_id (str): A unique name of a hyper parameter.\\n-\\n     Returns:\\n         A list of command strings.\\n     \\\"\\\"\\\"\\n     commands = re.split(' +', command)\\n     params = param_content['parameters']\\n     commands.append('--trial_id')\\n-    commands.append(trial_id)\\n+    commands.append(str(trial_id))\\n     commands.append('--config')\\n-    commands.append(config_path)\\n+    commands.append(str(config_path))\\n \\n     for param in params:\\n         # Fix a bug related a negative exponential parameters\\n\",\"diff --git a/examples/benchmark/config.yaml b/examples/benchmark/config.yaml\\nindex 565a3eb49..d69e38ec1 100644\\n--- a/examples/benchmark/config.yaml\\n+++ b/examples/benchmark/config.yaml\\n@@ -1,11 +1,15 @@\\n generic:\\n   workspace: \\\"./work\\\"\\n   job_command: \\\"python user.py\\\"\\n+  python_file: \\\"./user.py\\\"\\n+  function: \\\"main\\\"\\n   batch_job_timeout: 600\\n   #sleep_time: 0.01\\n \\n resource:\\n-  type: \\\"local\\\"\\n+  type: \\\"abci\\\"\\n+  #type: \\\"local\\\"\\n+  #type: \\\"python_local\\\"\\n   num_node: 4\\n \\n ABCI:\\n\",\"diff --git a/examples/resnet50_cifar10/config.yaml b/examples/resnet50_cifar10/config.yaml\\nindex 98c223008..d98a77f19 100644\\n--- a/examples/resnet50_cifar10/config.yaml\\n+++ b/examples/resnet50_cifar10/config.yaml\\n@@ -1,10 +1,14 @@\\n generic:\\n   workspace: \\\"./work\\\"\\n   job_command: \\\"python user.py\\\"\\n+  python_file: \\\"./user.py\\\"\\n+  function: \\\"main\\\"\\n   batch_job_timeout: 7200\\n \\n resource:\\n   type: \\\"abci\\\"\\n+  #type: \\\"local\\\"\\n+  #type: \\\"python_local\\\"\\n   num_node: 6\\n \\n ABCI:\\n\",\"diff --git a/examples/schwefel/config.yaml b/examples/schwefel/config.yaml\\nindex da69806f7..7aec10c8d 100644\\n--- a/examples/schwefel/config.yaml\\n+++ b/examples/schwefel/config.yaml\\n@@ -1,10 +1,14 @@\\n generic:\\n   workspace: \\\"./work\\\"\\n   job_command: \\\"python user.py\\\"\\n+  python_file: \\\"./user.py\\\"\\n+  function: \\\"main\\\"\\n   batch_job_timeout: 600\\n \\n resource:\\n   type: \\\"local\\\"\\n+  #type: \\\"local\\\"\\n+  #type: \\\"python_local\\\"\\n   num_node: 4\\n \\n ABCI:\\n\",\"diff --git a/examples/sphere/config.yaml b/examples/sphere/config.yaml\\nindex 90af1582d..bb58dfcc9 100644\\n--- a/examples/sphere/config.yaml\\n+++ b/examples/sphere/config.yaml\\n@@ -1,10 +1,14 @@\\n generic:\\n   workspace: \\\"./work\\\"\\n   job_command: \\\"python user.py\\\"\\n+  python_file: \\\"./user.py\\\"\\n+  function: \\\"main\\\"\\n   batch_job_timeout: 600\\n \\n resource:\\n   type: \\\"local\\\"\\n+  #type: \\\"local\\\"\\n+  #type: \\\"python_local\\\"\\n   num_node: 4\\n \\n ABCI:\\n\"]", "test_patch": "[\"diff --git a/aiaccel/util/aiaccel.py b/aiaccel/util/aiaccel.py\\nindex 9d6e7b742..11494cab3 100644\\n--- a/aiaccel/util/aiaccel.py\\n+++ b/aiaccel/util/aiaccel.py\\n@@ -1,42 +1,15 @@\\n-import argparse\\n import logging\\n-import pathlib\\n import subprocess\\n+from argparse import ArgumentParser\\n from functools import singledispatchmethod\\n-from typing import Any\\n+from logging import StreamHandler, getLogger\\n+from typing import Any, Union\\n+from pathlib import Path\\n \\n-import numpy as np\\n-\\n-import aiaccel\\n from aiaccel.config import Config\\n-from aiaccel.parameter import load_parameter\\n from aiaccel.storage.storage import Storage\\n from aiaccel.util.time_tools import get_time_now\\n \\n-SUPPORTED_TYPES = [\\n-    int,\\n-    float,\\n-    str,\\n-    np.int8,\\n-    np.int16,\\n-    np.int32,\\n-    np.int64,\\n-    np.uint8,\\n-    np.uint16,\\n-    np.uint32,\\n-    np.uint64,\\n-    np.float16,\\n-    np.float32,\\n-    np.float64,\\n-    np.float128,\\n-    np.complex64,\\n-    np.complex128,\\n-    np.complex256,\\n-    np.bool,\\n-    np.unicode,\\n-    np.object\\n-]\\n-\\n \\n class _Message:\\n     \\\"\\\"\\\"\\n@@ -49,9 +22,7 @@ class _Message:\\n     Example:\\n         self.m = Message(\\\"test\\\")\\n         self.m.out(\\\"hogehoge\\\")\\n-\\n         STDOUT: test:hogehoge\\n-\\n         self.parse_result(stdout)\\n             -> return hogehoge\\n     \\\"\\\"\\\"\\n@@ -93,6 +64,7 @@ def out(self, all=False) -> None:\\n \\n     def parse(self, raw_data: str) -> None:\\n         \\\"\\\"\\\"\\n+\\n         Args:\\n             raw_data (str): It is assumed the format\\n             e.g \\\"{label}:{message}\\\".format(\\n@@ -112,6 +84,9 @@ def parse(self, raw_data: str) -> None:\\n             target_data.append(\\\"\\\")\\n         return target_data\\n \\n+    def clear(self):\\n+        self.outputs = []\\n+\\n \\n class Messages:\\n     def __init__(self, *labels: tuple) -> None:\\n@@ -137,6 +112,9 @@ def out(self, label):\\n         \\\"\\\"\\\"\\n         self.d[label].out()\\n \\n+    def clear(self, label):\\n+        self.d[label].clear()\\n+\\n     def parse(self, label, mess):\\n         \\\"\\\"\\\"\\n         Args:\\n@@ -206,281 +184,180 @@ def out(self, objective_y=None, objective_err=None):\\n         self.stdout.out(\\\"objective_y\\\")\\n         self.stdout.out(\\\"objective_err\\\")\\n \\n-\\n-def report(objective_y=None, objective_err=None):\\n-    \\\"\\\"\\\" user side reporting function\\n-\\n-    Examples:\\n-        import from aiaccel.util import aiaccel\\n-\\n-        result = 0.0\\n-        opt.report(result)\\n-    \\\"\\\"\\\"\\n-    WrapperInterface().out(objective_y, objective_err)\\n+        self.stdout.clear(\\\"objective_y\\\")\\n+        self.stdout.clear(\\\"objective_err\\\")\\n \\n \\n class Run:\\n-    \\\"\\\"\\\"\\n-        It is assumed to refer to the user program\\n-    \\\"\\\"\\\"\\n-\\n-    def __init__(self) -> None:\\n+    def __init__(self, config_path: Union[str, Path, None] = None):\\n+        parser = ArgumentParser()\\n+        parser.add_argument('--config', type=str)\\n+        parser.add_argument('--trial_id', type=str, required=False)\\n \\n-        parser = argparse.ArgumentParser()\\n-        parser.add_argument('-i', '--trial_id', type=str, required=False)\\n-        parser.add_argument('-c', '--config', type=str, required=False)\\n+        args = parser.parse_known_args()[0]\\n \\n-        self.args = vars(parser.parse_known_args()[0])\\n+        self.args = vars(args)\\n+        self.trial_id = self.args[\\\"trial_id\\\"]\\n \\n-        self.xs = {}\\n-        self.ys = None\\n-        self.err = \\\"\\\"\\n+        if config_path is not None:\\n+            self.config_path = config_path\\n+            if type(self.config_path) == str:\\n+                self.config_path = Path(self.config_path).resolve()\\n+        else:\\n+            self.config_path = Path(self.args[\\\"config\\\"])\\n \\n-        self.trial_id = self.args[\\\"trial_id\\\"]\\n-        self.config = None\\n-        self.storage = None\\n-        if self.args[\\\"config\\\"] is not None:\\n-            self.config_path = pathlib.Path(self.args[\\\"config\\\"])\\n-            self.config = Config(self.config_path)\\n-\\n-            # create paths\\n-            # self.workspace = pathlib.Path(self.config.workspace.get())\\n-            self.workspace = pathlib.Path(self.config.workspace.get()).resolve()\\n-            self.dict_lock = self.workspace / aiaccel.dict_lock\\n-\\n-            # create database\\n-            self.storage = Storage(self.workspace)\\n-\\n-        parameters_config = load_parameter(self.config.hyperparameters.get())\\n-        for p in parameters_config.get_parameter_list():\\n-            type_func = str\\n-            if p.type == \\\"FLOAT\\\":\\n-                type_func = float\\n-            elif p.type == \\\"INT\\\":\\n-                type_func = int\\n-            # TODO Fix\\n-            # elif p.type == \\\"ORDINAL\\\":\\n-            #     type_func = float\\n-            parser.add_argument(f\\\"--{p.name}\\\", type=type_func)\\n-        # reparse arguments and load parameters\\n-        self.args = vars(parser.parse_args())\\n-        for p in parameters_config.get_parameter_list():\\n-            self.xs[p.name] = self.args[p.name]\\n+        self.config = Config(self.config_path)\\n+        self.workspace = Path(self.config.workspace.get()).resolve()\\n+        self.storage = Storage(self.workspace)\\n \\n         # logger\\n         log_dir = self.workspace / \\\"log\\\"\\n-        self.log_path = log_dir / f\\\"job_{self.trial_id}.log\\\"\\n+        log_path = log_dir / f\\\"job_{self.trial_id}.log\\\"\\n         if not log_dir.exists():\\n             log_dir.mkdir(parents=True)\\n-        logging.basicConfig(\\n-            filename=self.log_path,\\n-            level=logging.DEBUG\\n-        )\\n-\\n-        # t variables\\n-        self.start_time = None\\n-        self.end_time = None\\n+        logging.basicConfig(filename=log_path, level=logging.DEBUG)\\n+        self.logger = getLogger(__name__)\\n+        self.logger.addHandler(StreamHandler())\\n \\n         self.com = WrapperInterface()\\n \\n-    # @property\\n-    # def trial_id(self) -> str:\\n-    #     \\\"\\\"\\\" Get tha trial_id of this trial.\\n-\\n-    #     Returns:\\n-    #         index (str): trial_id of this trial.\\n-    #     \\\"\\\"\\\"\\n-    #     return self.index\\n-\\n-    @property\\n-    def parameters(self) -> dict:\\n-        \\\"\\\"\\\" Get parameters dictionary.\\n-\\n-        Returns\\n-            xs (dict): Parameters dictionary.\\n-        \\\"\\\"\\\"\\n-        return self.xs\\n-\\n-    @property\\n-    def objective(self) -> Any:\\n-        \\\"\\\"\\\" Get the objective value.\\n-\\n-        Returns\\n-            y (Any): Objective value.\\n-        \\\"\\\"\\\"\\n-        return self.ys\\n-\\n-    @property\\n-    def error(self):\\n-        \\\"\\\"\\\" Get the error message from user program.\\n-\\n-        Returns:\\n-            str: error message.\\n-        \\\"\\\"\\\"\\n-        return self.err\\n-\\n-    def exist_error(self) -> bool:\\n-        \\\"\\\"\\\" Return True if exist error else False.\\n-\\n-        Returns:\\n-            bool:   True : There is an error.\\n-                    False: There is no error.\\n-        \\\"\\\"\\\"\\n-        if self.err is None:\\n-            return False\\n-        if self.err != \\\"\\\":\\n-            return True\\n-        return False\\n-\\n-    def trial_stop(self) -> None:\\n-        \\\"\\\"\\\" Enforce an error to stop this trial.\\n-        \\\"\\\"\\\"\\n-        if self.exist_error():\\n-            pass\\n-        else:\\n-            self.set_error(\\\"Faital error\\\")\\n-        self.report(float('nan'))\\n-\\n-    def set_error(self, mess: str) -> None:\\n-        \\\"\\\"\\\" Set any error message.\\n-        \\\"\\\"\\\"\\n-        self.err = mess\\n-\\n-    def _generate_commands(self, command: str, auto_args) -> list:\\n+    def generate_commands(self, command: str, xs: list) -> list:\\n         \\\"\\\"\\\" Generate execution command of user program.\\n \\n         Returns:\\n             list: execution command.\\n         \\\"\\\"\\\"\\n         commands = command.split(\\\" \\\")\\n-        if not auto_args:\\n-            return commands\\n-\\n         commands.append(f\\\"--config={str(self.config_path)}\\\")\\n         commands.append(f\\\"--trial_id={self.trial_id}\\\")\\n \\n-        for key in self.xs:\\n+        for key in xs:\\n             name = key\\n-            value = self.xs[key]\\n+            value = xs[key]\\n             if value is not None:\\n                 command = f\\\"--{name}={value}\\\"\\n                 commands.append(command)\\n \\n         return commands\\n \\n+    def get_any_trial_xs(self, trial_id: int) -> dict:\\n+        params = self.storage.hp.get_any_trial_params(trial_id=trial_id)\\n+        if params is None:\\n+            return\\n+\\n+        xs = {}\\n+        for param in params:\\n+            cast = eval(param.param_type.lower())\\n+            xs[param.param_name] = cast(param.param_value)\\n+\\n+        return xs\\n+\\n+    def cast_y(self, y_value: any, y_data_type: Union[None, str]):\\n+        if y_data_type is None:\\n+            y = y_value\\n+        elif y_data_type.lower() == 'float':\\n+            y = float(y_value)\\n+        elif y_data_type.lower() == 'int':\\n+            y = int(float(y_value))\\n+        elif y_data_type.lower() == 'str':\\n+            y = str(y_value)\\n+        else:\\n+            TypeError(f'{y_data_type} cannot be specified')\\n+\\n+        return y\\n+\\n     @singledispatchmethod\\n-    def execute(self, func: callable):\\n+    def execute(self, func: callable, trial_id: int, y_data_type: Union[None, str]) -> tuple:\\n         \\\"\\\"\\\" Execution the target function.\\n \\n         Return:\\n             Objective value.\\n         \\\"\\\"\\\"\\n-        self.start_time = get_time_now()\\n+\\n+        xs = self.get_any_trial_xs(trial_id)\\n+        y = None\\n+        err = \\\"\\\"\\n \\n         try:\\n-            self.ys = func(self.xs)\\n+            y = self.cast_y(func(xs), y_data_type)\\n         except BaseException as e:\\n-            self.err = str(e)\\n+            err = str(e)\\n         finally:\\n-            self.end_time = get_time_now()\\n-            self.com.out(objective_y=self.ys, objective_err=self.err)\\n-\\n-        # stdout\\n+            self.com.out(objective_y=y, objective_err=err)\\n \\n-        return self.ys\\n+        return xs, y, err\\n \\n     @execute.register\\n-    def _(self, command: str, auto_args: bool = True):\\n+    def _(self, command: str, trial_id: int, y_data_type: Union[None, str]) -> tuple:\\n         \\\"\\\"\\\" Execution the user program.\\n \\n         Returns:\\n             ys (list): This is a list of the return values of the user program.\\n         \\\"\\\"\\\"\\n \\n+        xs = self.get_any_trial_xs(trial_id)\\n+        err = \\\"\\\"\\n+        y = None\\n+\\n         # Make running command of user program\\n         if command == \\\"\\\":\\n-            self.set_error(\\\"Invalid execute command\\\")\\n-            logging.debug(f\\\"execute(err): {self.err}\\\")\\n-            self.ys = [float(\\\"nan\\\")]\\n-            return self.ys\\n-\\n-        commands = self._generate_commands(command, auto_args)\\n-        logging.debug(f\\\"command: {commands}\\\")\\n+            y = [float(\\\"nan\\\")]\\n+            return xs, y, err\\n \\n-        self.start_time = get_time_now()\\n-        logging.debug(f\\\"start time: {self.start_time}\\\")\\n+        commands = self.generate_commands(command, xs)\\n \\n         output = subprocess.run(\\n             commands,\\n             stdout=subprocess.PIPE,\\n             stderr=subprocess.PIPE,\\n         )\\n-        ys, err = self.com.get_data(output)\\n-        self.ys = float(ys[0])  # todo: do refactoring\\n-        self.err = (\\\"\\\\n\\\").join(err)\\n-        logging.debug(f\\\"execute(out): {self.ys}\\\")\\n-        logging.debug(f\\\"execute(err): {self.err}\\\")\\n-\\n-        self.end_time = get_time_now()\\n-        logging.debug(f\\\"end time: {self.end_time}\\\")\\n-\\n-        return self.ys\\n \\n-    def report(self, y):\\n-        \\\"\\\"\\\" Write the result in yaml format to the result directory.\\n-\\n-        Args:\\n-            y (Union): Objective value. (return values of the user program.)\\n-        \\\"\\\"\\\"\\n-        if (\\n-            self.args[\\\"trial_id\\\"] is None or\\n-            self.args[\\\"config\\\"] is None\\n-        ):\\n-            return\\n-\\n-        err = self.err\\n-        if not type(y) in SUPPORTED_TYPES:\\n-            y = float(\\\"nan\\\")\\n-            err = f\\\"user function returns invalid type value, {type(y)}({y}).\\\"\\n-\\n-        self.storage.result.set_any_trial_objective(\\n-            trial_id=int(self.trial_id),\\n-            objective=y\\n-        )\\n-        self.storage.timestamp.set_any_trial_start_time(\\n-            trial_id=int(self.trial_id),\\n-            start_time=self.start_time\\n-        )\\n-        self.storage.timestamp.set_any_trial_end_time(\\n-            trial_id=int(self.trial_id),\\n-            end_time=self.end_time\\n-        )\\n+        ys, err = self.com.get_data(output)\\n+        if y_data_type is None:\\n+            y = self.cast_y(ys[0], 'float')\\n+        else:\\n+            y = self.cast_y(ys[0], y_data_type)\\n+        err = (\\\"\\\\n\\\").join(err)\\n \\n-        if err != \\\"\\\":\\n-            self.storage.error.set_any_trial_error(\\n-                trial_id=int(self.trial_id),\\n-                error_message=err\\n-            )\\n+        return xs, y, err\\n \\n     @singledispatchmethod\\n-    def execute_and_report(self, func: callable):\\n+    def execute_and_report(self, func: callable, y_data_type: Union[None, str] = None):\\n         \\\"\\\"\\\"\\n+\\n         Examples:\\n             def obj(p)\\n                 y = p[\\\"x1\\\"]\\n                 return y\\n-\\n             run = aiaccel.Run()\\n             run.execute_and_report(obj)\\n         \\\"\\\"\\\"\\n-        self.report(self.execute(func))\\n+        start_time = get_time_now()\\n+        xs, y, err = self.execute(func, self.trial_id, y_data_type)\\n+        end_time = get_time_now()\\n+\\n+        self.report(self.trial_id, xs, y, err, start_time, end_time)\\n \\n     @execute_and_report.register\\n-    def _(self, command: str, auto_args: bool = True):\\n+    def _(self, command: str, y_data_type: Union[None, str] = None):\\n         \\\"\\\"\\\"\\n+\\n         Examples:\\n             run = aiaccel.Run()\\n             p = run.parameters\\n             run.execute_and_report(f\\\"echo {p['x1']}\\\", False)\\n         \\\"\\\"\\\"\\n-        self.report(self.execute(command, auto_args))\\n+        start_time = get_time_now()\\n+        xs, y, err = self.execute(command, self.trial_id, y_data_type)\\n+        end_time = get_time_now()\\n+\\n+        self.report(self.trial_id, xs, y, err, start_time, end_time)\\n+\\n+    def report(self, trial_id: int, xs: dict, y: any, err: str, start_time: str, end_time: str) -> None:\\n+        \\\"\\\"\\\" Write the result in yaml format to the result directory.\\n+        \\\"\\\"\\\"\\n+        self.storage.result.set_any_trial_objective(trial_id, y)\\n+        self.storage.timestamp.set_any_trial_start_time(trial_id, start_time)\\n+        self.storage.timestamp.set_any_trial_end_time(trial_id, end_time)\\n+        if err != \\\"\\\":\\n+            self.storage.error.set_any_trial_error(trial_id, err)\\n\",\"diff --git a/tests/unit/abci_test/test_abci_batch.py b/tests/unit/abci_test/test_abci_batch.py\\nindex 9806ea08c..70723c3b6 100644\\n--- a/tests/unit/abci_test/test_abci_batch.py\\n+++ b/tests/unit/abci_test/test_abci_batch.py\\n@@ -24,18 +24,11 @@ def test_create_abci_batch_file(\\n         config = load_test_config()\\n         dict_lock = work_dir.joinpath('lock')\\n         batch_file = work_dir.joinpath('runner', 'run_test.sh')\\n-        options = {\\n-            'config': str(self.config_json),\\n-            'resume': None,\\n-            'clean': False,\\n-            'fs': False,\\n-        }\\n         commands = create_runner_command(\\n             config.job_command.get(),\\n             get_one_parameter(),\\n             'test',\\n-            'config.json',\\n-            options\\n+            'config.json'\\n         )\\n         wrapper_file = data_dir.joinpath(config.job_script_preamble.get())\\n         create_abci_batch_file(batch_file, wrapper_file, commands, dict_lock)\\n\",\"diff --git a/tests/unit/optimizer_test/test_abstract_optimizer.py b/tests/unit/optimizer_test/test_abstract_optimizer.py\\nindex 20bff7115..ac87721dc 100644\\n--- a/tests/unit/optimizer_test/test_abstract_optimizer.py\\n+++ b/tests/unit/optimizer_test/test_abstract_optimizer.py\\n@@ -91,15 +91,16 @@ def dummy_serialize(trial_id):\\n             with patch.object(self.optimizer, 'generate_parameter', return_value=None):\\n                 assert self.optimizer.inner_loop_main_process() is True\\n \\n-        with patch.object(self.optimizer, 'num_of_generated_parameter', 1):\\n-            with patch.object(self.optimizer, 'generate_parameter', return_value=param):\\n-                with patch.object(self.optimizer, 'register_new_parameters', dummy_register_new_parameters):\\n-                    with patch.object(self.optimizer.trial_id, 'increment', dummy_increment):\\n-                        with patch.object(self.optimizer, '_serialize', dummy_serialize):\\n-                            with patch.object(self.optimizer, 'all_parameter_generated', False):\\n-                                assert self.optimizer.inner_loop_main_process() is True\\n-                            with patch.object(self.optimizer, 'all_parameter_generated', True):\\n-                                assert self.optimizer.inner_loop_main_process() is False\\n+        with patch.object(self.optimizer, 'get_pool_size', return_value=1):\\n+            with patch.object(self.optimizer, 'num_of_generated_parameter', 1):\\n+                with patch.object(self.optimizer, 'generate_parameter', return_value=param):\\n+                    with patch.object(self.optimizer, 'register_new_parameters', dummy_register_new_parameters):\\n+                        with patch.object(self.optimizer.trial_id, 'increment', dummy_increment):\\n+                            with patch.object(self.optimizer, '_serialize', dummy_serialize):\\n+                                with patch.object(self.optimizer, 'all_parameter_generated', False):\\n+                                    assert self.optimizer.inner_loop_main_process() is True\\n+                                with patch.object(self.optimizer, 'all_parameter_generated', True):\\n+                                    assert self.optimizer.inner_loop_main_process() is False\\n \\n     def test__serialize(self):\\n         self.optimizer._rng = np.random.RandomState(0)\\n\",\"diff --git a/tests/unit/optimizer_test/test_nelder_mead.py b/tests/unit/optimizer_test/test_nelder_mead.py\\nindex 8bf137561..db47b08c5 100644\\n--- a/tests/unit/optimizer_test/test_nelder_mead.py\\n+++ b/tests/unit/optimizer_test/test_nelder_mead.py\\n@@ -108,7 +108,7 @@ def test_pop_result(self, clean_work_dir, setup_hp_finished, work_dir):\\n         #\\n         print(storage.get_finished())\\n         print(storage.result.get_all_result())\\n-        c = storage.get_hp_dict(trial_id_str='000')\\n+        c = storage.get_hp_dict(trial_id=0)\\n         assert c is not None\\n \\n         param = copy.copy(params[[p['vertex_id'] for p in params].index('001')])\\n\",\"diff --git a/tests/unit/storage/db/test_storage.py b/tests/unit/storage/db/test_storage.py\\nindex 8a2f9c50b..ad9feb204 100644\\n--- a/tests/unit/storage/db/test_storage.py\\n+++ b/tests/unit/storage/db/test_storage.py\\n@@ -511,7 +511,7 @@ def test_get_best_trial_dict():\\n     )\\n \\n     exp = {\\n-        'trial_id': str(trial_id),\\n+        'trial_id': trial_id,\\n         'parameters': [\\n             {\\n                 \\\"parameter_name\\\": param_name,\\n\",\"diff --git a/tests/unit/test_wrapper_tools.py b/tests/unit/test_wrapper_tools.py\\nindex 4e639ee14..58a419df8 100644\\n--- a/tests/unit/test_wrapper_tools.py\\n+++ b/tests/unit/test_wrapper_tools.py\\n@@ -33,8 +33,7 @@ def test_create_runner_command(\\n             config.job_command.get(),\\n             get_one_parameter(),\\n             'name',\\n-            'config.json',\\n-            options\\n+            'config.json'\\n         )\\n         assert commands[0] == 'python'\\n         assert commands[1] == 'original_main.py'\\n\",\"diff --git a/tests/unit/util_test/test_aiaccel.py b/tests/unit/util_test/test_aiaccel.py\\nindex e4ca316a2..e1b26ca21 100644\\n--- a/tests/unit/util_test/test_aiaccel.py\\n+++ b/tests/unit/util_test/test_aiaccel.py\\n@@ -1,19 +1,9 @@\\n-import argparse\\n-import pathlib\\n-import subprocess\\n import sys\\n-from subprocess import CompletedProcess\\n-from textwrap import wrap\\n-from unittest import mock\\n from unittest.mock import patch\\n \\n-import aiaccel\\n import numpy as np\\n-from aiaccel.storage.storage import Storage\\n-# from aiaccel.util.opt import Wrapper\\n-from aiaccel.util.aiaccel import Messages, Run, WrapperInterface, report\\n+from aiaccel.util.aiaccel import Messages, Run, WrapperInterface\\n \\n-# from aiaccel.util.opt import create_objective\\n from tests.base_test import BaseTest\\n \\n \\n@@ -114,88 +104,6 @@ def test_trial_id(self):\\n             run = Run()\\n             assert run.trial_id == \\\"0001\\\"\\n \\n-    # test module parameters\\n-    def test_parameters(self):\\n-        with patch.object(sys, 'argv', self.get_test_args()):\\n-            run = Run()\\n-            assert run.parameters[\\\"x1\\\"] == 0.1\\n-            assert run.parameters[\\\"x2\\\"] == 0.2\\n-            assert run.parameters[\\\"x3\\\"] == 0.3\\n-            assert run.parameters[\\\"x4\\\"] == 0.4\\n-            assert run.parameters[\\\"x5\\\"] == 0.5\\n-            assert run.parameters[\\\"x6\\\"] == 0.6\\n-            assert run.parameters[\\\"x7\\\"] == 0.7\\n-            assert run.parameters[\\\"x8\\\"] == 0.8\\n-            assert run.parameters[\\\"x9\\\"] == 0.9\\n-            assert run.parameters[\\\"x10\\\"] == 1.0\\n-\\n-    # test module: objective\\n-    def test_objective(self):\\n-        with patch.object(sys, 'argv', self.get_test_args()):\\n-            run = Run()\\n-            assert run.objective is None\\n-            run.execute_and_report(main)\\n-            assert run.objective is not None\\n-\\n-            assert run.execute_and_report(invalid_func) is None\\n-\\n-            with patch.object(run, \\\"args\\\", {'trial_id': 1, 'config':None}):\\n-                assert run.execute_and_report(invalid_func) is None\\n-\\n-    # test module: exist_error\\n-    def test_exist_error(self):\\n-        with patch.object(sys, 'argv', self.get_test_args()):\\n-            run = Run()\\n-            with patch.object(run, \\\"err\\\", \\\"\\\"):\\n-                assert run.exist_error() is False\\n-\\n-            with patch.object(run, \\\"err\\\", \\\"hoge\\\"):\\n-                assert run.exist_error() is True\\n-\\n-            with patch.object(run, \\\"err\\\", None):\\n-                assert run.exist_error() is False\\n-\\n-    # test module: trial_stop\\n-    def test_trial_stop(self):\\n-        with patch.object(sys, 'argv', self.get_test_args()):\\n-            run = Run()\\n-            with patch.object(run, \\\"err\\\", \\\"has any error\\\"):\\n-                assert run.trial_stop() is None\\n-            \\n-            with patch.object(run, \\\"err\\\", \\\"\\\"):\\n-                assert run.trial_stop() is None\\n-\\n-    # test module: set_error\\n-    def test_set_error(self):\\n-        # for i in range(10):\\n-        #     self.storage.result.set_any_trial_objective(\\n-        #         trial_id=i,\\n-        #         objective=0.0\\n-        #     )\\n-        # self.storage.result.all_delete()\\n-        # self.storage.timestamp.all_delete()\\n-        # self.storage.errors.all_delete()\\n-        with patch.object(sys, 'argv', self.get_test_args()):\\n-            run = Run()\\n-            run.index = '0001'\\n-            run.set_error(\\\"test error\\\")\\n-            assert run.err == \\\"test error\\\"\\n-            assert run.error == \\\"test error\\\"\\n-\\n-    # excute\\n-    def test_excute(self):\\n-        # success\\n-        \\\"\\\"\\\"\\n-            objective_y:42\\n-            ny_data_type:int\\n-            nobjective_error:\\n-        \\\"\\\"\\\"\\n-        with patch.object(sys, 'argv', self.get_test_args()):\\n-            run = Run()\\n-            y = run.execute(main)\\n-            assert y == 3.85\\n-\\n-\\n #\\n # Wrapper Interface\\n #\\n@@ -203,8 +111,3 @@ def wrapper_interface():\\n     wrp = WrapperInterface()\\n     assert wrp.get_data('objective_y: objective_err:') == (None, None)\\n     assert wrp.get_data('objective_y:123 objective_err:err') == ('123', 'err')\\n-\\n-def test_report():\\n-    assert report(objective_y=123, objective_err='error') is None\\n-    assert report(objective_y=[123], objective_err='') is None\\n-\"]", "hints_text": ""}
