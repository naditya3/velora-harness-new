{"instance_id": "1032336705612715", "repo": "cdonnerer/xgboost-distribution", "base_commit": "a5ac7a04b17e91e5e262414ca34df89ca58b7daa", "problem_statement": "Sample Weight Support?:\\nIs there any plan to implement sample weights here? I see they're explicitly disabled currently. Is there any ability to fix?", "FAIL_TO_PASS": ["tests/distributions/test_base.py::test_distribution_loss_shape[log-normal]", "tests/distributions/test_base.py::test_distribution_loss_shape[exponential]", "tests/distributions/test_base.py::test_distribution_loss_shape[poisson]", "tests/distributions/test_base.py::test_distribution_loss_shape[normal]", "tests/distributions/test_base.py::test_distribution_loss_shape[negative-binomial]", "tests/distributions/test_base.py::test_distribution_loss_shape[laplace]"], "PASS_TO_PASS": ["tests/distributions/test_log_normal.py::test_loss", "tests/distributions/test_laplace.py::test_loss", "tests/distributions/test_utils.py::test_check_is_ge_zero[x0]", "tests/distributions/test_poisson.py::test_target_validation_raises[invalid_target1]", "tests/distributions/test_utils.py::test_check_is_gt_zero_raises[x0]", "tests/distributions/test_utils.py::test_check_is_ge_zero_raises[x1]", "tests/distributions/test_utils.py::test_check_is_integer_raises[x0]", "tests/distributions/test_utils.py::test_check_is_gt_zero[x1]", "tests/distributions/test_base.py::test_format_distribution_name[NegativeBinomial-negative-binomial]", "tests/test_metrics.py::test_get_ll_score_func_distribution_exist", "tests/distributions/test_utils.py::test_check_is_integer_raises[x1]", "tests/distributions/test_log_normal.py::test_target_validation_raises[invalid_target0]", "tests/test_model.py::test_fit_1d_X_fit_fails", "tests/test_model.py::test_objective_and_evaluation_funcs_callable[log-normal]", "tests/test_model.py::test_objective_and_evaluation_funcs_callable[normal]", "tests/distributions/test_exponential.py::test_target_validation_raises[invalid_target0]", "tests/distributions/test_poisson.py::test_target_validation", "tests/distributions/test_base.py::test_get_distribution", "tests/test_model.py::test_objective_and_evaluation_funcs_callable[poisson]", "tests/test_model.py::test_predict_before_fit_fails", "tests/distributions/test_poisson.py::test_target_validation_raises[invalid_target0]", "tests/distributions/test_utils.py::test_check_is_ge_zero[x1]", "tests/test_model.py::test_setting_objective_in_init_fails", "tests/distributions/test_negative_binomial.py::test_target_validation_raises[invalid_target1]", "tests/distributions/test_normal.py::test_gradient_calculation[y1-params1-False-expected_grad1]", "tests/distributions/test_laplace.py::test_gradient_calculation[y1-params1-False-expected_grad1]", "tests/test_model.py::test_objective_and_evaluation_funcs_callable[negative-binomial]", "tests/test_model.py::test_objective_and_evaluation_funcs_callable[exponential]", "tests/distributions/test_exponential.py::test_loss", "tests/distributions/test_log_normal.py::test_target_validation_raises[invalid_target1]", "tests/distributions/test_utils.py::test_check_is_gt_zero[x0]", "tests/distributions/test_utils.py::test_check_is_integer[x1]", "tests/distributions/test_poisson.py::test_loss", "tests/distributions/test_exponential.py::test_target_validation", "tests/distributions/test_base.py::test_get_distribution_raises", "tests/distributions/test_negative_binomial.py::test_target_validation", "tests/distributions/test_utils.py::test_check_is_gt_zero_raises[x1]", "tests/distributions/test_base.py::test_format_distribution_name[LogNormal-log-normal]", "tests/distributions/test_log_normal.py::test_target_validation", "tests/distributions/test_negative_binomial.py::test_target_validation_raises[invalid_target0]", "tests/distributions/test_exponential.py::test_gradient_calculation[y1-params1-False-expected_grad1]", "tests/distributions/test_log_normal.py::test_gradient_calculation[y1-params1-False-expected_grad1]", "tests/test_model.py::test_objective_and_evaluation_funcs_callable[laplace]", "tests/distributions/test_utils.py::test_check_is_ge_zero_raises[x0]", "tests/distributions/test_poisson.py::test_gradient_calculation[y1-params1-False-expected_grad1]", "tests/distributions/test_base.py::test_get_distribution_doc", "tests/distributions/test_exponential.py::test_target_validation_raises[invalid_target1]", "tests/distributions/test_utils.py::test_check_is_integer[x0]"], "language": "python", "test_command": "source /saved/ENV || source /saved/*/ENV && tox -- --no-header -rA --tb=no -p no:cacheprovider --continue-on-collection-errors", "test_output_parser": "python/parse_log_pytest_v3", "image_storage_uri": "vmvm-registry.fbinfra.net/repomate_image_activ_pytest/cdonnerer_xgboost-distribution:a5ac7a04b17e91e5e262414ca34df89ca58b7daa", "patch": "[\"diff --git a/src/xgboost_distribution/model.py b/src/xgboost_distribution/model.py\\nindex 273dd59..0edd5c1 100644\\n--- a/src/xgboost_distribution/model.py\\n+++ b/src/xgboost_distribution/model.py\\n@@ -242,6 +242,7 @@ def obj(params: np.ndarray, data: DMatrix) -> Tuple[np.ndarray, np.ndarray]:\\n             grad, hess = self._distribution.gradient_and_hessian(\\n                 y=y, params=params, natural_gradient=self.natural_gradient\\n             )\\n+            # sample weights should apply to grad and hess here\\n             return grad.flatten(), hess.flatten()\\n \\n         return obj\\n@@ -249,6 +250,7 @@ def obj(params: np.ndarray, data: DMatrix) -> Tuple[np.ndarray, np.ndarray]:\\n     def _evaluation_func(self) -> Callable[[np.ndarray, DMatrix], Tuple[str, float]]:\\n         def feval(params: np.ndarray, data: DMatrix) -> Tuple[str, float]:\\n             y = data.get_label()\\n+            # sample weights should give us a weighted mean here\\n             return self._distribution.loss(y=y, params=params)\\n \\n         return feval\\n\",\"diff --git a/src/xgboost_distribution/distributions/normal.py b/src/xgboost_distribution/distributions/normal.py\\nindex fa06a04..19a4e19 100644\\n--- a/src/xgboost_distribution/distributions/normal.py\\n+++ b/src/xgboost_distribution/distributions/normal.py\\n@@ -81,7 +81,8 @@ def gradient_and_hessian(self, y, params, natural_gradient=True):\\n \\n     def loss(self, y, params):\\n         loc, scale = self.predict(params)\\n-        return \\\"NormalDistribution-NLL\\\", -norm.logpdf(y, loc=loc, scale=scale).mean()\\n+        return -norm.logpdf(y, loc=loc, scale=scale)\\n+        # return \\\"NormalDistribution-NLL\\\", -norm.logpdf(y, loc=loc, scale=scale).mean()\\n \\n     def predict(self, params):\\n         loc, log_scale = self._split_params(params)\\n\",\"diff --git a/src/xgboost_distribution/model.py b/src/xgboost_distribution/model.py\\nindex 0edd5c1..28cd4f3 100644\\n--- a/src/xgboost_distribution/model.py\\n+++ b/src/xgboost_distribution/model.py\\n@@ -119,11 +119,11 @@ def fit(\\n         else:\\n             base_margin_eval_set = None\\n \\n-        for param in [sample_weight, sample_weight_eval_set]:\\n-            if param is not None:\\n-                raise NotImplementedError(\\n-                    \\\"Sample weights are currently not supported by XGBDistribution!\\\"\\n-                )\\n+        # for param in [sample_weight, sample_weight_eval_set]:\\n+        #     if param is not None:\\n+        #         raise NotImplementedError(\\n+        #             \\\"Sample weights are currently not supported by XGBDistribution!\\\"\\n+        #         )\\n \\n         train_dmatrix, evals = _wrap_evaluation_matrices(\\n             missing=self.missing,\\n@@ -242,7 +242,14 @@ def obj(params: np.ndarray, data: DMatrix) -> Tuple[np.ndarray, np.ndarray]:\\n             grad, hess = self._distribution.gradient_and_hessian(\\n                 y=y, params=params, natural_gradient=self.natural_gradient\\n             )\\n-            # sample weights should apply to grad and hess here\\n+\\n+            weights = data.get_weight()\\n+            if weights.size != 0:\\n+                weights = weights.reshape(-1, 1)\\n+                # TODO: what if some weights go to zero?\\n+                grad *= weights\\n+                hess *= weights\\n+\\n             return grad.flatten(), hess.flatten()\\n \\n         return obj\\n@@ -250,8 +257,19 @@ def obj(params: np.ndarray, data: DMatrix) -> Tuple[np.ndarray, np.ndarray]:\\n     def _evaluation_func(self) -> Callable[[np.ndarray, DMatrix], Tuple[str, float]]:\\n         def feval(params: np.ndarray, data: DMatrix) -> Tuple[str, float]:\\n             y = data.get_label()\\n-            # sample weights should give us a weighted mean here\\n-            return self._distribution.loss(y=y, params=params)\\n+            weights = data.get_weight()\\n+            if weights.size == 0:\\n+                weights = np.ones_like(y)\\n+\\n+            # TODO: abstraction seems a bit wrong here, loss is no longer generic\\n+            # Options are to (1) make the loss specific (nll) for the distribution\\n+            # or (2) pass the weights down (or something else)\\n+            # Given that currently you can only get nll leaning towards (1)\\n+            nll = self._distribution.loss(y=y, params=params)\\n+            loss = np.average(nll, weights=weights)\\n+            loss_name = f\\\"{self.distribution}-NLL\\\"\\n+\\n+            return loss_name, loss\\n \\n         return feval\\n \\n\",\"diff --git a/src/xgboost_distribution/distributions/base.py b/src/xgboost_distribution/distributions/base.py\\nindex 10e6cf9..7e1fc87 100644\\n--- a/src/xgboost_distribution/distributions/base.py\\n+++ b/src/xgboost_distribution/distributions/base.py\\n@@ -13,6 +13,7 @@ class BaseDistribution(ABC):\\n \\n     def __init__(self):\\n         self.Predictions = namedtuple(\\\"Predictions\\\", (p for p in self.params))\\n+        # TODO: the below looks a bit dodgy, no?\\n         # attach to globals to make pickling of namedtuple work\\n         globals()[self.Predictions.__name__] = self.Predictions\\n \\n@@ -34,7 +35,7 @@ def gradient_and_hessian(self, y, params, natural_gradient=True):\\n \\n     @abstractmethod\\n     def loss(self, y, params):\\n-        \\\"\\\"\\\"Evaluate the loss (typically negative log-likelihood)\\\"\\\"\\\"\\n+        \\\"\\\"\\\"Evaluate the per sample loss (typically negative log-likelihood)\\\"\\\"\\\"\\n \\n     @abstractmethod\\n     def predict(self, params):\\n\",\"diff --git a/src/xgboost_distribution/distributions/exponential.py b/src/xgboost_distribution/distributions/exponential.py\\nindex 6ce2f48..4f45290 100644\\n--- a/src/xgboost_distribution/distributions/exponential.py\\n+++ b/src/xgboost_distribution/distributions/exponential.py\\n@@ -55,7 +55,7 @@ def gradient_and_hessian(self, y, params, natural_gradient=True):\\n \\n     def loss(self, y, params):\\n         scale = self.predict(params)\\n-        return \\\"Exponential-NLL\\\", -expon.logpdf(y, scale=scale).mean()\\n+        return \\\"Exponential-NLL\\\", -expon.logpdf(y, scale=scale)\\n \\n     def predict(self, params):\\n         log_scale = params\\n\",\"diff --git a/src/xgboost_distribution/distributions/laplace.py b/src/xgboost_distribution/distributions/laplace.py\\nindex 63bf987..cb44879 100644\\n--- a/src/xgboost_distribution/distributions/laplace.py\\n+++ b/src/xgboost_distribution/distributions/laplace.py\\n@@ -67,7 +67,7 @@ def gradient_and_hessian(self, y, params, natural_gradient=True):\\n \\n         if natural_gradient:\\n             fisher_matrix = np.zeros(shape=(len(y), 2, 2))\\n-            fisher_matrix[:, 0, 0] = 1 / scale ** 2\\n+            fisher_matrix[:, 0, 0] = 1 / scale**2\\n             fisher_matrix[:, 1, 1] = 1\\n \\n             grad = np.linalg.solve(fisher_matrix, grad)\\n@@ -82,7 +82,7 @@ def gradient_and_hessian(self, y, params, natural_gradient=True):\\n \\n     def loss(self, y, params):\\n         loc, scale = self.predict(params)\\n-        return \\\"Laplace-NLL\\\", -laplace.logpdf(y, loc=loc, scale=scale).mean()\\n+        return \\\"Laplace-NLL\\\", -laplace.logpdf(y, loc=loc, scale=scale)\\n \\n     def predict(self, params):\\n         loc, log_scale = self._split_params(params)\\n\",\"diff --git a/src/xgboost_distribution/distributions/log_normal.py b/src/xgboost_distribution/distributions/log_normal.py\\nindex efad5ff..7c39eee 100644\\n--- a/src/xgboost_distribution/distributions/log_normal.py\\n+++ b/src/xgboost_distribution/distributions/log_normal.py\\n@@ -72,7 +72,7 @@ def gradient_and_hessian(self, y, params, natural_gradient=True):\\n \\n     def loss(self, y, params):\\n         scale, s = self.predict(params)\\n-        return \\\"LogNormal-NLL\\\", -lognorm.logpdf(y, s=s, scale=scale).mean()\\n+        return \\\"LogNormal-NLL\\\", -lognorm.logpdf(y, s=s, scale=scale)\\n \\n     def predict(self, params):\\n         log_scale, log_s = self._split_params(params)\\n\",\"diff --git a/src/xgboost_distribution/distributions/negative_binomial.py b/src/xgboost_distribution/distributions/negative_binomial.py\\nindex 109d2c2..cb860b7 100644\\n--- a/src/xgboost_distribution/distributions/negative_binomial.py\\n+++ b/src/xgboost_distribution/distributions/negative_binomial.py\\n@@ -105,7 +105,7 @@ def gradient_and_hessian(self, y, params, natural_gradient=True):\\n \\n     def loss(self, y, params):\\n         n, p = self.predict(params)\\n-        return \\\"NegativeBinomial-NLL\\\", -nbinom.logpmf(y, n=n, p=p).mean()\\n+        return \\\"NegativeBinomial-NLL\\\", -nbinom.logpmf(y, n=n, p=p)\\n \\n     def predict(self, params):\\n         log_n, raw_p = params[:, 0], params[:, 1]\\n\",\"diff --git a/src/xgboost_distribution/distributions/normal.py b/src/xgboost_distribution/distributions/normal.py\\nindex 19a4e19..877b447 100644\\n--- a/src/xgboost_distribution/distributions/normal.py\\n+++ b/src/xgboost_distribution/distributions/normal.py\\n@@ -81,8 +81,9 @@ def gradient_and_hessian(self, y, params, natural_gradient=True):\\n \\n     def loss(self, y, params):\\n         loc, scale = self.predict(params)\\n-        return -norm.logpdf(y, loc=loc, scale=scale)\\n-        # return \\\"NormalDistribution-NLL\\\", -norm.logpdf(y, loc=loc, scale=scale).mean()\\n+        return \\\"NormalDistribution-NLL\\\", -norm.logpdf(\\n+            y, loc=loc, scale=scale\\n+        )  # .mean()\\n \\n     def predict(self, params):\\n         loc, log_scale = self._split_params(params)\\n\",\"diff --git a/src/xgboost_distribution/distributions/poisson.py b/src/xgboost_distribution/distributions/poisson.py\\nindex a6a4d70..5ca8acb 100644\\n--- a/src/xgboost_distribution/distributions/poisson.py\\n+++ b/src/xgboost_distribution/distributions/poisson.py\\n@@ -60,7 +60,7 @@ def gradient_and_hessian(self, y, params, natural_gradient=True):\\n \\n     def loss(self, y, params):\\n         mu = self.predict(params)\\n-        return \\\"Poisson-NLL\\\", -poisson.logpmf(y, mu=mu).mean()\\n+        return \\\"Poisson-NLL\\\", -poisson.logpmf(y, mu=mu)\\n \\n     def predict(self, params):\\n         log_mu = params\\n\",\"diff --git a/src/xgboost_distribution/model.py b/src/xgboost_distribution/model.py\\nindex 28cd4f3..fda6c8d 100644\\n--- a/src/xgboost_distribution/model.py\\n+++ b/src/xgboost_distribution/model.py\\n@@ -261,15 +261,8 @@ def feval(params: np.ndarray, data: DMatrix) -> Tuple[str, float]:\\n             if weights.size == 0:\\n                 weights = np.ones_like(y)\\n \\n-            # TODO: abstraction seems a bit wrong here, loss is no longer generic\\n-            # Options are to (1) make the loss specific (nll) for the distribution\\n-            # or (2) pass the weights down (or something else)\\n-            # Given that currently you can only get nll leaning towards (1)\\n-            nll = self._distribution.loss(y=y, params=params)\\n-            loss = np.average(nll, weights=weights)\\n-            loss_name = f\\\"{self.distribution}-NLL\\\"\\n-\\n-            return loss_name, loss\\n+            loss_name, loss = self._distribution.loss(y=y, params=params)\\n+            return loss_name, np.average(loss, weights=weights)\\n \\n         return feval\\n \\n\",\"diff --git a/src/xgboost_distribution/model.py b/src/xgboost_distribution/model.py\\nindex fda6c8d..13f6458 100644\\n--- a/src/xgboost_distribution/model.py\\n+++ b/src/xgboost_distribution/model.py\\n@@ -28,6 +28,7 @@ class XGBDistribution(XGBModel, RegressorMixin):\\n     @_deprecate_positional_args\\n     def __init__(\\n         self,\\n+        *,\\n         distribution: str = None,\\n         natural_gradient: bool = True,\\n         objective: str = None,\\n@@ -119,12 +120,6 @@ def fit(\\n         else:\\n             base_margin_eval_set = None\\n \\n-        # for param in [sample_weight, sample_weight_eval_set]:\\n-        #     if param is not None:\\n-        #         raise NotImplementedError(\\n-        #             \\\"Sample weights are currently not supported by XGBDistribution!\\\"\\n-        #         )\\n-\\n         train_dmatrix, evals = _wrap_evaluation_matrices(\\n             missing=self.missing,\\n             X=X,\\n\",\"diff --git a/src/xgboost_distribution/distributions/base.py b/src/xgboost_distribution/distributions/base.py\\nindex 7e1fc87..e13017e 100644\\n--- a/src/xgboost_distribution/distributions/base.py\\n+++ b/src/xgboost_distribution/distributions/base.py\\n@@ -13,7 +13,6 @@ class BaseDistribution(ABC):\\n \\n     def __init__(self):\\n         self.Predictions = namedtuple(\\\"Predictions\\\", (p for p in self.params))\\n-        # TODO: the below looks a bit dodgy, no?\\n         # attach to globals to make pickling of namedtuple work\\n         globals()[self.Predictions.__name__] = self.Predictions\\n \\n\",\"diff --git a/src/xgboost_distribution/distributions/normal.py b/src/xgboost_distribution/distributions/normal.py\\nindex 877b447..261330b 100644\\n--- a/src/xgboost_distribution/distributions/normal.py\\n+++ b/src/xgboost_distribution/distributions/normal.py\\n@@ -81,9 +81,7 @@ def gradient_and_hessian(self, y, params, natural_gradient=True):\\n \\n     def loss(self, y, params):\\n         loc, scale = self.predict(params)\\n-        return \\\"NormalDistribution-NLL\\\", -norm.logpdf(\\n-            y, loc=loc, scale=scale\\n-        )  # .mean()\\n+        return \\\"NormalDistribution-NLL\\\", -norm.logpdf(y, loc=loc, scale=scale)\\n \\n     def predict(self, params):\\n         loc, log_scale = self._split_params(params)\\n\",\"diff --git a/CHANGELOG.rst b/CHANGELOG.rst\\nindex 1b8fc4c..8b179c2 100644\\n--- a/CHANGELOG.rst\\n+++ b/CHANGELOG.rst\\n@@ -8,7 +8,7 @@ Development version\\n Version 0.2.X, 2022-xx-xx\\n --------------------------\\n \\n-- ...\\n+- Added support for sample weights, :issue:`45`\\n \\n \\n Current version\\n@@ -28,6 +28,7 @@ Older versions\\n \\n Version 0.2.4, 2022-04-23\\n --------------------------\\n+\\n - Added more precise loss description, negative log likelihood vs error\\n - Various updates to conform with xgboost==1.6.0 release\\n \\n\",\"diff --git a/src/xgboost_distribution/distributions/exponential.py b/src/xgboost_distribution/distributions/exponential.py\\nindex 4f45290..bb986e9 100644\\n--- a/src/xgboost_distribution/distributions/exponential.py\\n+++ b/src/xgboost_distribution/distributions/exponential.py\\n@@ -54,11 +54,11 @@ def gradient_and_hessian(self, y, params, natural_gradient=True):\\n         return grad, hess\\n \\n     def loss(self, y, params):\\n-        scale = self.predict(params)\\n+        (scale,) = self.predict(params)\\n         return \\\"Exponential-NLL\\\", -expon.logpdf(y, scale=scale)\\n \\n     def predict(self, params):\\n-        log_scale = params\\n+        log_scale = params  # params are shape (n,)\\n         scale = np.exp(log_scale)\\n         return self.Predictions(scale=scale)\\n \\n\",\"diff --git a/src/xgboost_distribution/distributions/normal.py b/src/xgboost_distribution/distributions/normal.py\\nindex 261330b..488216b 100644\\n--- a/src/xgboost_distribution/distributions/normal.py\\n+++ b/src/xgboost_distribution/distributions/normal.py\\n@@ -85,7 +85,8 @@ def loss(self, y, params):\\n \\n     def predict(self, params):\\n         loc, log_scale = self._split_params(params)\\n-        # log_scale = np.clip(log_scale, -100, 100)  # TODO: is this needed?\\n+        # TODO: do we need clipping for safety?\\n+        # log_scale = np.clip(log_scale, -100, 100)\\n         scale = np.exp(log_scale)\\n \\n         return self.Predictions(loc=loc, scale=scale)\\n\",\"diff --git a/src/xgboost_distribution/distributions/poisson.py b/src/xgboost_distribution/distributions/poisson.py\\nindex 5ca8acb..8f009f1 100644\\n--- a/src/xgboost_distribution/distributions/poisson.py\\n+++ b/src/xgboost_distribution/distributions/poisson.py\\n@@ -59,11 +59,11 @@ def gradient_and_hessian(self, y, params, natural_gradient=True):\\n         return grad, hess\\n \\n     def loss(self, y, params):\\n-        mu = self.predict(params)\\n+        (mu,) = self.predict(params)\\n         return \\\"Poisson-NLL\\\", -poisson.logpmf(y, mu=mu)\\n \\n     def predict(self, params):\\n-        log_mu = params\\n+        log_mu = params  # params are shape (n,)\\n         mu = np.exp(log_mu)\\n         return self.Predictions(mu=mu)\\n \\n\",\"diff --git a/examples/count_data.py b/examples/count_data.py\\nindex 2c0ce17..0aef9c5 100644\\n--- a/examples/count_data.py\\n+++ b/examples/count_data.py\\n@@ -21,10 +21,10 @@ def predict_distribution(model, X, y):\\n     \\\"\\\"\\\"Predict a distribution for a given X, and evaluate over y\\\"\\\"\\\"\\n \\n     distribution_func = {\\n-        \\\"normal\\\": getattr(stats, \\\"norm\\\").pdf,\\n-        \\\"laplace\\\": getattr(stats, \\\"laplace\\\").pdf,\\n-        \\\"poisson\\\": getattr(stats, \\\"poisson\\\").pmf,\\n-        \\\"negative-binomial\\\": getattr(stats, \\\"nbinom\\\").pmf,\\n+        \\\"normal\\\": stats.norm.pdf,\\n+        \\\"laplace\\\": stats.laplace.pdf,\\n+        \\\"poisson\\\": stats.poisson.pmf,\\n+        \\\"negative-binomial\\\": stats.nbinom.pmf,\\n     }\\n     preds = model.predict(X[..., np.newaxis])\\n \\n\",\"diff --git a/CHANGELOG.rst b/CHANGELOG.rst\\nindex 8b179c2..1c2bbbb 100644\\n--- a/CHANGELOG.rst\\n+++ b/CHANGELOG.rst\\n@@ -5,7 +5,7 @@ Changelog\\n Development version\\n ===================\\n \\n-Version 0.2.X, 2022-xx-xx\\n+Version 0.2.6, 2023-xx-xx\\n --------------------------\\n \\n - Added support for sample weights, :issue:`45`\\n\",\"diff --git a/src/xgboost_distribution/model.py b/src/xgboost_distribution/model.py\\nindex 13f6458..6fa4c9c 100644\\n--- a/src/xgboost_distribution/model.py\\n+++ b/src/xgboost_distribution/model.py\\n@@ -241,7 +241,6 @@ def obj(params: np.ndarray, data: DMatrix) -> Tuple[np.ndarray, np.ndarray]:\\n             weights = data.get_weight()\\n             if weights.size != 0:\\n                 weights = weights.reshape(-1, 1)\\n-                # TODO: what if some weights go to zero?\\n                 grad *= weights\\n                 hess *= weights\\n \\n@@ -254,7 +253,7 @@ def feval(params: np.ndarray, data: DMatrix) -> Tuple[str, float]:\\n             y = data.get_label()\\n             weights = data.get_weight()\\n             if weights.size == 0:\\n-                weights = np.ones_like(y)\\n+                weights = None\\n \\n             loss_name, loss = self._distribution.loss(y=y, params=params)\\n             return loss_name, np.average(loss, weights=weights)\\n\"]", "test_patch": "[\"diff --git a/tests/test_model.py b/tests/test_model.py\\nindex 271bb21..abf303d 100644\\n--- a/tests/test_model.py\\n+++ b/tests/test_model.py\\n@@ -67,6 +67,48 @@ def test_distribution_set_param(small_X_y_data):\\n     model.fit(X, y)\\n \\n \\n+def test_sample_weights(small_X_y_data):\\n+    X, y = small_X_y_data\\n+\\n+    random_weights = np.random.choice([1, 2], len(X))\\n+    model = XGBDistribution(distribution=\\\"normal\\\", n_estimators=1)\\n+    preds_without_weights = model.fit(X, y).predict(X)\\n+    preds_with_weights = model.fit(X, y, sample_weight=random_weights).predict(X)\\n+\\n+    with pytest.raises(AssertionError):\\n+        np.testing.assert_array_equal(preds_without_weights.loc, preds_with_weights.loc)\\n+        np.testing.assert_array_equal(\\n+            preds_without_weights.scale, preds_with_weights.scale\\n+        )\\n+\\n+\\n+def test_sample_weights_eval_set(small_train_test_data):\\n+    \\\"\\\"\\\"Check that predict with early stopping uses correct ntrees\\\"\\\"\\\"\\n+    X_train, X_test, y_train, y_test = small_train_test_data\\n+\\n+    weights_train = np.random.choice([1, 2], len(X_train))\\n+    weights_test = np.random.choice([1, 2], len(X_test))\\n+\\n+    model = XGBDistribution(distribution=\\\"normal\\\", n_estimators=2)\\n+    model.fit(\\n+        X_train, y_train, sample_weight=weights_train, eval_set=[(X_test, y_test)]\\n+    )\\n+    evals_result_without_weights = model.evals_result()\\n+    nll_without_weights = evals_result_without_weights[\\\"validation_0\\\"][\\\"normal-NLL\\\"]\\n+\\n+    model.fit(\\n+        X_train,\\n+        y_train,\\n+        sample_weight=weights_train,\\n+        eval_set=[(X_test, y_test)],\\n+        sample_weight_eval_set=[weights_test],\\n+    )\\n+    evals_result_with_weights = model.evals_result()\\n+    nll_with_weights = evals_result_with_weights[\\\"validation_0\\\"][\\\"normal-NLL\\\"]\\n+\\n+    assert all((nll_with_weights[i] != nll_without_weights[i] for i in [0, 1]))\\n+\\n+\\n # -------------------------------------------------------------------------------------\\n # Failure modes\\n # -------------------------------------------------------------------------------------\\n@@ -106,7 +148,7 @@ def test_train_with_sample_weights_fails(small_X_y_data):\\n \\n \\n # -------------------------------------------------------------------------------------\\n-#  Model internal tests\\n+#  Internal tests\\n # -------------------------------------------------------------------------------------\\n \\n \\n@@ -139,7 +181,7 @@ def test_objective_and_evaluation_funcs_callable(distribution):\\n \\n \\n # -------------------------------------------------------------------------------------\\n-#  Model IO tests\\n+#  IO tests\\n # -------------------------------------------------------------------------------------\\n \\n \\n\",\"diff --git a/README.rst b/README.rst\\nindex 25a91cf..ba3e31c 100644\\n--- a/README.rst\\n+++ b/README.rst\\n@@ -1,6 +1,9 @@\\n .. image:: https://github.com/CDonnerer/xgboost-distribution/actions/workflows/test.yml/badge.svg?branch=main\\n   :target: https://github.com/CDonnerer/xgboost-distribution/actions/workflows/test.yml\\n \\n+.. image:: https://img.shields.io/badge/code%20style-black-000000.svg\\n+  :target: https://github.com/psf/black\\n+\\n .. image:: https://coveralls.io/repos/github/CDonnerer/xgboost-distribution/badge.svg?branch=main\\n   :target: https://coveralls.io/github/CDonnerer/xgboost-distribution?branch=main\\n \\n\",\"diff --git a/README.rst b/README.rst\\nindex ba3e31c..bd2e881 100644\\n--- a/README.rst\\n+++ b/README.rst\\n@@ -1,12 +1,12 @@\\n .. image:: https://github.com/CDonnerer/xgboost-distribution/actions/workflows/test.yml/badge.svg?branch=main\\n   :target: https://github.com/CDonnerer/xgboost-distribution/actions/workflows/test.yml\\n \\n-.. image:: https://img.shields.io/badge/code%20style-black-000000.svg\\n-  :target: https://github.com/psf/black\\n-\\n .. image:: https://coveralls.io/repos/github/CDonnerer/xgboost-distribution/badge.svg?branch=main\\n   :target: https://coveralls.io/github/CDonnerer/xgboost-distribution?branch=main\\n \\n+.. image:: https://img.shields.io/badge/code%20style-black-000000.svg\\n+  :target: https://github.com/psf/black\\n+\\n .. image:: https://readthedocs.org/projects/xgboost-distribution/badge/?version=latest\\n   :target: https://xgboost-distribution.readthedocs.io/en/latest/?badge=latest\\n   :alt: Documentation Status\\n\",\"diff --git a/tests/test_model.py b/tests/test_model.py\\nindex abf303d..657ad50 100644\\n--- a/tests/test_model.py\\n+++ b/tests/test_model.py\\n@@ -94,7 +94,9 @@ def test_sample_weights_eval_set(small_train_test_data):\\n         X_train, y_train, sample_weight=weights_train, eval_set=[(X_test, y_test)]\\n     )\\n     evals_result_without_weights = model.evals_result()\\n-    nll_without_weights = evals_result_without_weights[\\\"validation_0\\\"][\\\"normal-NLL\\\"]\\n+    nll_without_weights = evals_result_without_weights[\\\"validation_0\\\"][\\n+        \\\"NormalDistribution-NLL\\\"\\n+    ]\\n \\n     model.fit(\\n         X_train,\\n@@ -104,7 +106,9 @@ def test_sample_weights_eval_set(small_train_test_data):\\n         sample_weight_eval_set=[weights_test],\\n     )\\n     evals_result_with_weights = model.evals_result()\\n-    nll_with_weights = evals_result_with_weights[\\\"validation_0\\\"][\\\"normal-NLL\\\"]\\n+    nll_with_weights = evals_result_with_weights[\\\"validation_0\\\"][\\n+        \\\"NormalDistribution-NLL\\\"\\n+    ]\\n \\n     assert all((nll_with_weights[i] != nll_without_weights[i] for i in [0, 1]))\\n \\n\",\"diff --git a/tests/test_model.py b/tests/test_model.py\\nindex 657ad50..577cc4b 100644\\n--- a/tests/test_model.py\\n+++ b/tests/test_model.py\\n@@ -67,11 +67,11 @@ def test_distribution_set_param(small_X_y_data):\\n     model.fit(X, y)\\n \\n \\n-def test_sample_weights(small_X_y_data):\\n+def test_fit_with_sample_weights_fails(small_X_y_data):\\n     X, y = small_X_y_data\\n \\n     random_weights = np.random.choice([1, 2], len(X))\\n-    model = XGBDistribution(distribution=\\\"normal\\\", n_estimators=1)\\n+    model = XGBDistribution(distribution=\\\"normal\\\", n_estimators=2)\\n     preds_without_weights = model.fit(X, y).predict(X)\\n     preds_with_weights = model.fit(X, y, sample_weight=random_weights).predict(X)\\n \\n@@ -140,17 +140,6 @@ def test_setting_objective_in_init_fails():\\n         XGBDistribution(objective=\\\"binary:logistic\\\")\\n \\n \\n-def test_train_with_sample_weights_fails(small_X_y_data):\\n-    X, y = small_X_y_data\\n-\\n-    model = XGBDistribution()\\n-    with pytest.raises(NotImplementedError):\\n-        model.fit(X, y, sample_weight=np.ones_like(y))\\n-\\n-    with pytest.raises(NotImplementedError):\\n-        model.fit(X, y, eval_set=[(X, y)], sample_weight_eval_set=np.ones_like(y))\\n-\\n-\\n # -------------------------------------------------------------------------------------\\n #  Internal tests\\n # -------------------------------------------------------------------------------------\\n\",\"diff --git a/tests/test_model.py b/tests/test_model.py\\nindex 577cc4b..d0924d1 100644\\n--- a/tests/test_model.py\\n+++ b/tests/test_model.py\\n@@ -1,6 +1,5 @@\\n \\\"\\\"\\\"Test suite for XGBDistribution model\\n \\\"\\\"\\\"\\n-\\n import os\\n import pickle\\n \\n\",\"diff --git a/tests/test_model.py b/tests/test_model.py\\nindex d0924d1..e1d1222 100644\\n--- a/tests/test_model.py\\n+++ b/tests/test_model.py\\n@@ -66,7 +66,7 @@ def test_distribution_set_param(small_X_y_data):\\n     model.fit(X, y)\\n \\n \\n-def test_fit_with_sample_weights_fails(small_X_y_data):\\n+def test_fit_with_sample_weights(small_X_y_data):\\n     X, y = small_X_y_data\\n \\n     random_weights = np.random.choice([1, 2], len(X))\\n\",\"diff --git a/tests/conftest.py b/tests/conftest.py\\nindex 4e5619e..b13161c 100644\\n--- a/tests/conftest.py\\n+++ b/tests/conftest.py\\n@@ -34,3 +34,29 @@ def small_train_test_data(request, small_X_y_data):\\n         X, y = pd.DataFrame(X), pd.Series(y)\\n \\n     return train_test_split(X, y, test_size=0.2, random_state=1)\\n+\\n+\\n+@pytest.fixture\\n+def small_X_y_count_data():\\n+    \\\"\\\"\\\"Small set of X, y data, with y being counts (positive int)\\\"\\\"\\\"\\n+\\n+    def generate_count_data(n_samples=100):\\n+        X = np.random.uniform(-2, 0, n_samples)\\n+        n = 66 * np.abs(np.cos(X))\\n+        p = 0.5 * np.abs(np.cos(X / 3))\\n+\\n+        y = np.random.negative_binomial(n=n, p=p, size=n_samples)\\n+        return X[..., np.newaxis], y\\n+\\n+    return generate_count_data(n_samples=100)\\n+\\n+\\n+@pytest.fixture(params=[\\\"numpy\\\", \\\"pandas\\\"])\\n+def small_train_test_count_data(request, small_X_y_count_data):\\n+    \\\"\\\"\\\"Small set of train-test split X, y data (positive int)\\\"\\\"\\\"\\n+    X, y = small_X_y_count_data\\n+\\n+    if request.param == \\\"pandas\\\":\\n+        X, y = pd.DataFrame(X), pd.Series(y)\\n+\\n+    return train_test_split(X, y, test_size=0.2, random_state=1)\\n\",\"diff --git a/tests/distributions/test_base.py b/tests/distributions/test_base.py\\nindex 9dd9d74..2cde742 100644\\n--- a/tests/distributions/test_base.py\\n+++ b/tests/distributions/test_base.py\\n@@ -2,7 +2,10 @@\\n \\\"\\\"\\\"\\n import pytest\\n \\n+import numpy as np\\n+\\n from xgboost_distribution.distributions import (\\n+    AVAILABLE_DISTRIBUTIONS,\\n     Normal,\\n     format_distribution_name,\\n     get_distribution,\\n@@ -34,3 +37,46 @@ def test_get_distribution_doc():\\n def test_format_distribution_name(name, expected_name):\\n     formatted_name = format_distribution_name(name)\\n     assert formatted_name == expected_name\\n+\\n+\\n+@pytest.mark.parametrize(\\\"distribution_name\\\", AVAILABLE_DISTRIBUTIONS.keys())\\n+def test_distribution_starting_params_shape(distribution_name):\\n+    \\\"\\\"\\\"We need to get as many starting params as distribution params\\\"\\\"\\\"\\n+    y = np.random.choice(\\n+        [\\n+            1,\\n+            2,\\n+        ],\\n+        5,\\n+    )\\n+\\n+    distribution = get_distribution(distribution_name)\\n+    starting_params = distribution.starting_params(y=y)\\n+\\n+    assert len(starting_params) == len(distribution.params)\\n+\\n+\\n+# @pytest.mark.skip\\n+@pytest.mark.parametrize(\\\"distribution_name\\\", AVAILABLE_DISTRIBUTIONS.keys())\\n+def test_distribution_loss_shape(distribution_name):\\n+    \\\"\\\"\\\"Ensure that evaluation fns return expect nd.array shape\\\"\\\"\\\"\\n+    y = np.random.choice(\\n+        [\\n+            1,\\n+            2,\\n+        ],\\n+        5,\\n+    )\\n+\\n+    distribution = get_distribution(distribution_name)\\n+    starting_params = distribution.starting_params(y=y)\\n+\\n+    params = np.concatenate([starting_params]) * np.ones_like(y).reshape(-1, 1)\\n+\\n+    if len(distribution.params) == 1:\\n+        params = params.squeeze()  # for 1 param we get a squeezed array (n,) from xgb\\n+\\n+    loss_name, loss = distribution.loss(y, params)\\n+\\n+    assert isinstance(loss_name, str)\\n+    assert loss.shape == y.shape\\n\",\"diff --git a/tests/distributions/test_poisson.py b/tests/distributions/test_poisson.py\\nindex b72ecbf..e9a83bb 100644\\n--- a/tests/distributions/test_poisson.py\\n+++ b/tests/distributions/test_poisson.py\\n@@ -53,7 +53,7 @@ def test_loss(poisson):\\n     loss_name, loss_value = poisson.loss(\\n         # fmt: off\\n         y=np.array([1, ]),\\n-        params=np.array([[np.log(1)], ]),\\n+        params=np.array([np.log(1), ]),\\n     )\\n     assert loss_name == \\\"Poisson-NLL\\\"\\n     assert loss_value == 1.0\\n\",\"diff --git a/tests/test_model.py b/tests/test_model.py\\nindex e1d1222..e4871b7 100644\\n--- a/tests/test_model.py\\n+++ b/tests/test_model.py\\n@@ -26,6 +26,26 @@ def test_XGBDistribution_early_stopping_fit(small_train_test_data):\\n     assert isinstance(evals_result, dict)\\n \\n \\n+def test_XGBDistribution_early_stopping_fit_single_param_distribution(\\n+    small_train_test_count_data,\\n+):\\n+    \\\"\\\"\\\"Integration test for single param dist\\\"\\\"\\\"\\n+\\n+    X_train, X_test, y_train, y_test = small_train_test_count_data\\n+\\n+    model = XGBDistribution(\\n+        distribution=\\\"exponential\\\",\\n+        max_depth=3,\\n+        n_estimators=500,\\n+        early_stopping_rounds=10,\\n+    )\\n+    model.fit(X_train, y_train, eval_set=[(X_test, y_test)])\\n+    evals_result = model.evals_result()\\n+\\n+    # assert model.best_iteration == 6\\n+    assert isinstance(evals_result, dict)\\n+\\n+\\n def test_XGBDistribution_early_stopping_predict(small_train_test_data):\\n     \\\"\\\"\\\"Check that predict with early stopping uses correct ntrees\\\"\\\"\\\"\\n     X_train, X_test, y_train, y_test = small_train_test_data\\n@@ -66,11 +86,12 @@ def test_distribution_set_param(small_X_y_data):\\n     model.fit(X, y)\\n \\n \\n-def test_fit_with_sample_weights(small_X_y_data):\\n+@pytest.mark.parametrize(\\\"distribution\\\", [\\\"normal\\\"])\\n+def test_fit_with_sample_weights(small_X_y_data, distribution):\\n     X, y = small_X_y_data\\n \\n     random_weights = np.random.choice([1, 2], len(X))\\n-    model = XGBDistribution(distribution=\\\"normal\\\", n_estimators=2)\\n+    model = XGBDistribution(distribution=distribution, n_estimators=2)\\n     preds_without_weights = model.fit(X, y).predict(X)\\n     preds_with_weights = model.fit(X, y, sample_weight=random_weights).predict(X)\\n \\n\",\"diff --git a/tests/distributions/test_base.py b/tests/distributions/test_base.py\\nindex 2cde742..5f1103b 100644\\n--- a/tests/distributions/test_base.py\\n+++ b/tests/distributions/test_base.py\\n@@ -60,13 +60,7 @@ def test_distribution_starting_params_shape(distribution_name):\\n @pytest.mark.parametrize(\\\"distribution_name\\\", AVAILABLE_DISTRIBUTIONS.keys())\\n def test_distribution_loss_shape(distribution_name):\\n     \\\"\\\"\\\"Ensure that evaluation fns return expect nd.array shape\\\"\\\"\\\"\\n-    y = np.random.choice(\\n-        [\\n-            1,\\n-            2,\\n-        ],\\n-        5,\\n-    )\\n+    y = np.random.choice([1, 2], 5)  # fmt: off\\n \\n     distribution = get_distribution(distribution_name)\\n     starting_params = distribution.starting_params(y=y)\\n\",\"diff --git a/tests/distributions/test_base.py b/tests/distributions/test_base.py\\nindex 5f1103b..d98e739 100644\\n--- a/tests/distributions/test_base.py\\n+++ b/tests/distributions/test_base.py\\n@@ -42,13 +42,7 @@ def test_format_distribution_name(name, expected_name):\\n @pytest.mark.parametrize(\\\"distribution_name\\\", AVAILABLE_DISTRIBUTIONS.keys())\\n def test_distribution_starting_params_shape(distribution_name):\\n     \\\"\\\"\\\"We need to get as many starting params as distribution params\\\"\\\"\\\"\\n-    y = np.random.choice(\\n-        [\\n-            1,\\n-            2,\\n-        ],\\n-        5,\\n-    )\\n+    y = np.random.choice([1, 2], 5)  # fmt: off\\n \\n     distribution = get_distribution(distribution_name)\\n     starting_params = distribution.starting_params(y=y)\\n@@ -56,7 +50,6 @@ def test_distribution_starting_params_shape(distribution_name):\\n     assert len(starting_params) == len(distribution.params)\\n \\n \\n-# @pytest.mark.skip\\n @pytest.mark.parametrize(\\\"distribution_name\\\", AVAILABLE_DISTRIBUTIONS.keys())\\n def test_distribution_loss_shape(distribution_name):\\n     \\\"\\\"\\\"Ensure that evaluation fns return expect nd.array shape\\\"\\\"\\\"\\n\",\"diff --git a/tests/conftest.py b/tests/conftest.py\\nindex b13161c..aff0571 100644\\n--- a/tests/conftest.py\\n+++ b/tests/conftest.py\\n@@ -41,6 +41,7 @@ def small_X_y_count_data():\\n     \\\"\\\"\\\"Small set of X, y data, with y being counts (positive int)\\\"\\\"\\\"\\n \\n     def generate_count_data(n_samples=100):\\n+        np.random.seed(11)  # 'tuned' to be simple to test against\\n         X = np.random.uniform(-2, 0, n_samples)\\n         n = 66 * np.abs(np.cos(X))\\n         p = 0.5 * np.abs(np.cos(X / 3))\\n\",\"diff --git a/tests/test_model.py b/tests/test_model.py\\nindex e4871b7..14e5f24 100644\\n--- a/tests/test_model.py\\n+++ b/tests/test_model.py\\n@@ -29,7 +29,7 @@ def test_XGBDistribution_early_stopping_fit(small_train_test_data):\\n def test_XGBDistribution_early_stopping_fit_single_param_distribution(\\n     small_train_test_count_data,\\n ):\\n-    \\\"\\\"\\\"Integration test for single param dist\\\"\\\"\\\"\\n+    \\\"\\\"\\\"Integration test for single param dist (which operate on squeezed arrays)\\\"\\\"\\\"\\n \\n     X_train, X_test, y_train, y_test = small_train_test_count_data\\n \\n@@ -42,7 +42,7 @@ def test_XGBDistribution_early_stopping_fit_single_param_distribution(\\n     model.fit(X_train, y_train, eval_set=[(X_test, y_test)])\\n     evals_result = model.evals_result()\\n \\n-    # assert model.best_iteration == 6\\n+    assert model.best_iteration == 10\\n     assert isinstance(evals_result, dict)\\n \\n \\n\",\"diff --git a/tests/test_model.py b/tests/test_model.py\\nindex 14e5f24..2bbe5ea 100644\\n--- a/tests/test_model.py\\n+++ b/tests/test_model.py\\n@@ -103,7 +103,7 @@ def test_fit_with_sample_weights(small_X_y_data, distribution):\\n \\n \\n def test_sample_weights_eval_set(small_train_test_data):\\n-    \\\"\\\"\\\"Check that predict with early stopping uses correct ntrees\\\"\\\"\\\"\\n+    \\\"\\\"\\\"Check weights for eval sets change NLL in training\\\"\\\"\\\"\\n     X_train, X_test, y_train, y_test = small_train_test_data\\n \\n     weights_train = np.random.choice([1, 2], len(X_train))\\n@@ -130,7 +130,7 @@ def test_sample_weights_eval_set(small_train_test_data):\\n         \\\"NormalDistribution-NLL\\\"\\n     ]\\n \\n-    assert all((nll_with_weights[i] != nll_without_weights[i] for i in [0, 1]))\\n+    assert all((nll_with_weights[i] != nll_without_weights[i] for i in range(2)))\\n \\n \\n # -------------------------------------------------------------------------------------\\n\",\"diff --git a/tests/test_model.py b/tests/test_model.py\\nindex 2bbe5ea..89acc00 100644\\n--- a/tests/test_model.py\\n+++ b/tests/test_model.py\\n@@ -103,7 +103,7 @@ def test_fit_with_sample_weights(small_X_y_data, distribution):\\n \\n \\n def test_sample_weights_eval_set(small_train_test_data):\\n-    \\\"\\\"\\\"Check weights for eval sets change NLL in training\\\"\\\"\\\"\\n+    \\\"\\\"\\\"Check weights for eval sets change NLL during training\\\"\\\"\\\"\\n     X_train, X_test, y_train, y_test = small_train_test_data\\n \\n     weights_train = np.random.choice([1, 2], len(X_train))\\n\",\"diff --git a/tests/distributions/test_exponential.py b/tests/distributions/test_exponential.py\\nindex 3e7d375..975db5c 100644\\n--- a/tests/distributions/test_exponential.py\\n+++ b/tests/distributions/test_exponential.py\\n@@ -50,10 +50,17 @@ def test_gradient_calculation(exponential, y, params, natural_gradient, expected\\n \\n \\n def test_loss(exponential):\\n-    loss_name, loss_value = exponential.loss(\\n+    loss_name, loss_values = exponential.loss(\\n         # fmt: off\\n         y=np.array([1, ]),\\n-        params=np.array([[np.log(1)], ]),\\n+        params=np.array([np.log(1), ]),\\n     )\\n     assert loss_name == \\\"Exponential-NLL\\\"\\n-    assert loss_value == 1.0\\n+    np.testing.assert_array_equal(\\n+        loss_values,\\n+        np.array(\\n+            [\\n+                1.0,\\n+            ]\\n+        ),\\n+    )\\n\",\"diff --git a/tests/distributions/test_laplace.py b/tests/distributions/test_laplace.py\\nindex f630646..a06fcce 100644\\n--- a/tests/distributions/test_laplace.py\\n+++ b/tests/distributions/test_laplace.py\\n@@ -35,10 +35,17 @@ def test_gradient_calculation(laplace, y, params, natural_gradient, expected_gra\\n \\n \\n def test_loss(laplace):\\n-    loss_name, loss_value = laplace.loss(\\n+    loss_name, loss_values = laplace.loss(\\n         # fmt: off\\n         y=np.array([1, ]),\\n         params=np.array([[1, np.log(1)], ]),\\n     )\\n     assert loss_name == \\\"Laplace-NLL\\\"\\n-    np.testing.assert_approx_equal(loss_value, -np.log(0.5))\\n+    np.testing.assert_approx_equal(\\n+        loss_values,\\n+        np.array(\\n+            [\\n+                -np.log(0.5),\\n+            ]\\n+        ),\\n+    )\\n\",\"diff --git a/tests/distributions/test_log_normal.py b/tests/distributions/test_log_normal.py\\nindex f3b830c..806f448 100644\\n--- a/tests/distributions/test_log_normal.py\\n+++ b/tests/distributions/test_log_normal.py\\n@@ -50,10 +50,17 @@ def test_gradient_calculation(lognormal, y, params, natural_gradient, expected_g\\n \\n \\n def test_loss(lognormal):\\n-    loss_name, loss_value = lognormal.loss(\\n+    loss_name, loss_values = lognormal.loss(\\n         # fmt: off\\n         y=np.array([0, ]),\\n         params=np.array([[1, 0], ]),\\n     )\\n     assert loss_name == \\\"LogNormal-NLL\\\"\\n-    assert loss_value == np.inf\\n+    np.testing.assert_array_equal(\\n+        loss_values,\\n+        np.array(\\n+            [\\n+                np.inf,\\n+            ]\\n+        ),\\n+    )\\n\",\"diff --git a/tests/distributions/test_negative_binomial.py b/tests/distributions/test_negative_binomial.py\\nindex 7cc386f..2608bb3 100644\\n--- a/tests/distributions/test_negative_binomial.py\\n+++ b/tests/distributions/test_negative_binomial.py\\n@@ -26,7 +26,7 @@ def negative_binomial():\\n def test_gradient_calculation(\\n     negative_binomial, y, params, natural_gradient, expected_grad\\n ):\\n-    grad, hess = negative_binomial.gradient_and_hessian(\\n+    grad, _ = negative_binomial.gradient_and_hessian(\\n         y, params, natural_gradient=natural_gradient\\n     )\\n     np.testing.assert_array_equal(grad, expected_grad)\\n\",\"diff --git a/tests/distributions/test_normal.py b/tests/distributions/test_normal.py\\nindex 5548a88..f018c34 100644\\n--- a/tests/distributions/test_normal.py\\n+++ b/tests/distributions/test_normal.py\\n@@ -28,7 +28,5 @@ def normal():\\n     ],\\n )\\n def test_gradient_calculation(normal, y, params, natural_gradient, expected_grad):\\n-    grad, hess = normal.gradient_and_hessian(\\n-        y, params, natural_gradient=natural_gradient\\n-    )\\n+    grad, _ = normal.gradient_and_hessian(y, params, natural_gradient=natural_gradient)\\n     np.testing.assert_array_equal(grad, expected_grad)\\n\",\"diff --git a/tests/distributions/test_poisson.py b/tests/distributions/test_poisson.py\\nindex e9a83bb..b2977d5 100644\\n--- a/tests/distributions/test_poisson.py\\n+++ b/tests/distributions/test_poisson.py\\n@@ -43,17 +43,22 @@ def test_target_validation_raises(poisson, invalid_target):\\n     ],\\n )\\n def test_gradient_calculation(poisson, y, params, natural_gradient, expected_grad):\\n-    grad, hess = poisson.gradient_and_hessian(\\n-        y, params, natural_gradient=natural_gradient\\n-    )\\n+    grad, _ = poisson.gradient_and_hessian(y, params, natural_gradient=natural_gradient)\\n     np.testing.assert_array_equal(grad, expected_grad)\\n \\n \\n def test_loss(poisson):\\n-    loss_name, loss_value = poisson.loss(\\n+    loss_name, loss_values = poisson.loss(\\n         # fmt: off\\n         y=np.array([1, ]),\\n         params=np.array([np.log(1), ]),\\n     )\\n     assert loss_name == \\\"Poisson-NLL\\\"\\n-    assert loss_value == 1.0\\n+    np.testing.assert_array_equal(\\n+        loss_values,\\n+        np.array(\\n+            [\\n+                1.0,\\n+            ]\\n+        ),\\n+    )\\n\",\"diff --git a/tests/distributions/test_exponential.py b/tests/distributions/test_exponential.py\\nindex 975db5c..9c6d050 100644\\n--- a/tests/distributions/test_exponential.py\\n+++ b/tests/distributions/test_exponential.py\\n@@ -56,11 +56,4 @@ def test_loss(exponential):\\n         params=np.array([np.log(1), ]),\\n     )\\n     assert loss_name == \\\"Exponential-NLL\\\"\\n-    np.testing.assert_array_equal(\\n-        loss_values,\\n-        np.array(\\n-            [\\n-                1.0,\\n-            ]\\n-        ),\\n-    )\\n+    np.testing.assert_array_equal(loss_values, np.array([1.0]))\\n\",\"diff --git a/tests/distributions/test_laplace.py b/tests/distributions/test_laplace.py\\nindex a06fcce..63682c2 100644\\n--- a/tests/distributions/test_laplace.py\\n+++ b/tests/distributions/test_laplace.py\\n@@ -41,11 +41,4 @@ def test_loss(laplace):\\n         params=np.array([[1, np.log(1)], ]),\\n     )\\n     assert loss_name == \\\"Laplace-NLL\\\"\\n-    np.testing.assert_approx_equal(\\n-        loss_values,\\n-        np.array(\\n-            [\\n-                -np.log(0.5),\\n-            ]\\n-        ),\\n-    )\\n+    np.testing.assert_approx_equal(loss_values, np.array([-np.log(0.5)]))\\n\",\"diff --git a/tests/distributions/test_log_normal.py b/tests/distributions/test_log_normal.py\\nindex 806f448..6c5391d 100644\\n--- a/tests/distributions/test_log_normal.py\\n+++ b/tests/distributions/test_log_normal.py\\n@@ -56,11 +56,4 @@ def test_loss(lognormal):\\n         params=np.array([[1, 0], ]),\\n     )\\n     assert loss_name == \\\"LogNormal-NLL\\\"\\n-    np.testing.assert_array_equal(\\n-        loss_values,\\n-        np.array(\\n-            [\\n-                np.inf,\\n-            ]\\n-        ),\\n-    )\\n+    np.testing.assert_array_equal(loss_values, np.array([np.inf]))\\n\",\"diff --git a/tests/distributions/test_poisson.py b/tests/distributions/test_poisson.py\\nindex b2977d5..39060a8 100644\\n--- a/tests/distributions/test_poisson.py\\n+++ b/tests/distributions/test_poisson.py\\n@@ -54,11 +54,4 @@ def test_loss(poisson):\\n         params=np.array([np.log(1), ]),\\n     )\\n     assert loss_name == \\\"Poisson-NLL\\\"\\n-    np.testing.assert_array_equal(\\n-        loss_values,\\n-        np.array(\\n-            [\\n-                1.0,\\n-            ]\\n-        ),\\n-    )\\n+    np.testing.assert_array_equal(loss_values, np.array([1.0]))\"]", "hints_text": ""}
