{"instance_id": "731106692981369", "repo": "lightly-ai/lightly", "base_commit": "3da59d2607caa052d349fe9990eb831275480e5b", "problem_statement": "Error while using VideoDataset without extension specified:\\n### Issue\\r\\nWhen using the VideoDataset I end up with an exception if the extension argument is not set. I can reproduce the error on different lightly versions as well as video files.\\r\\n\\r\\n![image](https://user-images.githubusercontent.com/1522766/102757196-3f761200-4371-11eb-85e6-657fdf29845e.png)\\r\\n\\r\\n\\r\\nOS: CentOS\\r\\nPython 3.7\\r\\nLightly1.07", "FAIL_TO_PASS": ["tests/data/test_data_collate.py::TestDataCollate::test_base_collate", "tests/data/test_data_collate.py::TestDataCollate::test_image_collate_tuple_input_size", "tests/data/test_data_collate.py::TestDataCollate::test_simclr_collate_tuple_input_size", "tests/data/test_LightlyDataset.py::TestLightlyDataset::test_create_lightly_dataset_from_folder", "tests/data/test_data_collate.py::TestDataCollate::test_image_collate", "tests/data/test_LightlyDataset.py::TestLightlyDataset::test_not_existing_folder_dataset", "tests/data/test_LightlyDataset.py::TestLightlyDataset::test_create_lightly_dataset_from_folder_nosubdir", "tests/data/test_LightlyDataset.py::TestLightlyDataset::test_from_torch_dataset"], "PASS_TO_PASS": ["tests/transforms/test_Rotation.py::TestGaussianBlur::test_on_pil_image", "tests/data/test_VideoDataset.py::TestVideoDataset::test_video_dataset_from_folder", "tests/transforms/test_GaussianBlur.py::TestGaussianBlur::test_on_pil_image", "tests/data/test_VideoDataset.py::TestVideoDataset::test_video_similar_timestamps_for_different_backends"], "language": "python", "test_command": "source /saved/ENV || source /saved/*/ENV && pytest --no-header -rA --tb=no -p no:cacheprovider --continue-on-collection-errors", "test_output_parser": "python/parse_log_pytest_v3", "image_storage_uri": "vmvm-registry.fbinfra.net/repomate_image_activ_pytest/lightly-ai_lightly:3da59d2607caa052d349fe9990eb831275480e5b", "patch": "[\"diff --git a/docs/Makefile b/docs/Makefile\\nindex 84774c1..473df0a 100644\\n--- a/docs/Makefile\\n+++ b/docs/Makefile\\n@@ -7,7 +7,7 @@ SPHINXOPTS     ?=\\n SPHINXBUILD    ?= sphinx-build\\n SOURCEDIR      = source\\n BUILDDIR       = build\\n-DATADIR\\t\\t   = _data\\n+DATADIR\\t       = _data\\n PACKAGESOURCE  = source/tutorials_source/package\\n PLATFORMSOURCE = source/tutorials_source/platform\\n \\n\",\"diff --git a/docs/source/getting_started/advanced.rst b/docs/source/getting_started/advanced.rst\\nindex 7e3072a..8e172b2 100644\\n--- a/docs/source/getting_started/advanced.rst\\n+++ b/docs/source/getting_started/advanced.rst\\n@@ -68,7 +68,8 @@ of samples using the\\n \\n The built-in collate class  \\n :py:class:`lightly.data.collate.ImageCollateFunction` provides a set of \\n-common augmentations used in SimCLR and MoCo.\\n+common augmentations used in SimCLR and MoCo. Instead of a single batch of images,\\n+it returns a tuple of two batches of randomly transformed images.\\n \\n Since Gaussian blur and random rotations by 90 degrees are not supported\\n by default in torchvision, we added them to lightly \\n\",\"diff --git a/docs/source/getting_started/lightly_at_a_glance.rst b/docs/source/getting_started/lightly_at_a_glance.rst\\nindex ac5bee0..bf81b53 100644\\n--- a/docs/source/getting_started/lightly_at_a_glance.rst\\n+++ b/docs/source/getting_started/lightly_at_a_glance.rst\\n@@ -45,7 +45,7 @@ Let's now load an image dataset and create a PyTorch dataloader with the collate\\n     import torch\\n \\n     # create a dataset from your image folder\\n-    dataset = data.LightlyDataset(from_folder='./my/cute/cats/dataset/')\\n+    dataset = data.LightlyDataset(input_dir='./my/cute/cats/dataset/')\\n \\n     # build a PyTorch dataloader\\n     dataloader = torch.utils.data.DataLoader(\\n\",\"diff --git a/docs/source/lightly.cli.rst b/docs/source/lightly.cli.rst\\nindex 0d85b3d..67b9514 100644\\n--- a/docs/source/lightly.cli.rst\\n+++ b/docs/source/lightly.cli.rst\\n@@ -74,3 +74,4 @@ Default Settings\\n ^^^^^^^^^^^^^^^^\\n \\n .. literalinclude:: ../../lightly/cli/config/config.yaml\\n+   :language: yaml\\n\",\"diff --git a/docs/source/tutorials/structure_your_input.rst b/docs/source/tutorials/structure_your_input.rst\\nindex 952771e..61ecc53 100644\\n--- a/docs/source/tutorials/structure_your_input.rst\\n+++ b/docs/source/tutorials/structure_your_input.rst\\n@@ -126,7 +126,7 @@ For the structure above, lightly will understand the input as follows:\\n         ...\\n         1,\\n         ...\\n-        10,\\n+        9,\\n     ]\\n \\n Video Folder Datasets\\n@@ -162,28 +162,6 @@ also work on video data. Give it a try!\\n     by working directly on video files, one can save a lot of disc space because the frames do not have to \\n     be exctracted beforehand.\\n \\n-Torchvision Datasets\\n---------------------\\n-\\n-Lightly also supports a direct interface to some of the `torchvision datasets <https://pytorch.org/docs/stable/torchvision/datasets.html>`_.\\n-From the command-line interface, they can easily be specified with the `data` and `root` keyowords. The following torchvision\\n-datasets are currently supported by the lightly Python package:\\n-\\n-* cifar10\\n-* cifar100\\n-* cityscapes\\n-* stl10\\n-* voc07-det\\n-* voc12-det\\n-* voc07-seg\\n-* voc12-seg\\n-\\n-For example, the following command downloads the cifar10 datasets and generates embeddings for all images:\\n-\\n-.. code-block:: bash\\n-\\n-    lightly-embed data='cifar10' root='./'\\n-\\n \\n Embedding Files\\n ---------------\\n@@ -269,7 +247,7 @@ could be useful if you have additional meta information about each sample.\\n Add Custom Embeddings\\n \\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\n \\n-To add custom embeddings you need to add mre embedding columns to the .csv file.\\n+To add custom embeddings you need to add more embedding columns to the .csv file.\\n Make sure you keep the enumeration of the embeddings in correct order.\\n \\n \\n\",\"diff --git a/lightly/api/upload.py b/lightly/api/upload.py\\nindex bf3e70c..21b0586 100644\\n--- a/lightly/api/upload.py\\n+++ b/lightly/api/upload.py\\n@@ -376,7 +376,7 @@ def upload_images_from_folder(path_to_folder: str,\\n     if isinstance(size, tuple) or size > 0:\\n         transform = torchvision.transforms.Resize(size)\\n \\n-    dataset = LightlyDataset(from_folder=path_to_folder, transform=transform)\\n+    dataset = LightlyDataset(input_dir=path_to_folder, transform=transform)\\n     upload_dataset(\\n         dataset,\\n         dataset_id,\\n\",\"diff --git a/lightly/cli/config/config.yaml b/lightly/cli/config/config.yaml\\nindex f4bb9c8..7524b40 100644\\n--- a/lightly/cli/config/config.yaml\\n+++ b/lightly/cli/config/config.yaml\\n@@ -20,11 +20,6 @@ emb_upload_bsz: 32            # Number of embeddings which are uploaded in a si\\n tag_name: 'initial-tag'       # Name of the requested tag on the Lightly platform.\\n \\n ### training and embeddings\\n-# The following arguments are required for training and embedding\\n-# images with the lightly Python package.\\n-data: ''                      # Name of the dataset (optional if input_dir is set)\\n-root: ''                      # Root directory (optional if input_dir is set)\\n-download: True                # Download dataset (optional if input_dir is set)\\n pre_trained: True             # Whether to use a pre-trained model or not\\n \\n # model namespace: Passed to lightly.models.ResNetGenerator.\\n\",\"diff --git a/lightly/cli/embed_cli.py b/lightly/cli/embed_cli.py\\nindex 34667fd..bcbb044 100644\\n--- a/lightly/cli/embed_cli.py\\n+++ b/lightly/cli/embed_cli.py\\n@@ -32,14 +32,7 @@ from lightly.cli._helpers import load_from_state_dict\\n \\n def _embed_cli(cfg, is_cli_call=True):\\n \\n-    data = cfg['data']\\n-    train = cfg.get('train', True)\\n     checkpoint = cfg['checkpoint']\\n-    download = cfg['download']\\n-\\n-    root = cfg['root']\\n-    if root and is_cli_call:\\n-        root = fix_input_path(root)\\n \\n     input_dir = cfg['input_dir']\\n     if input_dir and is_cli_call:\\n@@ -62,9 +55,7 @@ def _embed_cli(cfg, is_cli_call=True):\\n             std=[0.229, 0.224, 0.225])\\n     ])\\n \\n-    dataset = LightlyDataset(root,\\n-                           name=data, train=train, download=download,\\n-                           from_folder=input_dir, transform=transform)\\n+    dataset = LightlyDataset(input_dir, transform=transform)\\n \\n     cfg['loader']['drop_last'] = False\\n     cfg['loader']['shuffle'] = False\\n\",\"diff --git a/lightly/cli/train_cli.py b/lightly/cli/train_cli.py\\nindex de16cc3..753bc87 100644\\n--- a/lightly/cli/train_cli.py\\n+++ b/lightly/cli/train_cli.py\\n@@ -31,13 +31,6 @@ from lightly.cli._helpers import load_from_state_dict\\n \\n def _train_cli(cfg, is_cli_call=True):\\n \\n-    data = cfg['data']\\n-    download = cfg['download']\\n-\\n-    root = cfg['root']\\n-    if root and is_cli_call:\\n-        root = fix_input_path(root)\\n-\\n     input_dir = cfg['input_dir']\\n     if input_dir and is_cli_call:\\n         input_dir = fix_input_path(input_dir)\\n@@ -109,9 +102,7 @@ def _train_cli(cfg, is_cli_call=True):\\n     criterion = NTXentLoss(**cfg['criterion'])\\n     optimizer = torch.optim.SGD(model.parameters(), **cfg['optimizer'])\\n \\n-    dataset = LightlyDataset(root,\\n-                           name=data, train=True, download=download,\\n-                           from_folder=input_dir)\\n+    dataset = LightlyDataset(input_dir)\\n \\n     cfg['loader']['batch_size'] = min(\\n         cfg['loader']['batch_size'],\\n@@ -125,7 +116,7 @@ def _train_cli(cfg, is_cli_call=True):\\n \\n     encoder = SelfSupervisedEmbedding(model, criterion, optimizer, dataloader)\\n     encoder.init_checkpoint_callback(**cfg['checkpoint_callback'])\\n-    encoder = encoder.train_embedding(**cfg['trainer'])\\n+    encoder.train_embedding(**cfg['trainer'])\\n \\n     print('Best model is stored at: %s' % (encoder.checkpoint))\\n     return encoder.checkpoint\\n\",\"diff --git a/lightly/data/_image.py b/lightly/data/_image.py\\nindex e374792..d76b35e 100644\\n--- a/lightly/data/_image.py\\n+++ b/lightly/data/_image.py\\n@@ -25,10 +25,21 @@ def _make_dataset(directory, extensions=None, is_valid_file=None):\\n \\n     \\\"\\\"\\\"\\n \\n+    # handle is_valid_file and extensions the same way torchvision handles them:\\n+    # https://pytorch.org/docs/stable/_modules/torchvision/datasets/folder.html#ImageFolder\\n+    both_none = extensions is None and is_valid_file is None\\n+    both_something = extensions is not None and is_valid_file is not None\\n+    if both_none or both_something:\\n+        raise ValueError('Both extensions and is_valid_file cannot be None or '\\n+                         'not None at the same time')\\n+\\n     if extensions is not None:\\n         def _is_valid_file(filename):\\n             return filename.lower().endswith(extensions)\\n \\n+    if is_valid_file is not None:\\n+        _is_valid_file = is_valid_file\\n+\\n     instances = []\\n     for fname in os.listdir(directory):\\n \\n\",\"diff --git a/lightly/data/_video.py b/lightly/data/_video.py\\nindex ba5d26f..7c8e690 100644\\n--- a/lightly/data/_video.py\\n+++ b/lightly/data/_video.py\\n@@ -158,6 +158,14 @@ def _make_dataset(directory,\\n \\n     \\\"\\\"\\\"\\n \\n+    # handle is_valid_file and extensions the same way torchvision handles them:\\n+    # https://pytorch.org/docs/stable/_modules/torchvision/datasets/folder.html#ImageFolder\\n+    both_none = extensions is None and is_valid_file is None\\n+    both_something = extensions is not None and is_valid_file is not None\\n+    if both_none or both_something:\\n+        raise ValueError('Both extensions and is_valid_file cannot be None or '\\n+                         'not None at the same time')\\n+\\n     # use filename to find valid files\\n     if extensions is not None:\\n         def _is_valid_file(filename):\\n\",\"diff --git a/lightly/data/collate.py b/lightly/data/collate.py\\nindex a697df6..00d2c6b 100644\\n--- a/lightly/data/collate.py\\n+++ b/lightly/data/collate.py\\n@@ -48,11 +48,23 @@ class BaseCollateFunction(nn.Module):\\n                     a LightlyDataset.\\n \\n             Returns:\\n-                A tuple of batches of images, labels, and filenames. All are \\n-                exactly twice as long as the input batch size. For the labels\\n-                and filenames, the second half of the new batch is a copy of \\n-                the first half. For the images, the two halves represent \\n-                different transforms of the original images.\\n+                A tuple of images, labels, and filenames. The images consist of \\n+                two batches corresponding to the two transformations of the\\n+                input images.\\n+\\n+            Examples:\\n+                >>> # define a random transformation and the collate function\\n+                >>> transform = ... # some random augmentations\\n+                >>> collate_fn = BaseCollateFunction(transform)\\n+                >>>\\n+                >>> # input is a batch of tuples (here, batch_size = 1)\\n+                >>> input = [(img, 0, 'my-image.png')]\\n+                >>> output = collate_fn(input)\\n+                >>>\\n+                >>> # output consists of two random transforms of the images,\\n+                >>> # the labels, and the filenames in the batch\\n+                >>> (img_t0, img_t1), label, filename = output\\n+\\n         \\\"\\\"\\\"\\n         batch_size = len(batch)\\n \\n@@ -60,16 +72,17 @@ class BaseCollateFunction(nn.Module):\\n         transforms = [self.transform(batch[i % batch_size][0]).unsqueeze_(0)\\n                       for i in range(2 * batch_size)]\\n         # list of labels\\n-        labels = [batch[i % batch_size][1] for i in range(2 * batch_size)]\\n+        labels = torch.LongTensor([item[1] for item in batch])\\n         # list of filenames\\n         fnames = [item[2] for item in batch]\\n \\n-        tuple_of_batches = (\\n-            torch.cat(transforms, 0),\\n-            torch.LongTensor(labels),\\n-            fnames,\\n+        # tuple of transforms\\n+        transforms = (\\n+            torch.cat(transforms[:batch_size], 0),\\n+            torch.cat(transforms[batch_size:], 0)\\n         )\\n-        return tuple_of_batches\\n+\\n+        return transforms, labels, fnames\\n \\n \\n class ImageCollateFunction(BaseCollateFunction):\\n\",\"diff --git a/lightly/data/dataset.py b/lightly/data/dataset.py\\nindex 78b82e4..4f29dfa 100644\\n--- a/lightly/data/dataset.py\\n+++ b/lightly/data/dataset.py\\n@@ -15,7 +15,81 @@ from lightly.data._helpers import _load_dataset\\n from lightly.data._helpers import DatasetFolder\\n from lightly.data._video import VideoDataset\\n \\n-class LightlyDataset(data.Dataset):\\n+\\n+def _get_filename_by_index(dataset, index):\\n+    \\\"\\\"\\\"Default function which maps the index of an image to a filename.\\n+\\n+    \\\"\\\"\\\"\\n+    if isinstance(dataset, datasets.ImageFolder):\\n+        # filename is the path of the image relative to the dataset root\\n+        full_path = dataset.imgs[index][0]\\n+        return os.path.relpath(full_path, dataset.root)\\n+    elif isinstance(dataset, DatasetFolder):\\n+        # filename is the path of the image relative to the dataset root\\n+        full_path = dataset.samples[index][0]\\n+        return os.path.relpath(full_path, dataset.root)\\n+    elif isinstance(dataset, VideoDataset):\\n+        # filename is constructed by the video dataset\\n+        return dataset.get_filename(index)\\n+    else:\\n+        # dummy to prevent crashes\\n+        return str(index)\\n+\\n+\\n+def _ensure_dir(path):\\n+    \\\"\\\"\\\"Makes sure that the directory at path exists.\\n+\\n+    \\\"\\\"\\\"\\n+    dirname = os.path.dirname(path)\\n+    os.makedirs(dirname, exist_ok=True)\\n+\\n+\\n+def _copy_image(input_dir, output_dir, filename):\\n+    \\\"\\\"\\\"Copies an image from the input directory to the output directory.\\n+\\n+    \\\"\\\"\\\"\\n+    source = os.path.join(input_dir, filename)\\n+    target = os.path.join(output_dir, filename)\\n+    _ensure_dir(target)\\n+    shutil.copyfile(source, target)\\n+\\n+def _save_image(image, output_dir, filename, fmt):\\n+    \\\"\\\"\\\"Saves an image in the output directory.\\n+\\n+    \\\"\\\"\\\"\\n+    target = os.path.join(output_dir, filename)\\n+    _ensure_dir(target)\\n+    try:\\n+        # try to save the image with the specified format or\\n+        # derive the format from the filename (if format=None)\\n+        image.save(target, format=fmt)\\n+    except ValueError:\\n+        # could not determine format from filename\\n+        image.save(target, format='png')\\n+\\n+\\n+def _dump_image(dataset, output_dir, filename, index, fmt):\\n+    \\\"\\\"\\\"Saves a single image to the output directory.\\n+\\n+    Will copy the image from the input directory to the output directory\\n+    if possible. If not (e.g. for VideoDatasets), will load the image and\\n+    then save it to the output directory with the specified format.\\n+\\n+    \\\"\\\"\\\"\\n+\\n+    if isinstance(dataset, datasets.ImageFolder):\\n+        # can safely copy the image from the input to the output directory\\n+        _copy_image(dataset.root, output_dir, filename)\\n+    elif isinstance(dataset, DatasetFolder):\\n+        # can safely copy the image from the input to the output directory\\n+        _copy_image(dataset.root, output_dir, filename)\\n+    else:\\n+        # need to load the image and save it to the output directory\\n+        image, _ = dataset[index]\\n+        _save_image(image, output_dir, filename, fmt)\\n+\\n+\\n+class LightlyDataset:\\n     \\\"\\\"\\\"Provides a uniform data interface for the embedding models.\\n \\n     Should be used for all models and functions in the lightly package.\\n@@ -25,101 +99,119 @@ class LightlyDataset(data.Dataset):\\n     or to load a custom dataset from an input folder.\\n \\n     Args:\\n-        root:\\n-            Directory where the torchvision dataset should be stored.\\n-        name:\\n-            Name of the dataset if it is a torchvision dataset \\n-            (e.g. cifar10, cifar100).\\n-        train:\\n-            Use the training set if it is a torchvision dataset.\\n-        download:\\n-            Whether to download the torchvision dataset.\\n-        from_folder:\\n+        input_dir:\\n             Path to directory holding the images to load.\\n         transform:\\n             Image transforms (as in torchvision).\\n-        indices:\\n-            If provided, ignores samples not in indices.\\n+        index_to_filename:\\n+            Function which takes the dataset and index as input and returns\\n+            the filename of the file at the index. If None, uses default.\\n \\n     Examples:\\n-        >>> import lightly.data as data\\n-        >>> # load cifar10 from torchvision\\n-        >>> dataset = data.LightlyDataset(\\n-        >>>     root='./', name='cifar10', download=True)\\n         >>> # load cifar10 from a local folder\\n-        >>> dataset = data.LightlyDataset(from_folder='path/to/cifar10/')\\n+        >>> import lightly.data as data\\n+        >>> dataset = data.LightlyDataset(input_dir='path/to/cifar10/')\\n         >>> sample, target, fname = dataset[0]\\n \\n     \\\"\\\"\\\"\\n \\n     def __init__(self,\\n-                 root: str = '',\\n-                 name: str = 'cifar10',\\n-                 train: bool = True,\\n-                 download: bool = True,\\n-                 from_folder: str = '',\\n+                 input_dir: str,\\n                  transform=None,\\n-                 indices=None):\\n+                 index_to_filename=None):\\n+\\n+        # can pass input_dir=None to create an \\\"empty\\\" dataset\\n+        self.input_dir = input_dir\\n+        if self.input_dir is not None:\\n+            self.dataset = _load_dataset(self.input_dir, transform)\\n+\\n+        # initialize function to get filename of image\\n+        self.index_to_filename = _get_filename_by_index\\n+        if index_to_filename is  not None:\\n+            self.index_to_filename = index_to_filename\\n \\n-        super(LightlyDataset, self).__init__()\\n-        self.dataset = _load_dataset(\\n-            root, name, train, download, transform, from_folder\\n+    @classmethod\\n+    def from_torch_dataset(cls,\\n+                           dataset,\\n+                           transform=None,\\n+                           index_to_filename=None):\\n+        \\\"\\\"\\\"Builds a LightlyDataset from a PyTorch (or torchvision) dataset.\\n+\\n+        Args:\\n+            dataset:\\n+                PyTorch/torchvision dataset.\\n+            transform:\\n+                Image transforms (as in torchvision).\\n+            index_to_filename:\\n+                Function which takes the dataset and index as input and returns\\n+                the filename of the file at the index. If None, uses default.\\n+\\n+        Returns:\\n+            A LightlyDataset object.\\n+\\n+        Examples:\\n+        >>> # load cifar10 from torchvision\\n+        >>> import torchvision\\n+        >>> import lightly.data as data\\n+        >>> base = torchvision.datasets.CIFAR10(root='./')\\n+        >>> dataset = data.LightlyDataset.from_torch_dataset(base)\\n+\\n+        \\\"\\\"\\\"\\n+        # create an \\\"empty\\\" dataset object\\n+        dataset_obj = cls(\\n+            None,\\n+            transform=transform,\\n+            index_to_filename=index_to_filename\\n         )\\n-        self.root_folder = None\\n-        if from_folder:\\n-            self.root_folder = from_folder\\n-        \\n-        self.indices = indices\\n \\n-    def dump_image(self,\\n-                   output_dir: str,\\n-                   index: int,\\n-                   format: Union[str, None] = None):\\n-        \\\"\\\"\\\"Saves a single image to the output directory.\\n+        # populate it with the torch dataset\\n+        dataset_obj.dataset = dataset\\n+        return dataset_obj\\n \\n-        Will copy the image from the input directory to the output directory\\n-        if possible. If not (e.g. for VideoDatasets), will load the image and\\n-        then save it to the output directory with the specified format.\\n+    def __getitem__(self, index: int):\\n+        \\\"\\\"\\\"Returns (sample, target, fname) of item at index.\\n \\n         Args:\\n-            output_dir:\\n-                Output directory where the image is stored.\\n             index:\\n-                Index of the image to store.\\n-            format:\\n-                Image format.\\n+                Index of the queried item.\\n+\\n+        Returns:\\n+            The image, target, and filename of the item at index.\\n \\n         \\\"\\\"\\\"\\n-        if self.indices is not None:\\n-            index = self.indices[index]\\n+        fname = self.index_to_filename(self.dataset, index)\\n+        sample, target = self.dataset.__getitem__(index)\\n+        \\n+        return sample, target, fname\\n \\n-        image, _ = self.dataset[index]\\n-        filename = self._get_filename_by_index(index)\\n \\n-        source = os.path.join(self.root_folder, filename)\\n-        target = os.path.join(output_dir, filename)\\n+    def __len__(self):\\n+        \\\"\\\"\\\"Returns the length of the dataset.\\n \\n-        dirname = os.path.dirname(target)\\n-        os.makedirs(dirname, exist_ok=True)\\n+        \\\"\\\"\\\"\\n+        return len(self.dataset)\\n \\n-        if os.path.isfile(source):\\n-            # copy the file from the source to the target\\n-            shutil.copyfile(source, target)\\n-        else:\\n-            # the source is not a file (e.g. when loading a video frame)\\n-            try:\\n-                # try to save the image with the specified format or\\n-                # derive the format from the filename (if format=None)\\n-                image.save(target, format=format)\\n-            except ValueError:\\n-                # could not determine format from filename\\n-                image.save(os.path.join(output_dir, filename), format='png')\\n+    def __add__(self, other):\\n+        \\\"\\\"\\\"Adds another item to the dataset.\\n+\\n+        \\\"\\\"\\\"\\n+        raise NotImplementedError()\\n+\\n+    def get_filenames(self) -> List[str]:\\n+        \\\"\\\"\\\"Returns all filenames in the dataset.\\n+\\n+        \\\"\\\"\\\"\\n+        list_of_filenames = []\\n+        for index in range(len(self)):\\n+            fname = self.index_to_filename(self.dataset, index)\\n+            list_of_filenames.append(fname)\\n+        return list_of_filenames\\n \\n     def dump(self,\\n              output_dir: str,\\n              filenames: Union[List[str], None] = None,\\n              format: Union[str, None] = None):\\n-        \\\"\\\"\\\"Saves images to the output directory.\\n+        \\\"\\\"\\\"Saves images in the dataset to the output directory.\\n \\n         Will copy the images from the input directory to the output directory\\n         if possible. If not (e.g. for VideoDatasets), will load the images and\\n@@ -134,82 +226,25 @@ class LightlyDataset(data.Dataset):\\n                 Image format.\\n \\n         \\\"\\\"\\\"\\n-        # make sure no transforms are applied to the images\\n+\\n         if self.dataset.transform is not None:\\n-            pass\\n+            raise RuntimeError('Cannot dump dataset which applies transforms!')\\n \\n         # create directory if it doesn't exist yet\\n         os.makedirs(output_dir, exist_ok=True)\\n \\n-        # get all filenames\\n+        # dump all the files if no filenames were passed, otherwise dump only\\n+        # the ones referenced in the list\\n         if filenames is None:\\n             indices = [i for i in range(self.__len__())]\\n+            filenames = self.get_filenames()\\n         else:\\n-            indices = \\\\\\n-                [i for i, f in enumerate(self.get_filenames()) if f in filenames]\\n+            indices = []\\n+            all_filenames = self.get_filenames()\\n+            for i in range(len(filenames)):\\n+                if filenames[i] in all_filenames:\\n+                    indices.append(i)\\n \\n         # dump images\\n-        for index in indices:\\n-            self.dump_image(output_dir, index, format=format)\\n-\\n-    def get_filenames(self) -> List[str]:\\n-        \\\"\\\"\\\"Returns all filenames in the dataset.\\n-\\n-        \\\"\\\"\\\"\\n-        list_of_filenames = []\\n-        for index in range(len(self)):\\n-            fname = self._get_filename_by_index(index)\\n-            list_of_filenames.append(fname)\\n-        return list_of_filenames\\n-\\n-    def _get_filename_by_index(self, index) -> str:\\n-        \\\"\\\"\\\"Returns filename based on index\\n-        \\n-        \\\"\\\"\\\"\\n-        if self.indices is not None:\\n-            index = self.indices[index]\\n-\\n-        if isinstance(self.dataset, datasets.ImageFolder):\\n-            full_path = self.dataset.imgs[index][0]\\n-            return os.path.relpath(full_path, self.root_folder)\\n-        elif isinstance(self.dataset, DatasetFolder):\\n-            full_path = self.dataset.samples[index][0]\\n-            return os.path.relpath(full_path, self.root_folder)\\n-        elif isinstance(self.dataset, VideoDataset):\\n-            return self.dataset.get_filename(index)\\n-        else:\\n-            return str(index)\\n-\\n-    def __getitem__(self, index):\\n-        \\\"\\\"\\\" Get item at index. Supports torchvision.ImageFolder datasets and\\n-            all dataset which return the tuple (sample, target).\\n-\\n-        Args:\\n-         - index:   index of the queried item\\n-\\n-        Returns:\\n-         - sample:  sample at queried index\\n-         - target:  class_index of target class, 0 if there is no target\\n-         - fname:   filename of the sample, str(index) if there is no filename\\n-\\n-        \\\"\\\"\\\"\\n-        fname = self._get_filename_by_index(index)\\n-        \\n-        if self.indices is not None:\\n-            index = self.indices[index]\\n-        \\n-        sample, target = self.dataset.__getitem__(index)\\n-        \\n-        return sample, target, fname\\n-\\n-    def __len__(self):\\n-        \\\"\\\"\\\"Returns the length of the dataset.\\n-\\n-        \\\"\\\"\\\"\\n-        if self.indices is not None:\\n-            return len(self.indices)\\n-\\n-        return len(self.dataset)\\n-\\n-    def __add__(self, other):\\n-        raise NotImplementedError()\\n+        for i, filename in zip(indices, filenames):\\n+            _dump_image(self.dataset, output_dir, filename, i, fmt=format)\\n\",\"diff --git a/lightly/embedding/_base.py b/lightly/embedding/_base.py\\nindex 3d30de2..14c7bfa 100644\\n--- a/lightly/embedding/_base.py\\n+++ b/lightly/embedding/_base.py\\n@@ -45,16 +45,19 @@ class BaseEmbedding(lightning.LightningModule):\\n         self.checkpoint_callback = None\\n         self.init_checkpoint_callback()\\n \\n-    def forward(self, x):\\n-        return self.model(x)\\n+    def forward(self, x0, x1):\\n+        return self.model(x0, x1)\\n \\n     def training_step(self, batch, batch_idx):\\n \\n-        x, y, _ = batch\\n-        y_hat = self(x)\\n-        loss = self.criterion(y_hat, y)\\n+        # get the two image transformations\\n+        (x0, x1), _, _ = batch\\n+        # forward pass of the transformations\\n+        y0, y1 = self(x0, x1)\\n+        # calculate loss\\n+        loss = self.criterion(y0, y1)\\n+        # log loss and return\\n         self.log('loss', loss)\\n-\\n         return loss\\n \\n     def configure_optimizers(self):\\n@@ -100,40 +103,6 @@ class BaseEmbedding(lightning.LightningModule):\\n         self.checkpoint = self.checkpoint_callback.best_model_path\\n         self.checkpoint = os.path.join(self.cwd, self.checkpoint)\\n \\n-        return self\\n-\\n-    def recycle(self, new_output_dim, layer=None):\\n-        \\\"\\\"\\\"Build a copy of the embedding model with a new output layer\\n-\\n-        Args:\\n-            new_output_dim: (int)\\n-            layer: (int)\\n-\\n-        Returns:\\n-            Copy of the embedding model with new output layer\\n-        \\\"\\\"\\\"\\n-\\n-        layer = -1 if layer is None else layer\\n-\\n-        modules = []\\n-        for module in self.model.features:\\n-            module_copy = copy.deepcopy(module)\\n-            modules.append(module_copy)\\n-\\n-        output_dim = None\\n-        if isinstance(modules[-1], nn.AdaptiveAvgPool2d):\\n-            output_dim = modules[-2].out_channels\\n-        elif isinstance(modules[-1], nn.Linear):\\n-            output_dim = modules[-1].out_features\\n-        else:\\n-            msg = 'Could not determine output_size. Last layer is {}'\\n-            msg = msg.format(type(modules[-1]))\\n-            raise NotImplementedError(msg)\\n-\\n-        modules.append(nn.Flatten())\\n-        modules.append(nn.Linear(output_dim, new_output_dim))\\n-        return nn.Sequential(*modules)\\n-\\n     def embed(self, *args, **kwargs):\\n         \\\"\\\"\\\"Must be implemented by classes which inherit from BaseEmbedding.\\n \\n@@ -165,4 +134,4 @@ class BaseEmbedding(lightning.LightningModule):\\n         self.checkpoint_callback.monitor = monitor\\n \\n         dirpath = self.cwd if dirpath is None else dirpath\\n-        self.checkpoint_callback.dirpath = dirpath\\n\\\\ No newline at end of file\\n+        self.checkpoint_callback.dirpath = dirpath\\n\",\"diff --git a/lightly/embedding/embedding.py b/lightly/embedding/embedding.py\\nindex 5e9f72e..91c75e7 100644\\n--- a/lightly/embedding/embedding.py\\n+++ b/lightly/embedding/embedding.py\\n@@ -6,11 +6,11 @@\\n import time\\n \\n import torch\\n-from lightly import is_prefetch_generator_available\\n+import lightly\\n from lightly.embedding._base import BaseEmbedding\\n from tqdm import tqdm\\n \\n-if is_prefetch_generator_available():\\n+if lightly._is_prefetch_generator_available():\\n     from prefetch_generator import BackgroundGenerator\\n \\n \\n@@ -26,8 +26,6 @@ class SelfSupervisedEmbedding(BaseEmbedding):\\n \\n     The implementation is based on contrastive learning.\\n \\n-    MCM: https://arxiv.org/abs/1906.05849\\n-\\n     SimCLR: https://arxiv.org/abs/2002.05709\\n \\n     MoCo: https://arxiv.org/abs/1911.05722\\n@@ -97,7 +95,7 @@ class SelfSupervisedEmbedding(BaseEmbedding):\\n         self.model.eval()\\n         embeddings, labels, fnames = None, None, []\\n \\n-        if is_prefetch_generator_available():\\n+        if lightly._is_prefetch_generator_available():\\n             pbar = tqdm(BackgroundGenerator(dataloader, max_prefetch=3),\\n                         total=len(dataloader))\\n         else:\\n@@ -140,15 +138,3 @@ class SelfSupervisedEmbedding(BaseEmbedding):\\n                 labels = labels.cpu().numpy()\\n \\n         return embeddings, labels, fnames\\n-\\n-\\n-class _VAEEmbedding(BaseEmbedding):\\n-    \\\"\\\"\\\" Unsupervised embedding based on variational auto-encoders.\\n-\\n-    \\\"\\\"\\\"\\n-\\n-    def embed(self, dataloader):\\n-        \\\"\\\"\\\" TODO\\n-\\n-        \\\"\\\"\\\"\\n-        raise NotImplementedError(\\\"This site is under construction...\\\")\\n\",\"diff --git a/lightly/loss/memory_bank.py b/lightly/loss/memory_bank.py\\nindex 350518c..ca20f12 100644\\n--- a/lightly/loss/memory_bank.py\\n+++ b/lightly/loss/memory_bank.py\\n@@ -88,7 +88,8 @@ class MemoryBankModule(torch.nn.Module):\\n \\n     def forward(self,\\n                 output: torch.Tensor,\\n-                labels: torch.Tensor = None):\\n+                labels: torch.Tensor = None,\\n+                update: bool = False):\\n         \\\"\\\"\\\"Query memory bank for additional negative samples\\n \\n         Args:\\n@@ -107,8 +108,7 @@ class MemoryBankModule(torch.nn.Module):\\n         if self.size == 0:\\n             return output, None\\n \\n-        batch_size, dim = output.shape\\n-        batch_size = batch_size // 2\\n+        _, dim = output.shape\\n \\n         # initialize the memory bank if it is not already done\\n         if self.bank is None:\\n@@ -118,6 +118,7 @@ class MemoryBankModule(torch.nn.Module):\\n         bank = self.bank.clone().detach()\\n \\n         # only update memory bank if we later do backward pass (gradient)\\n-        if output.requires_grad:\\n-            self._dequeue_and_enqueue(output[batch_size:])\\n+        if update:\\n+            self._dequeue_and_enqueue(output)\\n+\\n         return output, bank\\n\",\"diff --git a/lightly/models/moco.py b/lightly/models/moco.py\\nindex a2d525d..9837c16 100644\\n--- a/lightly/models/moco.py\\n+++ b/lightly/models/moco.py\\n@@ -59,44 +59,77 @@ class MoCo(nn.Module, _MomentumEncoderMixin):\\n         # initialize momentum features and momentum projection head\\n         self._init_momentum_encoder()\\n \\n-    def forward(self, x: torch.Tensor):\\n+    def forward(self,\\n+                x0: torch.Tensor,\\n+                x1: torch.Tensor = None,\\n+                return_features: bool = False):\\n         \\\"\\\"\\\"Embeds and projects the input image.\\n \\n-        Splits the input batch into q and k following the notation of MoCo.\\n-        Extracts features with the ResNet backbone and applies the projection\\n-        head to the output space.\\n+        Performs the momentum update, extracts features with the backbone and \\n+        applies the projection head to the output space. If both x0 and x1 are\\n+        not None, both will be passed through the backbone and projection head.\\n+        If x1 is None, only x0 will be forwarded.\\n \\n         Args:\\n-            x:\\n-                Tensor of shape bsz x channels x W x H\\n+            x0:\\n+                Tensor of shape bsz x channels x W x H.\\n+            x1:\\n+                Tensor of shape bsz x channels x W x H.\\n+            return_features:\\n+                Whether or not to return the intermediate features backbone(x).\\n \\n         Returns:\\n-            Tensor of shape bsz x out_dim\\n+            The output projection of x0 and (if x1 is not None) the output\\n+            projection of x1. If return_features is True, the output for each x\\n+            is a tuple (out, f) where f are the features before the projection\\n+            head.\\n+\\n+        Examples:\\n+            >>> # single input, single output\\n+            >>> out = model(x) \\n+            >>> \\n+            >>> # single input with return_features=True\\n+            >>> out, f = model(x, return_features=True)\\n+            >>>\\n+            >>> # two inputs, two outputs\\n+            >>> out0, out1 = model(x0, x1)\\n+            >>>\\n+            >>> # two inputs, two outputs with return_features=True\\n+            >>> (out0, f0), (out1, f1) = model(x0, x1, return_features=True)\\n \\n         \\\"\\\"\\\"\\n         self._momentum_update(self.m)\\n-\\n-        # adopting the notation of the moco paper\\n-        batch_size = x.shape[0] // 2\\n-        q = x[:batch_size]\\n-        k = x[batch_size:]\\n         \\n-        # embed queries\\n-        emb_q = self.backbone(q).squeeze()\\n-        out_q = self.projection_head(emb_q)\\n+        # forward pass of first input x0\\n+        f0 = self.backbone(x0).squeeze()\\n+        out0 = self.projection_head(f0)\\n+\\n+        # append features if requested\\n+        if return_features:\\n+            out0 = (out0, f0)\\n \\n-        # embed keys\\n+        # return out0 if x1 is None\\n+        if x1 is None:\\n+            return out0\\n+\\n+        # forward pass of second input x1\\n         with torch.no_grad():\\n \\n             # shuffle for batchnorm\\n             if self.batch_shuffle:\\n-                k, shuffle = self._batch_shuffle(k)\\n+                x1, shuffle = self._batch_shuffle(x1)\\n \\n-            emb_k = self.momentum_backbone(k).squeeze()\\n-            out_k = self.momentum_projection_head(emb_k).detach()\\n+            # run x1 through momentum encoder\\n+            f1 = self.momentum_backbone(x1).squeeze()\\n+            out1 = self.momentum_projection_head(f1).detach()\\n         \\n             # unshuffle for batchnorm\\n             if self.batch_shuffle:\\n-                out_k = self._batch_unshuffle(out_k, shuffle)\\n+                f1 = self._batch_unshuffle(f1, shuffle)\\n+                out1 = self._batch_unshuffle(out1, shuffle)\\n+\\n+            # append features if requested\\n+            if return_features:\\n+                out1 = (out1, f1)\\n \\n-        return torch.cat([out_q, out_k], axis=0)\\n+        return out0, out1\\n\",\"diff --git a/lightly/models/simclr.py b/lightly/models/simclr.py\\nindex 4cefe59..aed66c2 100644\\n--- a/lightly/models/simclr.py\\n+++ b/lightly/models/simclr.py\\n@@ -46,22 +46,65 @@ class SimCLR(nn.Module):\\n         self.backbone = backbone\\n         self.projection_head = _get_simclr_projection_head(num_ftrs, out_dim)\\n \\n-    def forward(self, x: torch.Tensor):\\n+    def forward(self,\\n+                x0: torch.Tensor,\\n+                x1: torch.Tensor = None,\\n+                return_features: bool = False):\\n         \\\"\\\"\\\"Embeds and projects the input images.\\n \\n         Extracts features with the backbone and applies the projection\\n-        head to the output space.\\n+        head to the output space. If both x0 and x1 are not None, both will be\\n+        passed through the backbone and projection head. If x1 is None, only\\n+        x0 will be forwarded.\\n \\n         Args:\\n-            x:\\n-                Tensor of shape bsz x channels x W x H\\n+            x0:\\n+                Tensor of shape bsz x channels x W x H.\\n+            x1:\\n+                Tensor of shape bsz x channels x W x H.\\n+            return_features:\\n+                Whether or not to return the intermediate features backbone(x).\\n \\n         Returns:\\n-            Tensor of shape bsz x out_dim\\n+            The output projection of x0 and (if x1 is not None) the output\\n+            projection of x1. If return_features is True, the output for each x\\n+            is a tuple (out, f) where f are the features before the projection\\n+            head.\\n+\\n+        Examples:\\n+            >>> # single input, single output\\n+            >>> out = model(x) \\n+            >>> \\n+            >>> # single input with return_features=True\\n+            >>> out, f = model(x, return_features=True)\\n+            >>>\\n+            >>> # two inputs, two outputs\\n+            >>> out0, out1 = model(x0, x1)\\n+            >>>\\n+            >>> # two inputs, two outputs with return_features=True\\n+            >>> (out0, f0), (out1, f1) = model(x0, x1, return_features=True)\\n \\n         \\\"\\\"\\\"\\n-        # embed images in feature space\\n-        emb = self.backbone(x).squeeze()\\n+        \\n+        # forward pass of first input x0\\n+        f0 = self.backbone(x0).squeeze()\\n+        out0 = self.projection_head(f0)\\n \\n-        # return projection to space for loss calcs\\n-        return self.projection_head(emb)\\n+        # append features if requested\\n+        if return_features:\\n+            out0 = (out0, f0)\\n+\\n+        # return out0 if x1 is None\\n+        if x1 is None:\\n+            return out0\\n+\\n+        # forward pass of second input x1\\n+        f1 = self.backbone(x1).squeeze()\\n+        out1 = self.projection_head(f1)\\n+\\n+        # append features if requested\\n+        if return_features:\\n+            out1 = (out1, f1)\\n+\\n+        # return both outputs\\n+        return out0, out1\\n\"]", "test_patch": "[\"diff --git a/docs/source/tutorials_source/package/tutorial_moco_memory_bank.py b/docs/source/tutorials_source/package/tutorial_moco_memory_bank.py\\nindex 8e6e4c8..8cdc52b 100644\\n--- a/docs/source/tutorials_source/package/tutorial_moco_memory_bank.py\\n+++ b/docs/source/tutorials_source/package/tutorial_moco_memory_bank.py\\n@@ -146,7 +146,7 @@ test_transforms = torchvision.transforms.Compose([\\n \\n # We use the moco augmentations for training moco\\n dataset_train_moco = lightly.data.LightlyDataset(\\n-    from_folder=path_to_train\\n+    input_dir=path_to_train\\n )\\n \\n # Since we also train a linear classifier on the pre-trained moco model we\\n@@ -155,12 +155,12 @@ dataset_train_moco = lightly.data.LightlyDataset(\\n # Our linear layer will be trained using cross entropy loss and labels provided\\n # by the dataset. Therefore we chose light augmentations.)\\n dataset_train_classifier = lightly.data.LightlyDataset(\\n-    from_folder=path_to_train,\\n+    input_dir=path_to_train,\\n     transform=train_classifier_transforms\\n )\\n \\n dataset_test = lightly.data.LightlyDataset(\\n-    from_folder=path_to_test,\\n+    input_dir=path_to_test,\\n     transform=test_transforms\\n )\\n \\n@@ -237,9 +237,9 @@ class MocoModel(pl.LightningModule):\\n                 name, params, self.current_epoch)\\n \\n     def training_step(self, batch, batch_idx):\\n-        x, y, _ = batch\\n-        projection = self.resnet_moco(x)\\n-        loss = self.criterion(projection)\\n+        (x0, x1), _, _ = batch\\n+        y0, y1 = self.resnet_moco(x0, x1)\\n+        loss = self.criterion(y0, y1)\\n         self.log('train_loss_ssl', loss)\\n         return loss\\n \\n\",\"diff --git a/docs/source/tutorials_source/package/tutorial_simclr_clothing.py b/docs/source/tutorials_source/package/tutorial_simclr_clothing.py\\nindex 3519870..dd72892 100644\\n--- a/docs/source/tutorials_source/package/tutorial_simclr_clothing.py\\n+++ b/docs/source/tutorials_source/package/tutorial_simclr_clothing.py\\n@@ -71,7 +71,7 @@ pl.seed_everything(seed)\\n # Make sure `path_to_data` points to the downloaded clothing dataset.\\n # You can download it using \\n # `git clone https://github.com/alexeygrigorev/clothing-dataset.git`\\n-path_to_data = '/datasets/clothing_dataset/images'\\n+path_to_data = '/datasets/clothing-dataset/images'\\n \\n \\n # %%\\n@@ -105,11 +105,11 @@ test_transforms = torchvision.transforms.Compose([\\n ])\\n \\n dataset_train_simclr = lightly.data.LightlyDataset(\\n-    from_folder=path_to_data\\n+    input_dir=path_to_data\\n )\\n \\n dataset_test = lightly.data.LightlyDataset(\\n-    from_folder=path_to_data,\\n+    input_dir=path_to_data,\\n     transform=test_transforms\\n )\\n \\n@@ -167,14 +167,14 @@ gpus = 1 if torch.cuda.is_available() else 0\\n # --------------------\\n # The encoder itself wraps a PyTorch-Lightning module. We can pass any \\n # lightning trainer parameter (e.g. gpus=, max_epochs=) to the train_embedding method.\\n-encoder = encoder.train_embedding(gpus=gpus, \\n-                                  progress_bar_refresh_rate=100,\\n-                                  max_epochs=max_epochs)\\n+encoder.train_embedding(gpus=gpus, \\n+                        progress_bar_refresh_rate=100,\\n+                        max_epochs=max_epochs)\\n \\n # %%\\n # Now, let's make sure we move the trained model to the gpu if we have one\\n device = 'cuda' if gpus==1 else 'cpu'\\n-encoder.to(device)\\n+encoder = encoder.to(device)\\n \\n # %%\\n # We can use the .embed method to create an embedding of the dataset. The method\\n@@ -264,9 +264,12 @@ encoder = lightly.embedding.SelfSupervisedEmbedding(\\n     optimizer,\\n     dataloader_train_simclr\\n )\\n-encoder = encoder.train_embedding(gpus=gpus, \\n-                                  progress_bar_refresh_rate=100,\\n-                                  max_epochs=max_epochs).to(device)\\n+\\n+encoder.train_embedding(gpus=gpus, \\n+                        progress_bar_refresh_rate=100,\\n+                        max_epochs=max_epochs)\\n+encoder = encoder.to(device)\\n+\\n embeddings, _, fnames = encoder.embed(dataloader_test, device=device)\\n embeddings = normalize(embeddings)\\n \\n@@ -278,4 +281,4 @@ plot_knn_examples(embeddings)\\n # What's next?\\n \\n # You could use the pre-trained model and train a classifier on top.\\n-pretrained_resnet_backbone = model.backbone\\n\\\\ No newline at end of file\\n+pretrained_resnet_backbone = model.backbone\\n\",\"diff --git a/lightly/__init__.py b/lightly/__init__.py\\nindex 5ee07a5..306093d 100644\\n--- a/lightly/__init__.py\\n+++ b/lightly/__init__.py\\n@@ -94,7 +94,7 @@ else:\\n     else:\\n         _prefetch_generator_available = True\\n \\n-    def is_prefetch_generator_available():\\n+    def _is_prefetch_generator_available():\\n         return _prefetch_generator_available\\n \\n     # import core functionalities\\n@@ -104,7 +104,7 @@ else:\\n \\n \\n     # compare current version v0 to other version v1\\n-    def version_compare(v0, v1):\\n+    def _version_compare(v0, v1):\\n         v0 = [int(n) for n in v0.split('.')][::-1]\\n         v1 = [int(n) for n in v1.split('.')][::-1]\\n         pairs = list(zip(v0, v1))[::-1]\\n@@ -117,7 +117,7 @@ else:\\n \\n \\n     # message if current version is not latest version\\n-    def pretty_print_latest_version(latest_version, width=70):\\n+    def _pretty_print_latest_version(latest_version, width=70):\\n         lines = [\\n             'There is a newer version of the package available.',\\n             'For compatability reasons, please upgrade your current version.',\\n@@ -130,10 +130,10 @@ else:\\n \\n \\n     # check for latest version\\n-    from lightly.api import get_version\\n-    latest_version = get_version(__version__)\\n-    if latest_version is not None:\\n-        if version_compare(__version__, latest_version) < 0:\\n+    from lightly.api import get_version as _get_version\\n+    _latest_version = _get_version(__version__)\\n+    if _latest_version is not None:\\n+        if _version_compare(__version__, _latest_version) < 0:\\n             # local version is behind latest version\\n             # pretty_print_latest_version(latest_version)\\n             pass\\n\",\"diff --git a/lightly/data/_helpers.py b/lightly/data/_helpers.py\\nindex 14b74d8..fb44ed0 100644\\n--- a/lightly/data/_helpers.py\\n+++ b/lightly/data/_helpers.py\\n@@ -93,12 +93,8 @@ def _load_dataset_from_folder(root: str, transform):\\n     return dataset\\n \\n \\n-def _load_dataset(root: str = '',\\n-                  name: str = 'cifar10',\\n-                  train: bool = True,\\n-                  download: bool = True,\\n-                  transform=None,\\n-                  from_folder: str = ''):\\n+def _load_dataset(input_dir: str,\\n+                  transform=None):\\n     \\\"\\\"\\\"Initializes dataset from torchvision or from folder.\\n \\n     Args:\\n@@ -117,80 +113,7 @@ def _load_dataset(root: str = '',\\n \\n     \\\"\\\"\\\"\\n \\n-    if from_folder and os.path.exists(from_folder):\\n-        # load data from directory\\n-        dataset = _load_dataset_from_folder(from_folder,\\n-                                            transform)\\n-\\n-    elif name.lower() == 'cifar10' and root:\\n-        # load cifar10\\n-        dataset = datasets.CIFAR10(root,\\n-                                   train=train,\\n-                                   download=download,\\n-                                   transform=transform)\\n-\\n-    elif name.lower() == 'cifar100' and root:\\n-        # load cifar100\\n-        dataset = datasets.CIFAR100(root,\\n-                                    train=train,\\n-                                    download=download,\\n-                                    transform=transform)\\n-\\n-    elif name.lower() == 'cityscapes' and root:\\n-        # load cityscapes\\n-        root = os.path.join(root, 'cityscapes/')\\n-        split = 'train' if train else 'val'\\n-        dataset = datasets.Cityscapes(root,\\n-                                      split=split,\\n-                                      transform=transform)\\n-\\n-    elif name.lower() == 'stl10' and root:\\n-        # load stl10\\n-        split = 'train' if train else 'test'\\n-        dataset = datasets.STL10(root,\\n-                                 split=split,\\n-                                 download=download,\\n-                                 transform=transform)\\n-\\n-    elif name.lower() == 'voc07-seg' and root:\\n-        # load pascal voc 07 segmentation dataset\\n-        image_set = 'train' if train else 'val'\\n-        dataset = datasets.VOCSegmentation(root,\\n-                                           year='2007',\\n-                                           image_set=image_set,\\n-                                           download=download,\\n-                                           transform=transform)\\n-\\n-    elif name.lower() == 'voc12-seg' and root:\\n-        # load pascal voc 12 segmentation dataset\\n-        image_set = 'train' if train else 'val'\\n-        dataset = datasets.VOCSegmentation(root,\\n-                                           year='2012',\\n-                                           image_set=image_set,\\n-                                           download=download,\\n-                                           transform=transform)\\n-\\n-    elif name.lower() == 'voc07-det' and root:\\n-        # load pascal voc 07 object detection dataset\\n-        image_set = 'train' if train else 'val'\\n-        dataset = datasets.VOCDetection(root,\\n-                                        year='2007',\\n-                                        image_set=image_set,\\n-                                        download=True,\\n-                                        transform=transform)\\n-\\n-    elif name.lower() == 'voc12-det' and root:\\n-        # load pascal voc 12 object detection dataset\\n-        image_set = 'train' if train else 'val'\\n-        dataset = datasets.VOCDetection(root,\\n-                                        year='2012',\\n-                                        image_set=image_set,\\n-                                        download=True,\\n-                                        transform=transform)\\n+    if not os.path.exists(input_dir):\\n+        raise ValueError(f'The input directory {input_dir} does not exist!')\\n \\n-    else:\\n-        raise ValueError(\\n-            'The specified dataset (%s) or datafolder (%s) does not exist '\\n-            % (name, from_folder))\\n-\\n-    return dataset\\n+    return _load_dataset_from_folder(input_dir, transform)\\n\",\"diff --git a/lightly/loss/ntx_ent_loss.py b/lightly/loss/ntx_ent_loss.py\\nindex e829920..1f08079 100644\\n--- a/lightly/loss/ntx_ent_loss.py\\n+++ b/lightly/loss/ntx_ent_loss.py\\n@@ -97,44 +97,48 @@ class NTXentLoss(MemoryBankModule):\\n         v = self._cosine_similarity(x.unsqueeze(1), y.unsqueeze(0))\\n         return v\\n \\n-    def forward(self, output: torch.Tensor, labels: torch.Tensor = None):\\n+    def forward(self,\\n+                out0: torch.Tensor,\\n+                out1: torch.Tensor):\\n         \\\"\\\"\\\"Forward pass through Contrastive Cross Entropy Loss.\\n \\n+        If used with a memory bank, the samples from the memory bank are used\\n+        as negative examples. Otherwise, within-batch samples are used as \\n+        negative samples.\\n+\\n             Args:\\n-                output:\\n-                    Output from the model with shape: 2*bsz x d.\\n-                labels:\\n-                    Labels associated with the inputs.\\n+                out0:\\n+                    Output projections of the first set of transformed images.\\n+                out1:\\n+                    Output projections of the second set of transformed images.\\n \\n             Returns:\\n                 Contrastive Cross Entropy Loss value.\\n \\n-            Raises:\\n-                ValueError if shape of output is not multiple of batch_size.\\n         \\\"\\\"\\\"\\n \\n-        if output.shape[0] % 2:\\n-            raise ValueError('Expected output of shape 2*bsz x dim but got '\\n-                             f'shape {output.shape[0]} x {output.shape[1]}.')\\n-\\n-        device = output.device\\n-        batch_size, dim = output.shape\\n-        batch_size = batch_size // 2\\n+        device = out0.device\\n+        batch_size, _ = out0.shape\\n \\n         # normalize the output to length 1\\n-        output = torch.nn.functional.normalize(output, dim=1)\\n+        out0 = torch.nn.functional.normalize(out0, dim=1)\\n+        out1 = torch.nn.functional.normalize(out1, dim=1)\\n \\n-        # ask memory bank for negative samples\\n-        output, negatives = super(NTXentLoss, self).forward(output)\\n+        # ask memory bank for negative samples and extend it with out1 if \\n+        # out1 requires a gradient, otherwise keep the same vectors in the \\n+        # memory bank (this allows for keeping the memory bank constant e.g.\\n+        # for evaluating the loss on the test set)\\n+        out1, negatives = \\\\\\n+            super(NTXentLoss, self).forward(out1, update=out0.requires_grad)\\n \\n         if negatives is not None:\\n             negatives = negatives.to(device)\\n-            q, k = output[:batch_size], output[batch_size:]\\n             # use negatives from memory bank\\n-            l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)\\n-            l_neg = torch.einsum('nc,ck->nk', [q, negatives.clone().detach()])\\n+            l_pos = torch.einsum('nc,nc->n', [out0, out1]).unsqueeze(-1)\\n+            l_neg = torch.einsum('nc,ck->nk', [out0, negatives.clone().detach()])\\n         else:\\n             # use other samples from batch as negatives\\n+            output = torch.cat((out0, out1), axis=0)\\n             similarity_matrix = self.similarity_function(output, output)\\n \\n             # filter out the scores from the positive samples\\n\",\"diff --git a/tests/data/test_LightlyDataset.py b/tests/data/test_LightlyDataset.py\\nindex c15ac40..11cb68d 100644\\n--- a/tests/data/test_LightlyDataset.py\\n+++ b/tests/data/test_LightlyDataset.py\\n@@ -86,7 +86,7 @@ class TestLightlyDataset(unittest.TestCase):\\n             n_samples_per_subfolder\\n         )\\n \\n-        dataset = LightlyDataset(from_folder=dataset_dir)\\n+        dataset = LightlyDataset(input_dir=dataset_dir)\\n         filenames = dataset.get_filenames()\\n \\n         fnames = []\\n@@ -124,7 +124,7 @@ class TestLightlyDataset(unittest.TestCase):\\n             data[0].save(path)\\n \\n         # create lightly dataset\\n-        dataset = LightlyDataset(from_folder=tmp_dir)\\n+        dataset = LightlyDataset(input_dir=tmp_dir)\\n         filenames = dataset.get_filenames()\\n \\n         # tests\\n@@ -135,33 +135,18 @@ class TestLightlyDataset(unittest.TestCase):\\n         for i in range(n_tot):\\n             sample, target, fname = dataset[i]\\n \\n-    @pytest.mark.slow\\n-    def test_create_lightly_dataset_from_torchvision(self):\\n-        tmp_dir = tempfile.mkdtemp()\\n-\\n-        for dataset_name in self.available_dataset_names:\\n-            dataset = LightlyDataset(root=tmp_dir, name=dataset_name)\\n-            self.assertIsNotNone(dataset)\\n-    \\n-        shutil.rmtree(tmp_dir)\\n-\\n-    def test_not_existing_torchvision_dataset(self):\\n-        list_of_non_existing_names = [\\n-            'a-random-dataset',\\n-            'cifar-100',\\n-            'googleset_ 200'\\n-        ]\\n-        tmp_dir = tempfile.mkdtemp() \\n-        for dataset_name in list_of_non_existing_names:\\n-            with self.assertRaises(ValueError):\\n-                LightlyDataset(root=tmp_dir, name=dataset_name)\\n-\\n     def test_not_existing_folder_dataset(self):\\n         with self.assertRaises(ValueError):\\n             LightlyDataset(\\n-                from_folder='/a-random-hopefully-non/existing-path-to-nowhere/'\\n+                '/a-random-hopefully-non/existing-path-to-nowhere/'\\n             )\\n \\n+    def test_from_torch_dataset(self):\\n+        _dataset = torchvision.datasets.FakeData(size=1, image_size=(3, 32, 32))\\n+        dataset = LightlyDataset.from_torch_dataset(_dataset)\\n+        self.assertEqual(len(_dataset), len(dataset))\\n+        self.assertEqual(len(dataset.get_filenames()), len(dataset))\\n+\\n     def test_video_dataset(self):\\n \\n         if not VIDEO_DATASET_AVAILABLE:\\n@@ -175,7 +160,7 @@ class TestLightlyDataset(unittest.TestCase):\\n             image.save(path)\\n             os.rename(path, os.path.join(tmp_dir, 'my_file.avi'))\\n             with self.assertRaises(ImportError):\\n-                dataset = LightlyDataset(from_folder=tmp_dir)\\n+                dataset = LightlyDataset(input_dir=tmp_dir)\\n \\n             warnings.warn(\\n                 'Did not test video dataset because of missing requirements')        \\n@@ -183,38 +168,8 @@ class TestLightlyDataset(unittest.TestCase):\\n             return\\n         \\n         self.create_video_dataset()\\n-        dataset = LightlyDataset(from_folder=self.input_dir)\\n+        dataset = LightlyDataset(input_dir=self.input_dir)\\n \\n         out_dir = tempfile.mkdtemp()\\n         dataset.dump(out_dir)\\n         self.assertEqual(len(os.listdir(out_dir)), len(dataset))\\n-\\n-    def test_create_lightly_with_indices(self):\\n-        n_subfolders = 5\\n-        n_samples_per_subfolder = 10\\n-        n_tot_files = n_subfolders * n_samples_per_subfolder\\n-\\n-        indices = random.sample(range(n_tot_files), 20)\\n-\\n-        dataset_dir, folder_names, sample_names = self.create_dataset(\\n-            n_subfolders,\\n-            n_samples_per_subfolder\\n-        )\\n-\\n-        dataset = LightlyDataset(from_folder=dataset_dir, indices=indices)\\n-        self.assertEqual(len(dataset), 20)\\n-        self.assertLess(len(dataset), n_tot_files)\\n-        \\n-        filenames = dataset.get_filenames()\\n-        self.assertEqual(len(filenames), 20)\\n-        \\n-        fnames = []\\n-        for dir_name in folder_names:\\n-            for fname in sample_names:\\n-                fnames.append(os.path.join(dir_name, fname))\\n-        \\n-        fnames = [fnames[i] for i in indices]\\n-\\n-        self.assertListEqual(sorted(fnames), sorted(filenames))\\n-\\n-        shutil.rmtree(dataset_dir)\\n\\\\ No newline at end of file\\n\",\"diff --git a/tests/data/test_data_collate.py b/tests/data/test_data_collate.py\\nindex 63340e9..069d243 100644\\n--- a/tests/data/test_data_collate.py\\n+++ b/tests/data/test_data_collate.py\\n@@ -29,17 +29,21 @@ class TestDataCollate(unittest.TestCase):\\n         transform = transforms.ToTensor()\\n         collate = BaseCollateFunction(transform)\\n         samples, labels, fnames = collate(batch)\\n+        samples0, samples1 = samples\\n \\n         self.assertIsNotNone(collate)\\n-        self.assertEqual(len(samples), len(labels), len(fnames))\\n+        self.assertEqual(len(samples0), len(samples1))\\n+        self.assertEqual(len(samples1), len(labels), len(fnames))\\n \\n     def test_image_collate(self):\\n         batch = self.create_batch()\\n         img_collate = ImageCollateFunction()\\n         samples, labels, fnames = img_collate(batch)\\n+        samples0, samples1 = samples\\n \\n         self.assertIsNotNone(img_collate)\\n-        self.assertEqual(len(samples), len(labels), len(fnames))\\n+        self.assertEqual(len(samples0), len(samples1))\\n+        self.assertEqual(len(samples1), len(labels), len(fnames))\\n \\n     def test_image_collate_tuple_input_size(self):\\n         batch = self.create_batch()\\n@@ -47,9 +51,11 @@ class TestDataCollate(unittest.TestCase):\\n             input_size=(32, 32),\\n         )\\n         samples, labels, fnames = img_collate(batch)\\n+        samples0, samples1 = samples\\n \\n         self.assertIsNotNone(img_collate)\\n-        self.assertEqual(len(samples), len(labels), len(fnames)) \\n+        self.assertEqual(len(samples0), len(samples1))\\n+        self.assertEqual(len(samples1), len(labels), len(fnames))\\n \\n     def test_simclr_collate_tuple_input_size(self):\\n         batch = self.create_batch()\\n@@ -57,7 +63,8 @@ class TestDataCollate(unittest.TestCase):\\n             input_size=(32, 32),\\n         )\\n         samples, labels, fnames = img_collate(batch)\\n+        samples0, samples1 = samples\\n \\n         self.assertIsNotNone(img_collate)\\n-        self.assertEqual(len(samples), len(labels), len(fnames)) \\n-\\n+        self.assertEqual(len(samples0), len(samples1))\\n+        self.assertEqual(len(samples1), len(labels), len(fnames))\\n\",\"diff --git a/tests/loss/test_MemoryBank.py b/tests/loss/test_MemoryBank.py\\nindex 0657e7d..4c0730e 100644\\n--- a/tests/loss/test_MemoryBank.py\\n+++ b/tests/loss/test_MemoryBank.py\\n@@ -21,11 +21,13 @@ class TestNTXentLoss(unittest.TestCase):\\n \\n             output = torch.randn(2 * bsz, dim)\\n             output.requires_grad = True\\n-            _, curr_memory_bank = memory_bank(output)\\n+            out0, out1 = output[:bsz], output[bsz:]\\n+\\n+            _, curr_memory_bank = memory_bank(out1, update=True)\\n             next_memory_bank = memory_bank.bank\\n \\n-            curr_diff = output[bsz:].T - curr_memory_bank[:, ptr:ptr + bsz]\\n-            next_diff = output[bsz:].T - next_memory_bank[:, ptr:ptr + bsz]\\n+            curr_diff = out0.T - curr_memory_bank[:, ptr:ptr + bsz]\\n+            next_diff = out1.T - next_memory_bank[:, ptr:ptr + bsz]\\n \\n             # the current memory bank should not hold the batch yet\\n             self.assertGreater(curr_diff.norm(), 1e-5)\\n@@ -44,5 +46,5 @@ class TestNTXentLoss(unittest.TestCase):\\n \\n             # see if there are any problems when the bank size\\n             # is no multiple of the batch size\\n-            output = torch.randn(2 * bsz, dim)\\n+            output = torch.randn(bsz, dim)\\n             _, _ = memory_bank(output)\\n\",\"diff --git a/tests/loss/test_NTXentLoss.py b/tests/loss/test_NTXentLoss.py\\nindex e282932..adec382 100644\\n--- a/tests/loss/test_NTXentLoss.py\\n+++ b/tests/loss/test_NTXentLoss.py\\n@@ -29,8 +29,8 @@ class TestNTXentLoss(unittest.TestCase):\\n             batch_2 = torch.randn((bsz, 32))\\n \\n             # symmetry\\n-            l1 = loss(torch.cat((batch_1, batch_2), 0))\\n-            l2 = loss(torch.cat((batch_2, batch_1), 0))\\n+            l1 = loss(batch_1, batch_2)\\n+            l2 = loss(batch_2, batch_1)\\n             self.assertAlmostEqual((l1 - l2).pow(2).item(), 0.)\\n \\n     def test_forward_pass_1d(self):\\n@@ -41,8 +41,8 @@ class TestNTXentLoss(unittest.TestCase):\\n             batch_2 = torch.randn((bsz, 1))\\n \\n             # symmetry\\n-            l1 = loss(torch.cat((batch_1, batch_2), 0))\\n-            l2 = loss(torch.cat((batch_2, batch_1), 0))\\n+            l1 = loss(batch_1, batch_2)\\n+            l2 = loss(batch_2, batch_1)\\n             self.assertAlmostEqual((l1 - l2).pow(2).item(), 0.)\\n \\n     def test_forward_pass_neg_temp(self):\\n@@ -53,15 +53,16 @@ class TestNTXentLoss(unittest.TestCase):\\n             batch_2 = torch.randn((bsz, 32))\\n \\n             # symmetry\\n-            l1 = loss(torch.cat((batch_1, batch_2), 0))\\n-            l2 = loss(torch.cat((batch_2, batch_1), 0))\\n+            l1 = loss(batch_1, batch_2)\\n+            l2 = loss(batch_2, batch_1)\\n             self.assertAlmostEqual((l1 - l2).pow(2).item(), 0.)\\n     \\n     def test_forward_pass_memory_bank(self):\\n         loss = NTXentLoss(memory_bank_size=64)\\n         for bsz in range(1, 20):\\n-            batch = torch.randn(2*bsz, 32)\\n-            l = loss(batch)\\n+            batch_1 = torch.randn((bsz, 32))\\n+            batch_2 = torch.randn((bsz, 32))\\n+            l = loss(batch_1, batch_2)\\n \\n     def test_forward_pass_memory_bank_cuda(self):\\n         if not torch.cuda.is_available():\\n@@ -69,8 +70,9 @@ class TestNTXentLoss(unittest.TestCase):\\n \\n         loss = NTXentLoss(memory_bank_size=64)\\n         for bsz in range(1, 20):\\n-            batch = torch.randn(2*bsz, 32).cuda()\\n-            l = loss(batch)\\n+            batch_1 = torch.randn((bsz, 32)).cuda()\\n+            batch_2 = torch.randn((bsz, 32)).cuda()\\n+            l = loss(batch_1, batch_2)\\n \\n     def test_forward_pass_cuda(self):\\n         if torch.cuda.is_available():\\n@@ -81,8 +83,8 @@ class TestNTXentLoss(unittest.TestCase):\\n                 batch_2 = torch.randn((bsz, 32)).cuda()\\n \\n                 # symmetry\\n-                l1 = loss(torch.cat((batch_1, batch_2), 0))\\n-                l2 = loss(torch.cat((batch_2, batch_1), 0))\\n+                l1 = loss(batch_1, batch_2)\\n+                l2 = loss(batch_2, batch_1)\\n                 self.assertAlmostEqual((l1 - l2).pow(2).item(), 0.)\\n         else:\\n             pass\\n\",\"diff --git a/tests/models/test_ModelsMoCo.py b/tests/models/test_ModelsMoCo.py\\nindex 208703c..9eb721d 100644\\n--- a/tests/models/test_ModelsMoCo.py\\n+++ b/tests/models/test_ModelsMoCo.py\\n@@ -87,6 +87,31 @@ class TestModelsMoCo(unittest.TestCase):\\n                 self.assertIsNotNone(model)\\n                 self.assertIsNotNone(out)\\n \\n+    def test_tuple_input(self):\\n+        device = 'cuda' if torch.cuda.is_available() else 'cpu'\\n+        resnet = ResNetGenerator('resnet-18')\\n+        model = MoCo(get_backbone(resnet, num_ftrs=32), out_dim=128).to(device)\\n+\\n+        x0 = torch.rand((self.batch_size, 3, 64, 64)).to(device)\\n+        x1 = torch.rand((self.batch_size, 3, 64, 64)).to(device)\\n+\\n+        out = model(x0)\\n+        self.assertEqual(out.shape, (self.batch_size, 128))\\n+\\n+        out, features = model(x0, return_features=True)\\n+        self.assertEqual(out.shape, (self.batch_size, 128))\\n+        self.assertEqual(features.shape, (self.batch_size, 32))\\n+\\n+        out0, out1 = model(x0, x1)\\n+        self.assertEqual(out0.shape, (self.batch_size, 128))\\n+        self.assertEqual(out1.shape, (self.batch_size, 128))\\n+\\n+        (out0, f0), (out1, f1) = model(x0, x1, return_features=True)\\n+        self.assertEqual(out0.shape, (self.batch_size, 128))\\n+        self.assertEqual(out1.shape, (self.batch_size, 128))\\n+        self.assertEqual(f0.shape, (self.batch_size, 32))\\n+        self.assertEqual(f1.shape, (self.batch_size, 32))\\n+\\n \\n if __name__ == '__main__':\\n     unittest.main()\\n\",\"diff --git a/tests/models/test_ModelsSimCLR.py b/tests/models/test_ModelsSimCLR.py\\nindex 682d148..749e259 100644\\n--- a/tests/models/test_ModelsSimCLR.py\\n+++ b/tests/models/test_ModelsSimCLR.py\\n@@ -87,6 +87,31 @@ class TestModelsSimCLR(unittest.TestCase):\\n                 self.assertIsNotNone(model)\\n                 self.assertIsNotNone(out)\\n \\n+    def test_tuple_input(self):\\n+        device = 'cuda' if torch.cuda.is_available() else 'cpu'\\n+        resnet = ResNetGenerator('resnet-18')\\n+        model = SimCLR(get_backbone(resnet, num_ftrs=32), out_dim=128).to(device)\\n+\\n+        x0 = torch.rand((self.batch_size, 3, 64, 64)).to(device)\\n+        x1 = torch.rand((self.batch_size, 3, 64, 64)).to(device)\\n+\\n+        out = model(x0)\\n+        self.assertEqual(out.shape, (self.batch_size, 128))\\n+\\n+        out, features = model(x0, return_features=True)\\n+        self.assertEqual(out.shape, (self.batch_size, 128))\\n+        self.assertEqual(features.shape, (self.batch_size, 32))\\n+\\n+        out0, out1 = model(x0, x1)\\n+        self.assertEqual(out0.shape, (self.batch_size, 128))\\n+        self.assertEqual(out1.shape, (self.batch_size, 128))\\n+\\n+        (out0, f0), (out1, f1) = model(x0, x1, return_features=True)\\n+        self.assertEqual(out0.shape, (self.batch_size, 128))\\n+        self.assertEqual(out1.shape, (self.batch_size, 128))\\n+        self.assertEqual(f0.shape, (self.batch_size, 32))\\n+        self.assertEqual(f1.shape, (self.batch_size, 32))\\n+\\n \\n if __name__ == '__main__':\\n     unittest.main()\"]", "hints_text": ""}
