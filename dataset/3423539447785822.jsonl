{"instance_id": "3423539447785822", "repo": "open-eo/openeo-python-client", "base_commit": "d167ab17d892a609e41adf14be102bd7aa1d4237", "problem_statement": "minor: Connection.create_job vs DataCube.send_job:\\n`Connection.create_job` vs `DataCube.send_job`\\r\\nhaving differently named methods for same thing in different context is bit confusing when depending on code completion feature.\\r\\n\\r\\nLet's settle on `create_job` (because that is also the verb used in openEO api)?", "FAIL_TO_PASS": ["tests/rest/datacube/test_datacube100.py::TestBatchJob::test_create_job_basic", "tests/rest/datacube/test_mlmodel.py::test_fit_class_random_forest_basic_create_job", "tests/rest/datacube/test_datacube100.py::TestBatchJob::test_legacy_send_job"], "PASS_TO_PASS": [], "language": "python", "test_command": "source /saved/ENV || source /saved/*/ENV && pytest --no-header -rA --tb=no -p no:cacheprovider --continue-on-collection-errors", "test_output_parser": "python/parse_log_pytest_v3", "image_storage_uri": "vmvm-registry.fbinfra.net/repomate_image_activ_pytest/open-eo_openeo-python-client:d167ab17d892a609e41adf14be102bd7aa1d4237", "patch": "[\"diff --git a/CHANGELOG.md b/CHANGELOG.md\\nindex 7e1b5f65b..b9159f684 100644\\n--- a/CHANGELOG.md\\n+++ b/CHANGELOG.md\\n@@ -21,6 +21,7 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\\n - Drop `ImageCollection` from `DataCube`'s class hierarchy. \\n - This practically removes very old (pre-0.4.0) methods like `date_range_filter` and `bbox_filter` from `DataCube`.\\n   ([#100](https://github.com/Open-EO/openeo-python-client/issues/100), [#278](https://github.com/Open-EO/openeo-python-client/issues/278))\\n+- Deprecate `DataCube.send_job` in favor of `DataCube.create_job` for better consistency (internally and with other libraries) ([#276](https://github.com/Open-EO/openeo-python-client/issues/276))\\n \\n ### Removed\\n \\n\",\"diff --git a/examples/R_example.py b/examples/R_example.py\\nindex 0ee915bb4..c56909c73 100644\\n--- a/examples/R_example.py\\n+++ b/examples/R_example.py\\n@@ -50,7 +50,7 @@\\n \\n # Test Job\\n \\n-job = pg.send_job()\\n+job = pg.create_job()\\n print(job.job_id)\\n print(job.start_job())\\n print (job.describe_job())\\n\",\"diff --git a/examples/eodc_example.py b/examples/eodc_example.py\\nindex 863b43117..140eb4f10 100644\\n--- a/examples/eodc_example.py\\n+++ b/examples/eodc_example.py\\n@@ -35,7 +35,7 @@\\n composite = timeseries.min_time()\\n logging.debug(timeseries.to_json(indent=None))\\n \\n-job = timeseries.send_job()\\n+job = timeseries.create_job()\\n logging.debug(\\\"{}\\\".format(job.job_id))\\n \\n status = job.queue()\\n\",\"diff --git a/examples/eurac_example.py b/examples/eurac_example.py\\nindex 26f8bd108..aef935dd6 100644\\n--- a/examples/eurac_example.py\\n+++ b/examples/eurac_example.py\\n@@ -42,7 +42,7 @@\\n print(datacube.to_json())\\n \\n # Submit your process graph as new batch job to back-end\\n-job = datacube.send_job()\\n+job = datacube.create_job()\\n \\n # Launch processing of submitted batch job\\n if job.job_id:\\n\",\"diff --git a/examples/gee_example.py b/examples/gee_example.py\\nindex dfc96363d..ede297ad2 100644\\n--- a/examples/gee_example.py\\n+++ b/examples/gee_example.py\\n@@ -50,7 +50,7 @@\\n \\n \\n # Send Job to backend\\n-job = datacube.send_job()\\n+job = datacube.create_job()\\n print(job.describe_job())\\n \\n # Wait for job to finish and download\\n\",\"diff --git a/examples/gee_uc1_pol.py b/examples/gee_uc1_pol.py\\nindex b8d5d6bf7..0b009206a 100644\\n--- a/examples/gee_uc1_pol.py\\n+++ b/examples/gee_uc1_pol.py\\n@@ -52,7 +52,7 @@\\n print(datacube.to_json())\\n \\n # Send Job to backend\\n-job = datacube.send_job()\\n+job = datacube.create_job()\\n \\n res = job.start_and_wait().download_results()\\n for key, val in res.items():\\n\",\"diff --git a/examples/gee_uc1_temp.py b/examples/gee_uc1_temp.py\\nindex f783ec026..7074cbcc7 100644\\n--- a/examples/gee_uc1_temp.py\\n+++ b/examples/gee_uc1_temp.py\\n@@ -37,7 +37,7 @@\\n print(datacube.to_json())\\n \\n # Send Job to backend\\n-job = datacube.send_job()\\n+job = datacube.create_job()\\n job.start_and_wait().download_results()\\n #print(job.job_id)\\n #print(job.start_job())\\n\",\"diff --git a/examples/notebooks/EODC_Forum_2019/EODC.ipynb b/examples/notebooks/EODC_Forum_2019/EODC.ipynb\\nindex b6b0d4d95..cd3ac6de2 100755\\n--- a/examples/notebooks/EODC_Forum_2019/EODC.ipynb\\n+++ b/examples/notebooks/EODC_Forum_2019/EODC.ipynb\\n@@ -262,7 +262,7 @@\\n    ],\\n    \\\"source\\\": [\\n     \\\"# Sending the job to the backend\\\\n\\\",\\n-    \\\"job = datacube.send_job()\\\\n\\\",\\n+    \\\"job = datacube.create_job()\\\\n\\\",\\n     \\\"job.start_job()\\\\n\\\",\\n     \\\"job\\\"\\n    ]\\n\",\"diff --git a/examples/notebooks/EODC_Forum_2019/EURAC.ipynb b/examples/notebooks/EODC_Forum_2019/EURAC.ipynb\\nindex 0973f008d..dea171358 100644\\n--- a/examples/notebooks/EODC_Forum_2019/EURAC.ipynb\\n+++ b/examples/notebooks/EODC_Forum_2019/EURAC.ipynb\\n@@ -451,7 +451,7 @@\\n    ],\\n    \\\"source\\\": [\\n     \\\"# Sending the job to the backend\\\\n\\\",\\n-    \\\"job = datacube.send_job()\\\\n\\\",\\n+    \\\"job = datacube.create_job()\\\\n\\\",\\n     \\\"job.start_job()\\\\n\\\",\\n     \\\"job\\\"\\n    ]\\n\",\"diff --git a/examples/notebooks/EODC_Forum_2019/GEE.ipynb b/examples/notebooks/EODC_Forum_2019/GEE.ipynb\\nindex b92939ee5..4769e9185 100644\\n--- a/examples/notebooks/EODC_Forum_2019/GEE.ipynb\\n+++ b/examples/notebooks/EODC_Forum_2019/GEE.ipynb\\n@@ -734,7 +734,7 @@\\n    ],\\n    \\\"source\\\": [\\n     \\\"# Sending the job to the backend\\\\n\\\",\\n-    \\\"job = datacube.send_job()\\\\n\\\",\\n+    \\\"job = datacube.create_job()\\\\n\\\",\\n     \\\"job.start_job()\\\\n\\\",\\n     \\\"job\\\"\\n    ]\\n\",\"diff --git a/examples/notebooks/EODC_Forum_2019/RBackend.ipynb b/examples/notebooks/EODC_Forum_2019/RBackend.ipynb\\nindex a02eb245b..2b8db1fd4 100644\\n--- a/examples/notebooks/EODC_Forum_2019/RBackend.ipynb\\n+++ b/examples/notebooks/EODC_Forum_2019/RBackend.ipynb\\n@@ -471,7 +471,7 @@\\n    ],\\n    \\\"source\\\": [\\n     \\\"# Sending the job to the backend\\\\n\\\",\\n-    \\\"job = datacube.send_job()\\\\n\\\",\\n+    \\\"job = datacube.create_job()\\\\n\\\",\\n     \\\"job.start_job()\\\\n\\\",\\n     \\\"job\\\"\\n    ]\\n\",\"diff --git a/examples/notebooks/EODC_Forum_2019/VITO.ipynb b/examples/notebooks/EODC_Forum_2019/VITO.ipynb\\nindex 15b78c269..c3b7f3834 100644\\n--- a/examples/notebooks/EODC_Forum_2019/VITO.ipynb\\n+++ b/examples/notebooks/EODC_Forum_2019/VITO.ipynb\\n@@ -763,7 +763,7 @@\\n    ],\\n    \\\"source\\\": [\\n     \\\"# Sending the job to the backend\\\\n\\\",\\n-    \\\"job = datacube.send_job()\\\\n\\\",\\n+    \\\"job = datacube.create_job()\\\\n\\\",\\n     \\\"job.start_job()\\\\n\\\",\\n     \\\"job\\\"\\n    ]\\n\",\"diff --git a/examples/notebooks/PoC_EODC.ipynb b/examples/notebooks/PoC_EODC.ipynb\\nindex bfa38822a..d7182711e 100755\\n--- a/examples/notebooks/PoC_EODC.ipynb\\n+++ b/examples/notebooks/PoC_EODC.ipynb\\n@@ -288,7 +288,7 @@\\n    ],\\n    \\\"source\\\": [\\n     \\\"# Sending the job to the backend\\\\n\\\",\\n-    \\\"job = timeseries.send_job()\\\\n\\\",\\n+    \\\"job = timeseries.create_job()\\\\n\\\",\\n     \\\"\\\\n\\\",\\n     \\\"job\\\"\\n    ]\\n\",\"diff --git a/examples/notebooks/Sentinel2_NDVI_GEE.ipynb b/examples/notebooks/Sentinel2_NDVI_GEE.ipynb\\nindex 7c759d12a..d918264eb 100644\\n--- a/examples/notebooks/Sentinel2_NDVI_GEE.ipynb\\n+++ b/examples/notebooks/Sentinel2_NDVI_GEE.ipynb\\n@@ -2117,7 +2117,7 @@\\n      \\\"traceback\\\": [\\n       \\\"\\\\u001b[0;31m---------------------------------------------------------------------------\\\\u001b[0m\\\",\\n       \\\"\\\\u001b[0;31mJobFailedException\\\\u001b[0m                        Traceback (most recent call last)\\\",\\n-      \\\"\\\\u001b[0;32m<ipython-input-10-b5e05ed255c3>\\\\u001b[0m in \\\\u001b[0;36m<module>\\\\u001b[0;34m()\\\\u001b[0m\\\\n\\\\u001b[1;32m      1\\\\u001b[0m \\\\u001b[0;31m# Sending the job to the backend\\\\u001b[0m\\\\u001b[0;34m\\\\u001b[0m\\\\u001b[0;34m\\\\u001b[0m\\\\u001b[0m\\\\n\\\\u001b[1;32m      2\\\\u001b[0m \\\\u001b[0mjob\\\\u001b[0m \\\\u001b[0;34m=\\\\u001b[0m \\\\u001b[0mdatacube\\\\u001b[0m\\\\u001b[0;34m.\\\\u001b[0m\\\\u001b[0msend_job\\\\u001b[0m\\\\u001b[0;34m(\\\\u001b[0m\\\\u001b[0;34m)\\\\u001b[0m\\\\u001b[0;34m\\\\u001b[0m\\\\u001b[0m\\\\n\\\\u001b[0;32m----> 3\\\\u001b[0;31m \\\\u001b[0mjob\\\\u001b[0m\\\\u001b[0;34m.\\\\u001b[0m\\\\u001b[0mstart_and_wait\\\\u001b[0m\\\\u001b[0;34m(\\\\u001b[0m\\\\u001b[0;34m)\\\\u001b[0m\\\\u001b[0;34m.\\\\u001b[0m\\\\u001b[0mdownload_results\\\\u001b[0m\\\\u001b[0;34m(\\\\u001b[0m\\\\u001b[0mOUTPUT_FILE\\\\u001b[0m\\\\u001b[0;34m)\\\\u001b[0m\\\\u001b[0;34m\\\\u001b[0m\\\\u001b[0m\\\\n\\\\u001b[0m\\\\u001b[1;32m      4\\\\u001b[0m \\\\u001b[0mjob\\\\u001b[0m\\\\u001b[0;34m\\\\u001b[0m\\\\u001b[0m\\\\n\\\",\\n+      \\\"\\\\u001b[0;32m<ipython-input-10-b5e05ed255c3>\\\\u001b[0m in \\\\u001b[0;36m<module>\\\\u001b[0;34m()\\\\u001b[0m\\\\n\\\\u001b[1;32m      1\\\\u001b[0m \\\\u001b[0;31m# Sending the job to the backend\\\\u001b[0m\\\\u001b[0;34m\\\\u001b[0m\\\\u001b[0;34m\\\\u001b[0m\\\\u001b[0m\\\\n\\\\u001b[1;32m      2\\\\u001b[0m \\\\u001b[0mjob\\\\u001b[0m \\\\u001b[0;34m=\\\\u001b[0m \\\\u001b[0mdatacube\\\\u001b[0m\\\\u001b[0;34m.\\\\u001b[0m\\\\u001b[0mcreate_job\\\\u001b[0m\\\\u001b[0;34m(\\\\u001b[0m\\\\u001b[0;34m)\\\\u001b[0m\\\\u001b[0;34m\\\\u001b[0m\\\\u001b[0m\\\\n\\\\u001b[0;32m----> 3\\\\u001b[0;31m \\\\u001b[0mjob\\\\u001b[0m\\\\u001b[0;34m.\\\\u001b[0m\\\\u001b[0mstart_and_wait\\\\u001b[0m\\\\u001b[0;34m(\\\\u001b[0m\\\\u001b[0;34m)\\\\u001b[0m\\\\u001b[0;34m.\\\\u001b[0m\\\\u001b[0mdownload_results\\\\u001b[0m\\\\u001b[0;34m(\\\\u001b[0m\\\\u001b[0mOUTPUT_FILE\\\\u001b[0m\\\\u001b[0;34m)\\\\u001b[0m\\\\u001b[0;34m\\\\u001b[0m\\\\u001b[0m\\\\n\\\\u001b[0m\\\\u001b[1;32m      4\\\\u001b[0m \\\\u001b[0mjob\\\\u001b[0m\\\\u001b[0;34m\\\\u001b[0m\\\\u001b[0m\\\\n\\\",\\n       \\\"\\\\u001b[0;32m/data/REPO/openeo-python-client/openeo/rest/job.py\\\\u001b[0m in \\\\u001b[0;36mstart_and_wait\\\\u001b[0;34m(self, print, max_poll_interval, connection_retry_interval)\\\\u001b[0m\\\\n\\\\u001b[1;32m    222\\\\u001b[0m             raise JobFailedException(\\\\\\\"Batch job {i} didn't finish properly. Status: {s} (after {t}).\\\\\\\".format(\\\\n\\\\u001b[1;32m    223\\\\u001b[0m                 \\\\u001b[0mi\\\\u001b[0m\\\\u001b[0;34m=\\\\u001b[0m\\\\u001b[0mself\\\\u001b[0m\\\\u001b[0;34m.\\\\u001b[0m\\\\u001b[0mjob_id\\\\u001b[0m\\\\u001b[0;34m,\\\\u001b[0m \\\\u001b[0ms\\\\u001b[0m\\\\u001b[0;34m=\\\\u001b[0m\\\\u001b[0mstatus\\\\u001b[0m\\\\u001b[0;34m,\\\\u001b[0m \\\\u001b[0mt\\\\u001b[0m\\\\u001b[0;34m=\\\\u001b[0m\\\\u001b[0melapsed\\\\u001b[0m\\\\u001b[0;34m\\\\u001b[0m\\\\u001b[0m\\\\n\\\\u001b[0;32m--> 224\\\\u001b[0;31m             ), job=self)\\\\n\\\\u001b[0m\\\\u001b[1;32m    225\\\\u001b[0m \\\\u001b[0;34m\\\\u001b[0m\\\\u001b[0m\\\\n\\\\u001b[1;32m    226\\\\u001b[0m         \\\\u001b[0;32mreturn\\\\u001b[0m \\\\u001b[0mself\\\\u001b[0m\\\\u001b[0;34m\\\\u001b[0m\\\\u001b[0m\\\\n\\\",\\n       \\\"\\\\u001b[0;31mJobFailedException\\\\u001b[0m: Batch job XfUBRlYFgKe3SBvA didn't finish properly. Status: error (after 0:00:11.604513).\\\"\\n      ]\\n@@ -2125,7 +2125,7 @@\\n    ],\\n    \\\"source\\\": [\\n     \\\"# Sending the job to the backend\\\\n\\\",\\n-    \\\"job = datacube.send_job()\\\\n\\\",\\n+    \\\"job = datacube.create_job()\\\\n\\\",\\n     \\\"job.start_and_wait().download_results(OUTPUT_FILE)\\\\n\\\",\\n     \\\"job\\\"\\n    ]\\n\",\"diff --git a/examples/notebooks/SimpleCompositor.ipynb b/examples/notebooks/SimpleCompositor.ipynb\\nindex 2b9802fc9..2fa13b19e 100644\\n--- a/examples/notebooks/SimpleCompositor.ipynb\\n+++ b/examples/notebooks/SimpleCompositor.ipynb\\n@@ -1455,7 +1455,7 @@\\n     }\\n    ],\\n    \\\"source\\\": [\\n-    \\\"composite_job = composite.save_result(format='gtiff').send_job()\\\\n\\\",\\n+    \\\"composite_job = composite.save_result(format='gtiff').create_job()\\\\n\\\",\\n     \\\"composite_job.start_and_wait().get_results().download_file(\\\\\\\"./composite.tiff\\\\\\\")\\\"\\n    ]\\n   },\\n\",\"diff --git a/examples/notebooks/UC1_GEE_Pol.ipynb b/examples/notebooks/UC1_GEE_Pol.ipynb\\nindex f8f2fb462..3ba1fe05e 100644\\n--- a/examples/notebooks/UC1_GEE_Pol.ipynb\\n+++ b/examples/notebooks/UC1_GEE_Pol.ipynb\\n@@ -232,7 +232,7 @@\\n    ],\\n    \\\"source\\\": [\\n     \\\"# Sending the job to the backend\\\\n\\\",\\n-    \\\"job = datacube.send_job()\\\\n\\\",\\n+    \\\"job = datacube.create_job()\\\\n\\\",\\n     \\\"results = job.start_and_wait().download_results()\\\\n\\\",\\n     \\\"results\\\"\\n    ]\\n\",\"diff --git a/examples/notebooks/UC1_GEE_Temp.ipynb b/examples/notebooks/UC1_GEE_Temp.ipynb\\nindex fc8820525..fea1ff3d1 100644\\n--- a/examples/notebooks/UC1_GEE_Temp.ipynb\\n+++ b/examples/notebooks/UC1_GEE_Temp.ipynb\\n@@ -223,7 +223,7 @@\\n    ],\\n    \\\"source\\\": [\\n     \\\"# Sending the job to the backend\\\\n\\\",\\n-    \\\"job = datacube.send_job()\\\\n\\\",\\n+    \\\"job = datacube.create_job()\\\\n\\\",\\n     \\\"results = job.start_and_wait().download_results()\\\\n\\\",\\n     \\\"results\\\"\\n    ]\\n\",\"diff --git a/examples/notebooks/openEO_Python_Sentinel2_EURAC.ipynb b/examples/notebooks/openEO_Python_Sentinel2_EURAC.ipynb\\nindex c1b321542..6eed2a039 100644\\n--- a/examples/notebooks/openEO_Python_Sentinel2_EURAC.ipynb\\n+++ b/examples/notebooks/openEO_Python_Sentinel2_EURAC.ipynb\\n@@ -299,7 +299,7 @@\\n    \\\"outputs\\\": [],\\n    \\\"source\\\": [\\n     \\\"# submit your process graph as new batch job to back-end\\\\n\\\",\\n-    \\\"job = datacube.send_job()\\\"\\n+    \\\"job = datacube.create_job()\\\"\\n    ]\\n   },\\n   {\\n\",\"diff --git a/examples/notebooks/openeo-terrascope-webinar.ipynb b/examples/notebooks/openeo-terrascope-webinar.ipynb\\nindex d7f8bf545..2f23abb89 100644\\n--- a/examples/notebooks/openeo-terrascope-webinar.ipynb\\n+++ b/examples/notebooks/openeo-terrascope-webinar.ipynb\\n@@ -1201,7 +1201,7 @@\\n    \\\"metadata\\\": {},\\n    \\\"outputs\\\": [],\\n    \\\"source\\\": [\\n-    \\\"job = masked_timeseries.send_job()\\\"\\n+    \\\"job = masked_timeseries.create_job()\\\"\\n    ]\\n   },\\n   {\\n\",\"diff --git a/openeo/imagecollection.py b/openeo/imagecollection.py\\nindex 8ddccbb50..e23619c4c 100644\\n--- a/openeo/imagecollection.py\\n+++ b/openeo/imagecollection.py\\n@@ -9,8 +9,7 @@\\n \\n from openeo.rest.job import RESTJob\\n from openeo.rest.service import Service\\n-from openeo.util import get_temporal_extent, first_not_none, dict_no_none\\n-\\n+from openeo.util import get_temporal_extent, first_not_none, dict_no_none, legacy_alias\\n \\n if hasattr(typing, 'TYPE_CHECKING') and typing.TYPE_CHECKING:\\n     # Imports for type checking only (circular import issue at runtime). `hasattr` is Python 3.5 workaround #210\\n@@ -539,7 +538,7 @@ def execute_batch(\\n         This method is mostly recommended if the batch job is expected to run in a reasonable amount of time.\\n \\n         For very long running jobs, you probably do not want to keep the client running. In that case, using\\n-        :func:`~openeo.imagecollection.ImageCollection.send_job` might be more appropriate.\\n+        :func:`~openeo.imagecollection.ImageCollection.create_job` might be more appropriate.\\n \\n         :param job_options: A dictionary containing (custom) job options\\n         :param outputfile: The path of a file to which a result can be written\\n@@ -549,7 +548,7 @@ def execute_batch(\\n         \\\"\\\"\\\"\\n         pass\\n \\n-    def send_job(self, out_format:str=None, job_options:Dict=None, **format_options) -> RESTJob:\\n+    def create_job(self, out_format:str=None, job_options:Dict=None, **format_options) -> RESTJob:\\n         \\\"\\\"\\\"\\n         Sends a job to the backend and returns a RESTJob instance. The job will still need to be started and managed explicitly.\\n         The :func:`~openeo.imagecollection.ImageCollection.execute_batch` method allows you to run batch jobs without managing it.\\n@@ -561,6 +560,8 @@ def send_job(self, out_format:str=None, job_options:Dict=None, **format_options)\\n         \\\"\\\"\\\"\\n         pass\\n \\n+    send_job = legacy_alias(create_job, name=\\\"send_job\\\")\\n+\\n     def pipe(self, func: Callable, *args, **kwargs):\\n         \\\"\\\"\\\"\\n         Pipe the image collection through a function and return the result.\\n\",\"diff --git a/openeo/rest/connection.py b/openeo/rest/connection.py\\nindex d8fb5a597..0a5305e49 100644\\n--- a/openeo/rest/connection.py\\n+++ b/openeo/rest/connection.py\\n@@ -1048,7 +1048,6 @@ def create_job(\\n         :return: job_id: String Job id of the new created job\\n         \\\"\\\"\\\"\\n         # TODO move all this (RESTJob factory) logic to RESTJob?\\n-        # TODO: unify Connection.create_job vs DataCube.send_job. #276\\n         req = self._build_request_with_process_graph(\\n             process_graph=process_graph,\\n             **dict_no_none(title=title, description=description, plan=plan, budget=budget)\\n\",\"diff --git a/openeo/rest/datacube.py b/openeo/rest/datacube.py\\nindex c95d2d768..212cde8fd 100644\\n--- a/openeo/rest/datacube.py\\n+++ b/openeo/rest/datacube.py\\n@@ -1543,13 +1543,13 @@ def execute_batch(\\n         :param format_options: String Parameters for the job result format\\n \\n         \\\"\\\"\\\"\\n-        job = self.send_job(out_format, job_options=job_options, **format_options)\\n+        job = self.create_job(out_format, job_options=job_options, **format_options)\\n         return job.run_synchronous(\\n             outputfile=outputfile,\\n             print=print, max_poll_interval=max_poll_interval, connection_retry_interval=connection_retry_interval\\n         )\\n \\n-    def send_job(\\n+    def create_job(\\n             self, out_format=None, title: str = None, description: str = None, plan: str = None, budget=None,\\n             job_options=None, **format_options\\n     ) -> RESTJob:\\n@@ -1563,7 +1563,6 @@ def send_job(\\n         :return: status: Job resulting job.\\n         \\\"\\\"\\\"\\n         # TODO: add option to also automatically start the job?\\n-        # TODO: unify Connection.create_job vs DataCube.send_job. #276\\n         img = self\\n         if out_format:\\n             # add `save_result` node\\n@@ -1573,6 +1572,8 @@ def send_job(\\n             title=title, description=description, plan=plan, budget=budget, additional=job_options\\n         )\\n \\n+    send_job = legacy_alias(create_job, name=\\\"send_job\\\")\\n+\\n     def save_user_defined_process(\\n             self,\\n             user_defined_process_id: str,\\n\",\"diff --git a/openeo/rest/imagecollectionclient.py b/openeo/rest/imagecollectionclient.py\\nindex d5816a2ee..c9f5137f6 100644\\n--- a/openeo/rest/imagecollectionclient.py\\n+++ b/openeo/rest/imagecollectionclient.py\\n@@ -1003,14 +1003,14 @@ def execute_batch(\\n         :param format_options: String Parameters for the job result format\\n \\n         \\\"\\\"\\\"\\n-        job = self.send_job(out_format, job_options=job_options, **format_options)\\n+        job = self.create_job(out_format, job_options=job_options, **format_options)\\n         return job.run_synchronous(\\n             # TODO #135 support multi file result sets too\\n             outputfile=outputfile,\\n             print=print, max_poll_interval=max_poll_interval, connection_retry_interval=connection_retry_interval\\n         )\\n \\n-    def send_job(\\n+    def create_job(\\n             self, out_format=None, title: str = None, description: str = None, plan: str = None, budget=None,\\n             job_options=None, **format_options\\n     ) -> RESTJob:\\n@@ -1033,6 +1033,8 @@ def send_job(\\n             title=title, description=description, plan=plan, budget=budget, additional=job_options\\n         )\\n \\n+    send_job = legacy_alias(create_job, name=\\\"send_job\\\")\\n+\\n     def execute(self) -> Dict:\\n         \\\"\\\"\\\"Executes the process graph of the imagery. \\\"\\\"\\\"\\n         newbuilder = self.builder.shallow_copy()\\n\",\"diff --git a/openeo/rest/job.py b/openeo/rest/job.py\\nindex a6ee9afb2..b7c8a5b86 100644\\n--- a/openeo/rest/job.py\\n+++ b/openeo/rest/job.py\\n@@ -44,6 +44,7 @@ def _repr_html_(self):\\n     def describe_job(self) -> dict:\\n         \\\"\\\"\\\" Get all job information.\\\"\\\"\\\"\\n         # GET /jobs/{job_id}\\n+        # TODO: rename to just `describe`? #280\\n         return self.connection.get(\\\"/jobs/{}\\\".format(self.job_id), expected_status=200).json()\\n \\n     def update_job(self, process_graph=None, output_format=None,\\n@@ -51,11 +52,13 @@ def update_job(self, process_graph=None, output_format=None,\\n                    plan=None, budget=None, additional=None):\\n         \\\"\\\"\\\" Update a job.\\\"\\\"\\\"\\n         # PATCH /jobs/{job_id}\\n+        # TODO: rename to just `update`? #280\\n         raise NotImplementedError\\n \\n     def delete_job(self):\\n         \\\"\\\"\\\" Delete a job.\\\"\\\"\\\"\\n         # DELETE /jobs/{job_id}\\n+        # TODO: rename to just `delete`? #280\\n         self.connection.delete(\\\"/jobs/{}\\\".format(self.job_id), expected_status=204)\\n \\n     def estimate_job(self):\\n@@ -68,11 +71,14 @@ def estimate_job(self):\\n     def start_job(self):\\n         \\\"\\\"\\\" Start / queue a job for processing.\\\"\\\"\\\"\\n         # POST /jobs/{job_id}/results\\n+        # TODO: rename to just `start`? #280\\n+        # TODO: return self, to allow chained calls\\n         self.connection.post(\\\"/jobs/{}/results\\\".format(self.job_id), expected_status=202)\\n \\n     def stop_job(self):\\n         \\\"\\\"\\\" Stop / cancel job processing.\\\"\\\"\\\"\\n         # DELETE /jobs/{job_id}/results\\n+        # TODO: rename to just `stop`? #280\\n         self.connection.delete(\\\"/jobs/{}/results\\\".format(self.job_id), expected_status=204)\\n \\n     @deprecated(\\\"Use :py:meth:`~RESTJOB.get_results` instead.\\\", version=\\\"0.4.10\\\")\\n\",\"diff --git a/openeo/rest/mlmodel.py b/openeo/rest/mlmodel.py\\nindex 3f1f9f306..8d99161a8 100644\\n--- a/openeo/rest/mlmodel.py\\n+++ b/openeo/rest/mlmodel.py\\n@@ -77,17 +77,16 @@ def execute_batch(\\n         :param outputfile: The path of a file to which a result can be written\\n         :param out_format: (optional) Format of the job result.\\n         :param format_options: String Parameters for the job result format\\n-\\n         \\\"\\\"\\\"\\n         # TODO: check/warn about final `save_ml_model` node?\\n-        job = self.send_job(additional=job_options)\\n+        job = self.create_job(additional=job_options)\\n         return job.run_synchronous(\\n             # TODO #135 support multi file result sets too\\n             outputfile=outputfile,\\n             print=print, max_poll_interval=max_poll_interval, connection_retry_interval=connection_retry_interval\\n         )\\n \\n-    def send_job(self, **kwargs) -> RESTJob:\\n+    def create_job(self, **kwargs) -> RESTJob:\\n         \\\"\\\"\\\"\\n         Sends a job to the backend and returns a ClientJob instance.\\n \\n\",\"diff --git a/openeo/rest/vectorcube.py b/openeo/rest/vectorcube.py\\nindex c0229f294..d8e620b5b 100644\\n--- a/openeo/rest/vectorcube.py\\n+++ b/openeo/rest/vectorcube.py\\n@@ -122,14 +122,14 @@ def execute_batch(\\n         :param format_options: String Parameters for the job result format\\n \\n         \\\"\\\"\\\"\\n-        job = self.send_job(out_format, job_options=job_options, **format_options)\\n+        job = self.create_job(out_format, job_options=job_options, **format_options)\\n         return job.run_synchronous(\\n             # TODO #135 support multi file result sets too\\n             outputfile=outputfile,\\n             print=print, max_poll_interval=max_poll_interval, connection_retry_interval=connection_retry_interval\\n         )\\n \\n-    def send_job(self, out_format=None, job_options=None, **format_options) -> RESTJob:\\n+    def create_job(self, out_format=None, job_options=None, **format_options) -> RESTJob:\\n         \\\"\\\"\\\"\\n         Sends a job to the backend and returns a ClientJob instance.\\n \\n@@ -143,3 +143,5 @@ def send_job(self, out_format=None, job_options=None, **format_options) -> RESTJ\\n             # add `save_result` node\\n             shp = shp.save_result(format=out_format, options=format_options)\\n         return self._connection.create_job(process_graph=shp.flat_graph(), additional=job_options)\\n+\\n+    send_job = legacy_alias(create_job, name=\\\"send_job\\\")\\n\"]", "test_patch": "[\"diff --git a/docs/cookbook/sampling.rst b/docs/cookbook/sampling.rst\\nindex 1dc25ef5e..0a799dea6 100644\\n--- a/docs/cookbook/sampling.rst\\n+++ b/docs/cookbook/sampling.rst\\n@@ -31,7 +31,7 @@ Combining all of this, results in the following sample code::\\n                                                temporal_extent=[\\\"2020-05-01\\\",\\\"2020-06-01\\\"]\\n                                                )\\n     s2_bands = s2_bands.filter_spatial(\\\"https://artifactory.vgt.vito.be/testdata-public/parcels/test_10.geojson\\\")\\n-    job = s2_bands.send_job(title=\\\"Sentinel2\\\", description=\\\"Sentinel-2 L2A bands\\\",out_format=\\\"netCDF\\\",sample_by_feature=True)\\n+    job = s2_bands.create_job(title=\\\"Sentinel2\\\", description=\\\"Sentinel-2 L2A bands\\\",out_format=\\\"netCDF\\\",sample_by_feature=True)\\n \\n Sampling only works for batch jobs, because it results in multiple output files, which can not be conveniently transferred\\n in a synchronous call.\\n\",\"diff --git a/examples/mundialis_example.py b/examples/mundialis_example.py\\nindex 38c401321..0561039d4 100644\\n--- a/examples/mundialis_example.py\\n+++ b/examples/mundialis_example.py\\n@@ -44,7 +44,7 @@\\n \\n # datacube.download(\\\"/tmp/testfile.tiff\\\", format=\\\"GeoTIFF\\\")\\n \\n-job = datacube.send_job()\\n+job = datacube.create_job()\\n if job:\\n     print(job.job_id)\\n     print(job.start_job())\\n\",\"diff --git a/examples/vito_example.py b/examples/vito_example.py\\nindex cf564b0c5..c4960bd84 100644\\n--- a/examples/vito_example.py\\n+++ b/examples/vito_example.py\\n@@ -38,7 +38,7 @@\\n \\n datacube.download(\\\"/tmp/testfile.tiff\\\")\\n \\n-job = datacube.send_job()\\n+job = datacube.create_job()\\n if job:\\n     print(job.job_id)\\n     print(job.run_synchronous(\\\"/tmp/testfile\\\"))\\n\",\"diff --git a/tests/data/batch_job.json b/tests/data/batch_job.json\\ndeleted file mode 100644\\nindex 03f274760..000000000\\n--- a/tests/data/batch_job.json\\n+++ /dev/null\\n@@ -1,26 +0,0 @@\\n-{\\n-  \\\"process_graph\\\": {\\n-    \\\"loadcollection1\\\": {\\n-      \\\"process_id\\\": \\\"load_collection\\\",\\n-      \\\"arguments\\\": {\\n-        \\\"id\\\": \\\"SENTINEL2_RADIOMETRY_10M\\\",\\n-        \\\"spatial_extent\\\": null,\\n-        \\\"temporal_extent\\\": null\\n-      },\\n-      \\\"result\\\": false\\n-    },\\n-    \\\"saveresult1\\\": {\\n-      \\\"process_id\\\": \\\"save_result\\\",\\n-      \\\"arguments\\\": {\\n-        \\\"data\\\": {\\n-          \\\"from_node\\\": \\\"loadcollection1\\\"\\n-        },\\n-        \\\"options\\\": {},\\n-        \\\"format\\\": \\\"GTIFF\\\"\\n-      },\\n-      \\\"result\\\": true\\n-    }\\n-  },\\n-  \\\"title\\\": \\\"my job\\\",\\n-  \\\"description\\\": \\\"just testing\\\"\\n-}\\n\\\\ No newline at end of file\\n\",\"diff --git a/tests/rest/datacube/test_datacube100.py b/tests/rest/datacube/test_datacube100.py\\nindex 255f88082..fed44a9ac 100644\\n--- a/tests/rest/datacube/test_datacube100.py\\n+++ b/tests/rest/datacube/test_datacube100.py\\n@@ -7,6 +7,7 @@\\n import re\\n import sys\\n import textwrap\\n+from typing import Optional\\n \\n import pytest\\n import requests\\n@@ -1535,3 +1536,42 @@ def test_merge_if(con100):\\n             \\\"result\\\": True\\n         }\\n     }\\n+\\n+\\n+class TestBatchJob:\\n+    _EXPECTED_SIMPLE_S2_JOB = {\\\"process\\\": {\\\"process_graph\\\": {\\n+        \\\"loadcollection1\\\": {\\n+            \\\"process_id\\\": \\\"load_collection\\\",\\n+            \\\"arguments\\\": {\\\"id\\\": \\\"S2\\\", \\\"spatial_extent\\\": None, \\\"temporal_extent\\\": None}\\n+        },\\n+        \\\"saveresult1\\\": {\\n+            \\\"process_id\\\": \\\"save_result\\\",\\n+            \\\"arguments\\\": {\\\"data\\\": {\\\"from_node\\\": \\\"loadcollection1\\\"}, \\\"format\\\": \\\"GTiff\\\", \\\"options\\\": {}},\\n+            \\\"result\\\": True,\\n+        }\\n+    }}}\\n+\\n+    def _get_handler_post_jobs(self, expected_post_data: Optional[dict] = None, job_id: str = \\\"myj0b1\\\"):\\n+        \\\"\\\"\\\"Create `POST /jobs` handler\\\"\\\"\\\"\\n+        expected_post_data = expected_post_data or self._EXPECTED_SIMPLE_S2_JOB\\n+\\n+        def post_jobs(request, context):\\n+            assert request.json() == expected_post_data\\n+            context.status_code = 201\\n+            context.headers[\\\"OpenEO-Identifier\\\"] = job_id\\n+\\n+        return post_jobs\\n+\\n+    def test_create_job_basic(self, con100, requests_mock):\\n+        requests_mock.post(API_URL + \\\"/jobs\\\", json=self._get_handler_post_jobs())\\n+        cube = con100.load_collection(\\\"S2\\\")\\n+        job = cube.create_job(out_format=\\\"GTiff\\\")\\n+        assert job.job_id == \\\"myj0b1\\\"\\n+\\n+    def test_legacy_send_job(self, con100, requests_mock):\\n+        \\\"\\\"\\\"Legacy `DataCube.send_job` alis for `create_job\\\"\\\"\\\"\\n+        requests_mock.post(API_URL + \\\"/jobs\\\", json=self._get_handler_post_jobs())\\n+        cube = con100.load_collection(\\\"S2\\\")\\n+        with pytest.warns(DeprecationWarning, match=\\\"Call to deprecated method `send_job`, use `create_job` instead.\\\"):\\n+            job = cube.send_job(out_format=\\\"GTiff\\\")\\n+        assert job.job_id == \\\"myj0b1\\\"\\n\",\"diff --git a/tests/rest/datacube/test_mlmodel.py b/tests/rest/datacube/test_mlmodel.py\\nindex c00479f80..bfbd5a4c4 100644\\n--- a/tests/rest/datacube/test_mlmodel.py\\n+++ b/tests/rest/datacube/test_mlmodel.py\\n@@ -73,5 +73,5 @@ def post_jobs(request, context):\\n \\n     requests_mock.post(API_URL + \\\"/jobs\\\", json=post_jobs)\\n \\n-    job = ml_model.send_job(title=\\\"Random forest\\\")\\n+    job = ml_model.create_job(title=\\\"Random forest\\\")\\n     assert job.job_id == \\\"job-rf\\\"\\n\",\"diff --git a/tests/test_batch_jobs.py b/tests/test_batch_jobs.py\\ndeleted file mode 100644\\nindex 8c6f58cc0..000000000\\n--- a/tests/test_batch_jobs.py\\n+++ /dev/null\\n@@ -1,33 +0,0 @@\\n-from unittest import TestCase\\n-\\n-import requests_mock\\n-\\n-import openeo\\n-from openeo.internal.graphbuilder_040 import GraphBuilder\\n-from . import load_json_resource\\n-\\n-\\n-@requests_mock.mock()\\n-class TestBatchJobs(TestCase):\\n-\\n-    def setUp(self) -> None:\\n-        GraphBuilder.id_counter = {}\\n-\\n-    def test_create_job(self, m):\\n-        m.get(\\\"http://localhost:8000/api/\\\", json={\\\"api_version\\\": \\\"0.4.0\\\"})\\n-        m.get(\\\"http://localhost:8000/api/collections/SENTINEL2_RADIOMETRY_10M\\\", json={})\\n-\\n-        def match_body(request):\\n-            assert request.json() == load_json_resource(\\\"data/batch_job.json\\\")\\n-            return True\\n-\\n-        headers = {\\n-            \\\"OpenEO-Identifier\\\": \\\"my-identifier\\\",\\n-            \\\"Location\\\": \\\"http://localhost:8000/api/jobs/my-identifier\\\"\\n-        }\\n-        m.post(\\\"http://localhost:8000/api/jobs\\\", status_code=201, headers=headers, additional_matcher=match_body)\\n-\\n-        session = openeo.connect(\\\"http://localhost:8000/api\\\")\\n-        s2_radio = session.imagecollection(\\\"SENTINEL2_RADIOMETRY_10M\\\")\\n-        job = s2_radio.send_job(out_format=\\\"GTIFF\\\", title=\\\"my job\\\", description=\\\"just testing\\\")\\n-        assert job.job_id == \\\"my-identifier\\\"\"]", "hints_text": ""}
