{"instance_id": "763371353221151", "repo": "anaphory/lexedata", "base_commit": "44adaf44cb476fc4ed02c9a085d34a920429e8c7", "problem_statement": "error in help?:\\nhttps://github.com/Anaphory/lexedata/blob/34657012e60dc9790b2f4aeaefa88e45fcf7328d/src/lexedata/importer/excel_long_format.py#L365\\r\\n\\r\\nI am confused with the --sheet=Name, shouldn't one type --sheet NAME? Also, why type multiple times --sheet, when in other cases it is ok to do --match-form COLUMN COLUMN COLUMN?", "FAIL_TO_PASS": ["test/test_various_parsers.py::test_listorfromfile_list", "test/test_various_parsers.py::test_listorfromfile_file", "test/test_various_parsers.py::test_filter_parser"], "PASS_TO_PASS": ["test/test_dataset_issues.py::test_alignments_must_match_segments", "test/test_cellparser.py::test_fields_of_formtable_no_source[data/cldf/smallmawetiguarani/cldf-metadata.json]", "test/test_homophones_merger.py::test_parse_merge_override", "test/test_cellparser.py::test_cellparser_unexpected_variant", "test/test_additional_import.py::test_concept_not_found[single_import_parameters0]", "test/test_excel_conversion.py::test_toexcel_runs[data/cldf/smallmawetiguarani/cldf-metadata.json-copy_to_temp_bad_bib]", "test/test_excel_import.py::test_properties_comment_regex_error", "test/test_cellparser.py::test_cellparser_separate_3", "test/test_segment.py::test_illegal_symbol", "test/test_additional_import.py::test_no_concept_separator[single_import_parameters0]", "test/test_cellparser.py::test_cellparser_form_4", "test/test_cellparser.py::test_cellparser_separate_2", "test/test_cognate_exporter.py::test_no_comment_column", "test/test_na_forms.py::test_edictor_exporter_no_na_forms", "test/test_cellparser.py::test_fields_of_formtable_no_language_reference[data/cldf/smallmawetiguarani/cldf-metadata.json]", "test/test_homophones_merger.py::test_errors", "test/test_na_forms.py::test_extended_cldf_validate", "test/test_excel_conversion.py::test_toexcel_runs[data/cldf/smallmawetiguarani/cldf-metadata.json-copy_to_temp]", "test/test_edictor.py::test_roundtrip[data/cldf/minimal/cldf-metadata.json-copy_to_temp]", "test/test_na_forms.py::test_phylogenetics_exporter_unknown", "test/test_form_matcher.py::test_form_association_id_after_normalization", "test/test_excel_import.py::test_cognate_parser_language_not_found", "test/test_homophones_merger.py::test_union", "test/test_excel_conversion.py::test_cell_comments_export", "test/test_cognate_exporter.py::test_adding_singleton_cognatesets", "test/test_edictor.py::test_match_cognatesets_1", "test/test_cellparser.py::test_parser_variant_lands_in_comment", "test/test_cellparser.py::test_cellparser_form_3", "test/test_edictor.py::test_roundtrip[data/cldf/minimal/cldf-metadata.json-copy_to_temp_bad_bib]", "test/test_cognate_detector.py::test_filter_function_factory", "test/test_central_concepts_cognatesets.py::test_central_concept_status_column[data/cldf/smallmawetiguarani/cldf-metadata.json]", "test/test_cognate_exporter.py::test_no_cognateset_table", "test/test_excel_conversion.py::test_toexcel_runs[data/cldf/smallmawetiguarani/cldf-metadata.json-copy_to_temp_no_bib]", "test/test_homophones_merger.py::test_merge_group_assertion_error", "test/test_dataset_issues.py::test_alignments_must_match_segments_ignore_gaps", "test/test_concept_guesser.py::test_add_concepts_to_maweti_cognatesets[data/cldf/smallmawetiguarani/cldf-metadata.json]", "test/test_na_forms.py::test_interleaved_import_dash", "test/test_cellparser.py::test_cellparser_form_5", "test/test_cellparser.py::test_fields_of_formtable_no_transcription[data/cldf/smallmawetiguarani/cldf-metadata.json]", "test/test_excel_conversion.py::test_roundtrip_separator_column[data/cldf/smallmawetiguarani/cldf-metadata.json-copy_to_temp]", "test/test_edictor.py::test_write_edictor_singleton_dataset", "test/test_small_example.py::test_add_cog_tables_correct", "test/test_cognate_exporter.py::test_missing_required_column", "test/test_edictor.py::test_roundtrip[data/cldf/smallmawetiguarani/cldf-metadata.json-copy_to_temp_bad_bib]", "test/test_segment.py::test_segment_inventory_report", "test/test_excel_import.py::test_language_comment_regex_error", "test/test_util.py::test_normal_table_names[data/cldf/smallmawetiguarani/cldf-metadata.json]", "test/test_central_concepts_cognatesets.py::test_value_error_no_parameterReference_for_cognateset[data/cldf/smallmawetiguarani/cldf-metadata.json]", "test/test_additional_import.py::test_import_report_existing_form[single_import_parameters0]", "test/test_various_parsers.py::test_loglevel_parser", "test/test_homophones_merger.py::test_preprocessing", "test/test_cellparser.py::test_mawetiparser_no_duplicate_sources", "test/test_additional_import.py::test_missing_columns1[single_import_parameters0]", "test/test_excel_conversion.py::test_excel_messy_row", "test/test_concept_guesser.py::test_value_error_no_concepticon_reference_for_concepts", "test/test_edictor.py::test_roundtrip[data/cldf/smallmawetiguarani/cldf-metadata.json-copy_to_temp]", "test/test_excel_conversion.py::test_fromexcel_correct[excel_wordlist0]", "test/test_excel_conversion.py::test_roundtrip_separator_column[data/cldf/minimal/cldf-metadata.json-copy_to_temp_bad_bib]", "test/test_matrix_exporter.py::test_toexcel_runs[data/cldf/minimal/cldf-metadata.json-copy_to_temp_no_bib]", "test/test_cellparser.py::test_cellparser_separate_warning", "test/test_edictor.py::test_roundtrip[data/cldf/minimal/cldf-metadata.json-copy_to_temp_no_bib]", "test/test_cognate_exporter.py::test_included_segments", "test/test_excel_conversion.py::test_roundtrip[data/cldf/minimal/cldf-metadata.json-copy_to_temp_no_bib]", "test/test_dataset_issues.py::test_missing_forms_not_coded", "test/test_excel_conversion.py::test_cell_comments", "test/test_additional_import.py::test_concept_file_not_found", "test/test_cellparser.py::test_cellparser_form_7", "test/test_cellparser.py::test_cellparser_form_1", "test/test_cellparser.py::test_source_from_source_string2", "test/test_cognate_exporter.py::test_no_cognate_table", "test/test_cellparser.py::test_fields_of_formtable_no_comment[data/cldf/smallmawetiguarani/cldf-metadata.json]", "test/test_excel_conversion.py::test_fromexcel_correct[excel_wordlist1]", "test/test_matrix_exporter.py::test_toexcel_filtered[data/cldf/minimal/cldf-metadata.json-copy_to_temp_bad_bib]", "test/test_edictor.py::test_forms_to_csv", "test/test_excel_import.py::test_no_dialect_excel_parser", "test/test_na_forms.py::test_coverage_reports_na", "test/test_excel_import.py::test_dialect_missing_key_excel_cognate_parser", "test/test_cellparser.py::test_cellparser_separate_5", "test/test_na_forms.py::test_single_excel_import_skips_na", "test/test_excel_conversion.py::test_roundtrip_separator_column[data/cldf/minimal/cldf-metadata.json-copy_to_temp]", "test/test_segment.py::test_segment_report", "test/test_excel_import.py::test_language_regex_error", "test/test_segment.py::test_add_segments_to_dataset", "test/test_clis.py::test_exit", "test/test_additional_import.py::test_superfluous_columns2[single_import_parameters0]", "test/test_cognate_exporter.py::test_adding_singleton_cognatesets_with_status", "test/test_additional_import.py::test_import_report_new_language[single_import_parameters0]", "test/test_excel_conversion.py::test_toexcel_runs[data/cldf/minimal/cldf-metadata.json-copy_to_temp]", "test/test_na_forms.py::test_add_segments_skips_na_forms", "test/test_coverage.py::test_coverage_report[data/cldf/smallmawetiguarani/cldf-metadata.json]", "test/test_excel_import.py::test_no_dialect_excel_cognate_parser", "test/test_dataset_issues.py::test_strict_metathesis_is_wrong", "test/test_matrix_exporter.py::test_toexcel_runs[data/cldf/minimal/cldf-metadata.json-copy_to_temp_bad_bib]", "test/test_concept_guesser.py::test_concepticon_id_of_concepts_correct[data/cldf/smallmawetiguarani/cldf-metadata.json]", "test/test_add_singleton.py::test_singletons", "test/test_matrix_exporter.py::test_toexcel_filtered[data/cldf/minimal/cldf-metadata.json-copy_to_temp_no_bib]", "test/test_small_example.py::test_interleaved", "test/test_matrix_exporter.py::test_toexcel_filtered[data/cldf/smallmawetiguarani/cldf-metadata.json-copy_to_temp]", "test/test_central_concepts_cognatesets.py::test_add_concepts_to_maweti_cognatesets[data/cldf/smallmawetiguarani/cldf-metadata.json]", "test/test_na_forms.py::test_homohpones_skips_na_forms", "test/test_cellparser.py::test_fields_of_formtable_no_value[data/cldf/smallmawetiguarani/cldf-metadata.json]", "test/test_excel_util.py::test_cell_value", "test/test_excel_import.py::test_db_chache", "test/test_na_forms.py::test_interleaved_import_skips_na", "test/test_excel_import.py::test_dialect_missing_key_excel_parser", "test/test_additional_import.py::test_new_concept_association[single_import_parameters0]", "test/test_cellparser.py::test_cellparser_empty1", "test/test_excel_conversion.py::test_roundtrip[data/cldf/smallmawetiguarani/cldf-metadata.json-copy_to_temp]", "test/test_coverage.py::test_uncoded_coverage_report[data/cldf/smallmawetiguarani/cldf-metadata.json]", "test/test_homophones_merger.py::test_merge_report_parser", "test/test_homophones_merger.py::test_constants", "test/test_additional_import.py::test_form_exists[single_import_parameters0]", "test/test_excel_conversion.py::test_roundtrip[data/cldf/minimal/cldf-metadata.json-copy_to_temp]", "test/test_dataset_issues.py::test_alignments_must_match_length", "test/test_excel_conversion.py::test_roundtrip_separator_column[data/cldf/smallmawetiguarani/cldf-metadata.json-copy_to_temp_bad_bib]", "test/test_form_matcher.py::test_form_association_identical", "test/test_cellparser.py::test_cellparser_separate_1", "test/test_additional_import.py::test_add_new_forms_maweti[single_import_parameters0]", "test/test_dataset_issues.py::test_overlong_slice_is_wrong", "test/test_na_forms.py::test_add_singlestons", "test/test_edictor.py::test_match_cognatesets_2", "test/test_coverage.py::test_no_primary_concepts[data/cldf/smallmawetiguarani/cldf-metadata.json]", "test/test_homophones_merger.py::test_concatenations", "test/test_concept_guesser.py::test_concepticon_reference_missing", "test/test_cellparser.py::test_fields_of_formtable_no_form[data/cldf/smallmawetiguarani/cldf-metadata.json]", "test/test_matrix_exporter.py::test_toexcel_filtered[data/cldf/minimal/cldf-metadata.json-copy_to_temp]", "test/test_excel_conversion.py::test_roundtrip[data/cldf/smallmawetiguarani/cldf-metadata.json-copy_to_temp_no_bib]", "test/test_cellparser.py::test_cellparser_form_2", "test/test_excel_util.py::test_normalize_header", "test/test_excel_conversion.py::test_roundtrip[data/cldf/smallmawetiguarani/cldf-metadata.json-copy_to_temp_bad_bib]", "test/test_homophones_merger.py::test_merge_group_not_implemented", "test/test_dataset_issues.py::test_python_slice_is_wrong", "test/test_add_singleton.py::test_no_status_column", "test/test_segment.py::test_prenasal_before_vowel", "test/test_excel_conversion.py::test_roundtrip_separator_column[data/cldf/minimal/cldf-metadata.json-copy_to_temp_no_bib]", "test/test_matrix_exporter.py::test_toexcel_filtered[data/cldf/smallmawetiguarani/cldf-metadata.json-copy_to_temp_bad_bib]", "test/test_dataset_issues.py::test_backward_slice_is_wrong", "test/test_cellparser.py::test_source_from_source_string1", "test/test_cellparser.py::test_cellparser_not_parsable", "test/test_excel_conversion.py::test_roundtrip[data/cldf/minimal/cldf-metadata.json-copy_to_temp_bad_bib]", "test/test_additional_import.py::test_missing_concept[single_import_parameters0]", "test/test_cellparser.py::test_misshaped_source", "test/test_additional_import.py::test_import_error_missing_parameter_column[single_import_parameters0]", "test/test_excel_conversion.py::test_fromexcel_runs[excel_wordlist0]", "test/test_small_example.py::test_create_metadata_valid", "test/test_form_matcher.py::test_all_ipa_symbols", "test/test_additional_import.py::test_import_report_skipped[single_import_parameters0]", "test/test_matrix_exporter.py::test_cell_comments_export", "test/test_homophones_merger.py::test_skip", "test/test_concept_guesser.py::test_add_concepticon_names_missing_column", "test/test_excel_import.py::test_no_wordlist_and_no_cogsets", "test/test_cellparser.py::test_cellparser_missmatching", "test/test_homophones_merger.py::test_order_merge", "test/test_cellparser.py::test_cellparser_form_6", "test/test_segment.py::test_deleting_symbols", "test/test_matrix_exporter.py::test_toexcel_runs[data/cldf/minimal/cldf-metadata.json-copy_to_temp]", "test/test_edictor.py::test_roundtrip[data/cldf/smallmawetiguarani/cldf-metadata.json-copy_to_temp_no_bib]", "test/test_homophones_merger.py::test_merge_2", "test/test_central_concepts_cognatesets.py::test_value_error_no_concepticonReferenc_for_concepts", "test/test_form_matcher.py::test_form_association", "test/test_homophones_merger.py::test_simple", "test/test_cellparser.py::test_mawetiparser_multiple_comments", "test/test_excel_conversion.py::test_toexcel_runs[data/cldf/minimal/cldf-metadata.json-copy_to_temp_bad_bib]", "test/test_additional_import.py::test_concept_separator[single_import_parameters0]", "test/test_cellparser.py::test_source_from_source_string3", "test/test_homophones_merger.py::test_merge_1", "test/test_excel_conversion.py::test_roundtrip_separator_column[data/cldf/smallmawetiguarani/cldf-metadata.json-copy_to_temp_no_bib]", "test/test_cellparser.py::test_mawetiparser_postprocessing", "test/test_matrix_exporter.py::test_toexcel_runs[data/cldf/smallmawetiguarani/cldf-metadata.json-copy_to_temp]", "test/test_cellparser.py::test_cellparser_no_real_variant", "test/test_cellparser.py::test_cellparser_empty2", "test/test_additional_import.py::test_import_report_add_concept[single_import_parameters0]", "test/test_small_example.py::test_create_metadata_correct", "test/test_segment.py::test_unkown_aspiration", "test/test_form_matcher.py::test_source_context", "test/test_excel_conversion.py::test_toexcel_runs[data/cldf/minimal/cldf-metadata.json-copy_to_temp_no_bib]", "test/test_cellparser.py::test_cellparser_default", "test/test_additional_import.py::test_add_concept_to_existing_form[single_import_parameters0]", "test/test_coverage.py::test_coverage_concept_report[data/cldf/smallmawetiguarani/cldf-metadata.json]", "test/test_dataset_issues.py::test_default_metathesis_is_okay", "test/test_edictor.py::test_write_edictor_empty_dataset", "test/test_util_simplify_ids.py::test_integer_ids", "test/test_homophones_merger.py::test_merge_3", "test/test_excel_import.py::test_no_first_row_in_excel", "test/test_additional_import.py::test_missing_columns2[single_import_parameters0]", "test/test_segment.py::test_unknown_sound", "test/test_additional_import.py::test_superfluous_columns1[single_import_parameters0]", "test/test_concept_guesser.py::test_value_error_no_parameter_reference_for_cognateset[data/cldf/smallmawetiguarani/cldf-metadata.json]", "test/test_excel_conversion.py::test_cell_comments_and_comment_column", "test/test_excel_conversion.py::test_fromexcel_runs[excel_wordlist1]", "test/test_excel_import.py::test_properties_regex_error", "test/test_na_forms.py::test_phylogenetics_exporter_dash_is_absence", "test/test_concept_guesser.py::test_no_concepticon_definition_column_added", "test/test_na_forms.py::test_single_excel_import_dash", "test/test_concept_guesser.py::test_concepticon_definitions[data/cldf/smallmawetiguarani/cldf-metadata.json]", "test/test_util_simplify_ids.py::test_update_ids", "test/test_cellparser.py::test_cellparser_separate_4", "test/test_matrix_exporter.py::test_toexcel_filtered[data/cldf/smallmawetiguarani/cldf-metadata.json-copy_to_temp_no_bib]", "test/test_matrix_exporter.py::test_toexcel_runs[data/cldf/smallmawetiguarani/cldf-metadata.json-copy_to_temp_bad_bib]", "test/test_small_example.py::test_add_cog_tables_valid", "test/test_matrix_exporter.py::test_toexcel_runs[data/cldf/smallmawetiguarani/cldf-metadata.json-copy_to_temp_no_bib]"], "language": "python", "test_command": "source /saved/ENV || source /saved/*/ENV && pytest --no-header -rA --tb=no -p no:cacheprovider --continue-on-collection-errors", "test_output_parser": "python/parse_log_pytest_v3", "image_storage_uri": "vmvm-registry.fbinfra.net/repomate_image_activ_pytest/anaphory_lexedata:44adaf44cb476fc4ed02c9a085d34a920429e8c7", "patch": "[\"diff --git a/docs/tour.rst b/docs/tour.rst\\nindex 41126a8..18f3d34 100644\\n--- a/docs/tour.rst\\n+++ b/docs/tour.rst\\n@@ -69,8 +69,9 @@ This is one of several formats supported by lexedata for import. The\\n corresponding importer is called ``excel_interleaved`` and it works like this::\\n \\n     $ python -m lexedata.importer.excel_interleaved --help\\n-    usage: excel_interleaved.py [-h] [--sheet SHEET] [--directory DIRECTORY]\\n-                                [--loglevel LOGLEVEL] [-q] [-v]\\n+    usage: excel_interleaved.py [-h] [--sheets SHEET [SHEET ...]]\\n+                                [--directory DIRECTORY] [--loglevel LOGLEVEL] [-q]\\n+                                [-v]\\n                                 EXCEL\\n \\n     Import data in the \\\"interleaved\\\" format from an Excel spreadsheet. [...]\\n@@ -81,10 +82,11 @@ corresponding importer is called ``excel_interleaved`` and it works like this::\\n \\n     option[...]:\\n       -h, --help            show this help message and exit\\n-      --sheet SHEET         Excel sheet name(s) to import (default: all sheets)\\n+      --sheets SHEET [SHEET ...]\\n+                            Excel sheet name(s) to import (default: all sheets)\\n       --directory DIRECTORY\\n-                            Path to directory where forms.csv is created (default:\\n-                            current working directory)\\n+                            Path to directory where forms.csv is to be created\\n+                            (default: current working directory)\\n \\n     Logging:\\n       --loglevel LOGLEVEL\\n@@ -464,7 +466,7 @@ tell ``clean_forms`` about new separators and re-run::\\n     $ git checkout .\\n     Updated 2 paths from the index\\n     $ sed -i.bak -e '/kikuyu_long_s2/s/(be long/(be long)/' forms.csv\\n-    $ python -m lexedata.edit.clean_forms -k '~' -k '*' -s ',' -s ';' -s '/'\\n+    $ python -m lexedata.edit.clean_forms -k '~' '*' -s ',' ';' '/'\\n     INFO:lexedata:Line 66: Split form 'pɔ́ / ewɔ́' into 2 elements.\\n     [...]\\n     INFO:lexedata:Line 588: Split form 'lə̀-náàŋgá * náàŋgá' into 2 elements.\\n\",\"diff --git a/src/lexedata/cli.py b/src/lexedata/cli.py\\nindex 72ca032..a6709e2 100644\\n--- a/src/lexedata/cli.py\\n+++ b/src/lexedata/cli.py\\n@@ -1,3 +1,4 @@\\n+import csv\\n import sys\\n import logging\\n import argparse\\n@@ -7,6 +8,8 @@ from pathlib import Path\\n \\n import tqdm\\n \\n+from lexedata import types\\n+\\n logger = logging.getLogger(\\\"lexedata\\\")\\n logging.basicConfig(level=logging.INFO)\\n \\n@@ -53,6 +56,87 @@ class ChangeLoglevel(argparse.Action):\\n         setattr(namespace, self.dest, getattr(namespace, self.dest) + self.change)\\n \\n \\n+class ListOrFromFile(argparse.Action):\\n+    def __init__(\\n+        self,\\n+        option_strings,\\n+        dest,\\n+        nargs=\\\"+\\\",\\n+        default=types.WorldSet(),\\n+        help=None,\\n+        autohelp=True,\\n+        metavar=None,\\n+        **kwargs,\\n+    ):\\n+        if nargs != \\\"+\\\":\\n+            if (\\n+                len(option_strings) == 1\\n+                and nargs == \\\"*\\\"\\n+                and not option_strings[0].startswith(\\\"-\\\")\\n+            ):\\n+                # Mandatory argument, can be not given as default.\\n+                pass\\n+            else:\\n+                raise ValueError(\\n+                    \\\"Optional ListOrFromFile makes sense only with variable argument count ('+)\\\"\\n+                )\\n+\\n+        if metavar is None:\\n+            metavar = option_strings[0].upper()\\n+            if option_strings[0].endswith(\\\"s\\\"):\\n+                metavar = metavar[:-1]\\n+            if option_strings[0].startswith(\\\"--\\\"):\\n+                metavar = metavar[2:]\\n+\\n+        if autohelp:\\n+            help = (\\n+                (help or \\\"\\\")\\n+                + f\\\" Instead of a list of individual {metavar}s on the command line, this argument accepts also the path to a single {metavar}S.CSV file (with header row), containing the relevant IDs in the first column.\\\"\\n+            )\\n+            if type(default) == types.WorldSet:\\n+                help += f\\\" (default: All {metavar.lower()}s in the dataset)\\\"\\n+            help = help.strip()\\n+\\n+        super().__init__(\\n+            option_strings,\\n+            dest,\\n+            nargs=nargs,\\n+            default=default,\\n+            help=help,\\n+            metavar=metavar,\\n+            **kwargs,\\n+        )\\n+\\n+    def __call__(self, parser, namespace, values, option_string=None):\\n+        # This coud be improved if we could defer this until the dataset has\\n+        # been loaded; but that requires major changes to the action reading\\n+        # --metadata.\\n+        if len(values) == 0:\\n+            # Keep default value\\n+            return\\n+        if len(values) == 1:\\n+            path = Path(values[0])\\n+            if path.exists():\\n+                values = []\\n+                for c, concept in enumerate(csv.reader(path.open())):\\n+                    first_column = concept[0]\\n+                    if c == 0:\\n+                        logger.info(\\n+                            \\\"Reading concept IDs from column with header %s\\\",\\n+                            first_column,\\n+                        )\\n+                    else:\\n+                        values.append(first_column)\\n+                setattr(namespace, self.dest, values)\\n+                return\\n+            logger.debug(\\n+                \\\"File %s not found, assuming you want a single %s\\\",\\n+                path,\\n+                str(option_string).lstrip(\\\"-\\\").rstrip(\\\"s\\\"),\\n+            )\\n+        setattr(namespace, self.dest, values)\\n+\\n+\\n def add_log_controls(parser: argparse.ArgumentParser):\\n     logcontrol = parser.add_argument_group(\\\"Logging\\\")\\n     logcontrol.add_argument(\\\"--loglevel\\\", type=int, default=logging.INFO)\\n\",\"diff --git a/src/lexedata/edit/add_concepticon.py b/src/lexedata/edit/add_concepticon.py\\nindex 5186702..2e194e4 100644\\n--- a/src/lexedata/edit/add_concepticon.py\\n+++ b/src/lexedata/edit/add_concepticon.py\\n@@ -244,12 +244,12 @@ if __name__ == \\\"__main__\\\":\\n         help=\\\"Add/update a column containing Concepticon's concept set definitions, for quick disambiguation\\\",\\n     )\\n     parser.add_argument(\\n-        \\\"--gloss-language\\\",\\n+        \\\"--gloss-languages\\\",\\n         \\\"-l\\\",\\n-        action=\\\"append\\\",\\n-        default=[],\\n+        nargs=\\\"+\\\",\\n+        default=None,\\n         type=equal_separated,\\n-        help=\\\"Maps from column names to 2-letter language codes. For example, if your ParameterTable has a column 'GLOSS', which contains English glosses, and a colum 'PORTUGUESE' with Portuguese translations, run this script with the arguments `-l GLOSS=en -l PORTUGUESE=pt`. If no language mappings are given, the assumption is that the #id column contains English glosses.\\\",\\n+        help=\\\"Maps from column names to 2-letter language codes. For example, if your ParameterTable has a column 'GLOSS', which contains English glosses, and a colum 'PORTUGUESE' with Portuguese translations, run this script with the arguments `-l GLOSS=en PORTUGUESE=pt`. If no language mappings are given, the assumption is that the #id column contains English glosses.\\\",\\n     )\\n     parser.add_argument(\\n         \\\"--status-update\\\",\\n@@ -264,7 +264,7 @@ if __name__ == \\\"__main__\\\":\\n \\n     create_concepticon_for_concepts(\\n         dataset=pycldf.Wordlist.from_metadata(args.metadata),\\n-        language=args.gloss_language,\\n+        language=args.gloss_languages,\\n         concepticon_glosses=args.add_concept_set_names,\\n         concepticon_definition=args.add_definitions,\\n         overwrite=args.overwrite,\\n\",\"diff --git a/src/lexedata/edit/add_segments.py b/src/lexedata/edit/add_segments.py\\nindex aa52397..e05db1b 100644\\n--- a/src/lexedata/edit/add_segments.py\\n+++ b/src/lexedata/edit/add_segments.py\\n@@ -1,24 +1,12 @@\\n \\\"\\\"\\\"Segment the form.\\n \\n+\\n Take a form, in a phonemic transcription compatible with IPA, and split it into\\n phonemic segments, which are written back to the Segments column of the\\n FormTable. Segmentation essentially uses `CLTS`_, including diphthongs and affricates.\\n \\n \\n-Details on the segmentation procedure\\n--------------------------------------\\n-\\n-First, apply some pre-processing replacements. Forms supplied contain all\\n-sorts of noise and lookalike symbols. This function comes with reasonable\\n-defaults, but if you encounter other problems, or you actually want to be\\n-strict about IPA transcriptions, pass a dictionary of your choice as\\n-`pre_replace`.\\n-\\n-Then, naïvely segment the form using the IPA tokenizer from the `segments`\\n-package. Check each returned segment to see whether it is valid according to\\n-CLTS's BIPA, and if not, try to fix some issues – in particular pre-aspirated\\n-or pre-nasalized consonants showing up as post-aspirated resp. post-nasalized\\n-vowels, which BIPA does not accept.\\n+For details on the segmentation procedure, see the manual.\\n \\n .. _CLTS: https://clts.clld.org/parameters\\n \\\"\\\"\\\"\\n\",\"diff --git a/src/lexedata/edit/add_status_column.py b/src/lexedata/edit/add_status_column.py\\nindex c270e5c..0a6e561 100644\\n--- a/src/lexedata/edit/add_status_column.py\\n+++ b/src/lexedata/edit/add_status_column.py\\n@@ -37,8 +37,8 @@ if __name__ == \\\"__main__\\\":\\n     parser.add_argument(\\n         \\\"--exclude\\\",\\n         type=str,\\n+        nargs=\\\"*\\\",\\n         default=[],\\n-        action=\\\"append\\\",\\n         help=\\\"Table names to exclude (takes precedence over table-names)\\\",\\n         metavar=\\\"TABLE\\\",\\n     )\\n\",\"diff --git a/src/lexedata/edit/clean_forms.py b/src/lexedata/edit/clean_forms.py\\nindex 8ecb74f..47475ff 100644\\n--- a/src/lexedata/edit/clean_forms.py\\n+++ b/src/lexedata/edit/clean_forms.py\\n@@ -199,24 +199,25 @@ if __name__ == \\\"__main__\\\":\\n     parser.add_argument(\\n         \\\"--brackets\\\",\\n         \\\"-b\\\",\\n-        action=\\\"append\\\",\\n+        metavar=\\\"LR\\\",\\n+        nargs=\\\"+\\\",\\n         default=[],\\n-        help=\\\"Remove brackets from forms, generating variants and comments.\\\",\\n+        help=\\\"Remove brackets from forms, generating variants and comments. Every LR must be a two-character string where L is the left bracket and R is the right bracket.\\\",\\n     )\\n \\n     parser.add_argument(\\n         \\\"--separator\\\",\\n         \\\"-s\\\",\\n-        action=\\\"append\\\",\\n-        default=[],\\n-        help=\\\"Take SEPARATOR as a separator between forms. If no SEPARATORs are given, use tilde '~', comma ',' and semicolon ';'.\\\",\\n+        nargs=\\\"*\\\",\\n+        default=None,\\n+        help=\\\"Take SEPARATOR as a separator between forms. If --separator is not given, use tilde '~', comma ',' and semicolon ';'.\\\",\\n     )\\n     parser.add_argument(\\n         \\\"--keep\\\",\\n         \\\"-k\\\",\\n-        action=\\\"append\\\",\\n-        default=[],\\n-        help=\\\"Take KSEPARATOR as a separator between forms, but include it in front of a variant. If no KSEPARATOR is given, keep '~' and none of the other separators.\\\",\\n+        nargs=\\\"*\\\",\\n+        default=None,\\n+        help=\\\"Take KSEPARATOR as a separator between forms, but include it in front of a variant. If --keep is not given, keep '~' and none of the other separators.\\\",\\n         metavar=\\\"KSEPARATOR\\\",\\n     )\\n \\n@@ -227,9 +228,9 @@ if __name__ == \\\"__main__\\\":\\n \\n     if not args.brackets:\\n         args.brackets = [(\\\"(\\\", \\\")\\\")]\\n-    if not args.separator:\\n+    if args.separator is None:\\n         args.separator = [\\\"~\\\", \\\",\\\", \\\";\\\"]\\n-    if not args.keep:\\n+    if args.keep is None:\\n         kseparators = {\\\"~\\\"}.intersection(args.separator)\\n     else:\\n         kseparators = set(args.keep)\\n\",\"diff --git a/src/lexedata/edit/simplify_ids.py b/src/lexedata/edit/simplify_ids.py\\nindex 171cc3d..db1d5e6 100644\\n--- a/src/lexedata/edit/simplify_ids.py\\n+++ b/src/lexedata/edit/simplify_ids.py\\n@@ -1,4 +1,4 @@\\n-\\\"\\\"\\\"Clean up all ID columns in the datasat.\\n+\\\"\\\"\\\"Clean up all ID columns in the dataset.\\n \\n Take every ID column and convert it to either an integer-valued or a restricted-string-valued (only containing a-z, 0-9, or _) column, maintaining uniqueness of IDs, and keeping IDs as they are where they fit the format.\\n \\n@@ -31,10 +31,9 @@ if __name__ == \\\"__main__\\\":\\n         help=\\\"Normalize to uppercase letters, instead of the default lowercase.\\\",\\n     )\\n     parser.add_argument(\\n-        \\\"--table\\\",\\n-        action=\\\"append\\\",\\n-        default=[],\\n-        help=\\\"Only fix the IDs of this table.\\\",\\n+        \\\"--tables\\\",\\n+        nargs=\\\"+\\\",\\n+        help=\\\"Only fix the IDs of these tables.\\\",\\n     )\\n     args = parser.parse_args()\\n     logger = cli.setup_logging(args)\\n@@ -45,9 +44,9 @@ if __name__ == \\\"__main__\\\":\\n \\n     ds = pycldf.Wordlist.from_metadata(args.metadata)\\n \\n-    if args.table:\\n+    if args.tables:\\n         tables = []\\n-        for table in args.table:\\n+        for table in args.tables:\\n             try:\\n                 tables.append(ds[table])\\n             except KeyError:\\n\",\"diff --git a/src/lexedata/exporter/edictor.py b/src/lexedata/exporter/edictor.py\\nindex 3296814..56f2f9f 100644\\n--- a/src/lexedata/exporter/edictor.py\\n+++ b/src/lexedata/exporter/edictor.py\\n@@ -419,44 +419,20 @@ if __name__ == \\\"__main__\\\":\\n     parser = cli.parser(\\n         description=\\\"Export #FormTable to tsv format for import to edictor\\\"\\n     )\\n-    # TODO: set these arguments correctly\\n-    # TODO: allow reading from a file instead of from command line, maybe as only option.\\n     parser.add_argument(\\n         \\\"--languages\\\",\\n-        type=str,\\n-        nargs=\\\"*\\\",\\n-        default=[],\\n-        help=\\\"Language references for form selection\\\",\\n-        metavar=\\\"LANGUAGE\\\",\\n-    )\\n-    parser.add_argument(\\n-        \\\"--languages-file\\\",\\n-        type=Path,\\n-        help=\\\"If both LANGUAGES_FILE and LANGUAGE are given, take the union.\\\",\\n+        action=cli.ListOrFromFile,\\n+        help=\\\"Export only forms from these languages.\\\",\\n     )\\n     parser.add_argument(\\n         \\\"--concepts\\\",\\n-        type=str,\\n-        nargs=\\\"*\\\",\\n-        default=[],\\n-        help=\\\"\\\",\\n-    )\\n-    parser.add_argument(\\n-        \\\"--concepts-file\\\",\\n-        type=Path,\\n-        help=\\\"\\\",\\n+        action=cli.ListOrFromFile,\\n+        help=\\\"Export only forms connected to these concepts.\\\",\\n     )\\n     parser.add_argument(\\n         \\\"--cognatesets\\\",\\n-        type=str,\\n-        nargs=\\\"*\\\",\\n-        default=[],\\n-        help=\\\"\\\",\\n-    )\\n-    parser.add_argument(\\n-        \\\"--cognatesets-file\\\",\\n-        type=Path,\\n-        help=\\\"\\\",\\n+        action=cli.ListOrFromFile,\\n+        help=\\\"Export only these cognate sets.\\\",\\n     )\\n     parser.add_argument(\\n         \\\"--output-file\\\",\\n@@ -469,29 +445,11 @@ if __name__ == \\\"__main__\\\":\\n     logger = cli.setup_logging(args)\\n     dataset = pycldf.Dataset.from_metadata(args.metadata)\\n \\n-    if args.languages_file:\\n-        for r, row in enumerate(csv.reader(args.languages_file.open())):\\n-            if r == 0 and row[0] == dataset[\\\"LanguageTable\\\", \\\"id\\\"].name:\\n-                continue\\n-            args.languages.append(row[0])\\n-\\n-    if args.concepts_file:\\n-        for r, row in enumerate(csv.reader(args.concepts_file.open())):\\n-            if r == 0 and row[0] == dataset[\\\"ParameterTable\\\", \\\"id\\\"].name:\\n-                continue\\n-            args.concepts.append(row[0])\\n-\\n-    if args.cognatesets_file:\\n-        for r, row in enumerate(csv.reader(args.cognatesets_file.open())):\\n-            if r == 0 and row[0] == dataset[\\\"CognatesetTable\\\", \\\"id\\\"].name:\\n-                continue\\n-            args.cognatesets.append(row[0])\\n-\\n     forms, judgements_about_form, cognateset_mapping = forms_to_tsv(\\n         dataset=dataset,\\n-        languages=set(args.languages) or types.WorldSet(),\\n-        concepts=set(args.concepts) or types.WorldSet(),\\n-        cognatesets=set(args.cognatesets) or types.WorldSet(),\\n+        languages=args.languages,\\n+        concepts=args.concepts,\\n+        cognatesets=args.cognatesets,\\n         logger=logger,\\n     )\\n \\n\",\"diff --git a/src/lexedata/exporter/matrix.py b/src/lexedata/exporter/matrix.py\\nindex 20cf0b0..f15f266 100644\\n--- a/src/lexedata/exporter/matrix.py\\n+++ b/src/lexedata/exporter/matrix.py\\n@@ -1,6 +1,5 @@\\n # -*- coding: utf-8 -*-\\n import re\\n-import csv\\n import typing as t\\n from pathlib import Path\\n \\n@@ -80,9 +79,9 @@ if __name__ == \\\"__main__\\\":\\n         help=\\\"File path for the generated cognate excel file.\\\",\\n     )\\n     parser.add_argument(\\n-        \\\"--concept-list\\\",\\n-        type=Path,\\n-        help=\\\"Output only the concepts listed in this file. I assume that CONCEPT_LIST is a CSV file, and the first comma-separated column contains the ID.\\\",\\n+        \\\"--concepts\\\",\\n+        action=cli.ListOrFromFile,\\n+        help=\\\"Concepts to output.\\\",\\n     )\\n     parser.add_argument(\\n         \\\"--sort-languages-by\\\",\\n@@ -99,21 +98,6 @@ if __name__ == \\\"__main__\\\":\\n     args = parser.parse_args()\\n     logger = cli.setup_logging(args)\\n \\n-    concept_list = None\\n-    if args.concept_list:\\n-        if not args.concept_list.exists():\\n-            logger.critical(\\\"Concept list file %s not found.\\\", args.concept_list)\\n-            cli.Exit.FILE_NOT_FOUND()\\n-        concept_list = []\\n-        for c, concept in enumerate(csv.reader(args.concept_list.open())):\\n-            first_column = concept[0]\\n-            if c == 0:\\n-                logger.info(\\n-                    \\\"Reading concept IDs from column with header %s\\\", first_column\\n-                )\\n-            else:\\n-                concept_list.append(first_column)\\n-\\n     E = MatrixExcelWriter(\\n         pycldf.Wordlist.from_metadata(args.metadata),\\n         database_url=args.url_template,\\n@@ -122,7 +106,7 @@ if __name__ == \\\"__main__\\\":\\n     E.create_excel(\\n         args.excel,\\n         language_order=args.sort_languages_by,\\n-        rows=concept_list,\\n+        rows=args.concepts,\\n     )\\n     E.wb.save(\\n         filename=args.excel,\\n\",\"diff --git a/src/lexedata/exporter/phylogenetics.py b/src/lexedata/exporter/phylogenetics.py\\nindex 1695928..1f24333 100644\\n--- a/src/lexedata/exporter/phylogenetics.py\\n+++ b/src/lexedata/exporter/phylogenetics.py\\n@@ -819,25 +819,19 @@ if __name__ == \\\"__main__\\\":\\n             first `data` tag in there.) (default: Write to stdout)\\\"\\\"\\\",\\n     )\\n     parser.add_argument(\\n-        \\\"--language-list\\\",\\n-        default=None,\\n-        type=Path,\\n-        metavar=\\\"LANGUAGE_FILE\\\",\\n-        help=\\\"File to load a list of languages from\\\",\\n+        \\\"--languages\\\",\\n+        action=cli.ListOrFromFile,\\n+        help=\\\"Languages to include in the alignment.\\\",\\n     )\\n     parser.add_argument(\\n-        \\\"--concept-list\\\",\\n-        default=None,\\n-        type=Path,\\n-        metavar=\\\"CONCEPT_FILE\\\",\\n-        help=\\\"File to load a list of concepts from\\\",\\n+        \\\"--concepts\\\",\\n+        action=cli.ListOrFromFile,\\n+        help=\\\"Concepts to be included or treated as primary concepts.\\\",\\n     )\\n     parser.add_argument(\\n-        \\\"--cognateset-list\\\",\\n-        default=None,\\n-        type=Path,\\n-        metavar=\\\"COGNATESETFILE_FILE\\\",\\n-        help=\\\"File to load a list of cognate sets from\\\",\\n+        \\\"--cognatesets\\\",\\n+        action=cli.ListOrFromFile,\\n+        help=\\\"Cognate sets to consider for the alignment.\\\",\\n     )\\n     parser.add_argument(\\n         \\\"--coding\\\",\\n@@ -872,38 +866,11 @@ if __name__ == \\\"__main__\\\":\\n     # Step 1: Load the raw data.\\n     dataset = pycldf.Dataset.from_metadata(args.metadata)\\n \\n-    languages: t.Set[str]\\n-    if args.language_list:\\n-        languages = {\\n-            lg.strip()\\n-            for lg in args.language_list.open(encoding=\\\"utf-8\\\").read().split(\\\"\\\\n\\\")\\n-        }\\n-    else:\\n-        languages = types.WorldSet()\\n-\\n-    concepts: t.Set[str]\\n-    if args.concept_list:\\n-        concepts = {\\n-            c.strip()\\n-            for c in args.concept_list.open(encoding=\\\"utf-8\\\").read().split(\\\"\\\\n\\\")\\n-        }\\n-    else:\\n-        concepts = types.WorldSet()\\n-\\n-    cognatesets: t.Set[str]\\n-    if args.cognateset_list:\\n-        cognatesets = {\\n-            c.strip()\\n-            for c in args.cognateset_list.open(encoding=\\\"utf-8\\\").read().split(\\\"\\\\n\\\")\\n-        }\\n-    else:\\n-        cognatesets = types.WorldSet()\\n-\\n     # Step 1: Load the raw data.\\n     ds: t.Mapping[Language_ID, t.Mapping[Parameter_ID, t.Set[Cognateset_ID]]] = {\\n-        language: {k: v for k, v in sequence.items() if k in concepts}\\n+        language: {k: v for k, v in sequence.items() if k in args.concepts}\\n         for language, sequence in read_cldf_dataset(dataset).items()\\n-        if language in languages\\n+        if language in args.languages\\n     }\\n \\n     logger.info(f\\\"Imported languages {set(ds)}.\\\")\\n@@ -914,7 +881,7 @@ if __name__ == \\\"__main__\\\":\\n     alignment: t.Mapping[Language_ID, str]\\n     if args.coding == \\\"rootpresence\\\":\\n         relevant_concepts = apply_heuristics(\\n-            dataset, args.absence_heuristic, primary_concepts=concepts\\n+            dataset, args.absence_heuristic, primary_concepts=args.concepts\\n         )\\n         binal, cognateset_indices = root_presence_code(\\n             ds, relevant_concepts=relevant_concepts, logger=logger\\n@@ -922,7 +889,7 @@ if __name__ == \\\"__main__\\\":\\n         exclude = {\\n             index\\n             for cognateset, index in cognateset_indices.items()\\n-            if cognateset not in cognatesets\\n+            if cognateset not in args.cognatesets\\n         }\\n         n_characters = len(next(iter(binal.values())))\\n         alignment = {\\n@@ -937,7 +904,7 @@ if __name__ == \\\"__main__\\\":\\n             index\\n             for concept, cognateset_indices in concept_cognateset_indices.items()\\n             for cognateset, index in cognateset_indices.items()\\n-            if cognateset not in cognatesets\\n+            if cognateset not in args.cognatesets\\n         }\\n         alignment = {\\n             key: \\\"\\\".join([v for i, v in enumerate(value) if i not in exclude])\\n\",\"diff --git a/src/lexedata/importer/excel_interleaved.py b/src/lexedata/importer/excel_interleaved.py\\nindex b608650..77d04cc 100644\\n--- a/src/lexedata/importer/excel_interleaved.py\\n+++ b/src/lexedata/importer/excel_interleaved.py\\n@@ -118,8 +118,9 @@ if __name__ == \\\"__main__\\\":\\n         \\\"excel\\\", type=Path, help=\\\"The Excel file to parse\\\", metavar=\\\"EXCEL\\\"\\n     )\\n     parser.add_argument(\\n-        \\\"--sheet\\\",\\n-        action=\\\"append\\\",\\n+        \\\"--sheets\\\",\\n+        metavar=\\\"SHEET\\\",\\n+        nargs=\\\"+\\\",\\n         default=[],\\n         help=\\\"Excel sheet name(s) to import (default: all sheets)\\\",\\n     )\\n@@ -127,7 +128,7 @@ if __name__ == \\\"__main__\\\":\\n         \\\"--directory\\\",\\n         type=Path,\\n         default=Path(os.getcwd()),\\n-        help=\\\"Path to directory where forms.csv is created (default: current working directory)\\\",\\n+        help=\\\"Path to directory where forms.csv is to be created (default: current working directory)\\\",\\n     )\\n     cli.add_log_controls(parser)\\n     args = parser.parse_args()\\n@@ -142,11 +143,11 @@ if __name__ == \\\"__main__\\\":\\n         [\\\"ID\\\", \\\"Language_ID\\\", \\\"Parameter_ID\\\", \\\"Form\\\", \\\"Comment\\\", \\\"Cognateset_ID\\\"]\\n     )\\n \\n-    if not args.sheet:\\n-        args.sheet = [sheet for sheet in ws.sheetnames]\\n+    if not args.sheets:\\n+        args.sheets = [sheet for sheet in ws.sheetnames]\\n \\n     ids: t.Set[str] = set()\\n-    for sheetname in args.sheet:\\n+    for sheetname in args.sheets:\\n         sheet = ws[sheetname]\\n         for row in import_interleaved(sheet, logger=logger, ids=ids):\\n             w.writerow(row)\\n\",\"diff --git a/src/lexedata/importer/excel_long_format.py b/src/lexedata/importer/excel_long_format.py\\nindex 8462e76..2251c98 100644\\n--- a/src/lexedata/importer/excel_long_format.py\\n+++ b/src/lexedata/importer/excel_long_format.py\\n@@ -362,18 +362,17 @@ if __name__ == \\\"__main__\\\":\\n         metavar=\\\"COLUMN\\\",\\n     )\\n     parser.add_argument(\\n-        \\\"--sheet\\\",\\n-        action=\\\"append\\\",\\n+        \\\"--sheets\\\",\\n         type=str,\\n-        default=[],\\n+        nargs=\\\"+\\\",\\n         metavar=\\\"SHEET_NAME\\\",\\n-        help=\\\"Sheet to parse. For multiple sheets, use multiple arguments of the shape --sheet Name1 --sheet Name2 (default: all sheets)\\\",\\n+        help=\\\"Sheets to parse. (default: all sheets)\\\",\\n     )\\n     parser.add_argument(\\n         \\\"--match-form\\\",\\n         \\\"-f\\\",\\n-        action=\\\"append\\\",\\n         type=str,\\n+        nargs=\\\"+\\\",\\n         default=[],\\n         metavar=\\\"COLUMN_NAME\\\",\\n         help=\\\"Forms are considered identical if all columns passed to -f/--match-form are identical\\\",\\n@@ -418,13 +417,13 @@ if __name__ == \\\"__main__\\\":\\n     args = parser.parse_args()\\n     logger = cli.setup_logging(args)\\n \\n-    if not args.sheet:\\n+    if not args.sheets:\\n         sheets = [\\n             sheet for sheet in args.excel if sheet.title not in args.exclude_sheet\\n         ]\\n-        logger.info(\\\"No sheets specified explicitly. Parsing sheets: %s\\\", args.sheet)\\n+        logger.info(\\\"No sheets specified explicitly. Parsing sheets: %s\\\", args.sheets)\\n     else:\\n-        sheets = [args.excel[s] for s in args.sheet]\\n+        sheets = [args.excel[s] for s in args.sheets]\\n \\n     report = add_single_languages(\\n         metadata=args.metadata,\\n\",\"diff --git a/src/lexedata/report/coverage.py b/src/lexedata/report/coverage.py\\nindex 17c4acf..91f507a 100644\\n--- a/src/lexedata/report/coverage.py\\n+++ b/src/lexedata/report/coverage.py\\n@@ -213,13 +213,11 @@ if __name__ == \\\"__main__\\\":\\n         help=\\\"Only list matching languages, don't report statistics\\\",\\n     )\\n     parser.add_argument(\\n-        \\\"--with-concept\\\",\\n+        \\\"--with-concepts\\\",\\n         \\\"-c\\\",\\n-        action=\\\"append\\\",\\n-        default=[],\\n-        type=str,\\n-        help=\\\"Only include languages that have a form for CONCEPT\\\",\\n+        action=cli.ListOrFromFile,\\n         metavar=\\\"CONCEPT\\\",\\n+        help=\\\"Only include languages that have a form for each CONCEPT.\\\",\\n     )\\n     parser.add_argument(\\n         \\\"--concept-report\\\",\\n@@ -255,7 +253,7 @@ if __name__ == \\\"__main__\\\":\\n     data, header = coverage_report(\\n         dataset,\\n         args.min_percentage,\\n-        args.with_concept,\\n+        args.with_concepts,\\n         missing=Missing.__members__[args.missing],\\n         only_coded=args.coded,\\n     )\\n\",\"diff --git a/src/lexedata/report/filter.py b/src/lexedata/report/filter.py\\nindex d3806c8..fdad51e 100644\\n--- a/src/lexedata/report/filter.py\\n+++ b/src/lexedata/report/filter.py\\n@@ -1,5 +1,6 @@\\n \\\"\\\"\\\"Filter some table by some column.\\n \\n+\\n Print the partial table to STDOUT or a file, so it can be used as subset-filter\\n for some other script, and output statistics (how many included, how many\\n excluded, what proportion, maybe sub-statistics for xxxReference columns, i.e.\\n@@ -76,7 +77,9 @@ def filter(\\n \\n \\n def parser():\\n-    parser = cli.parser(description=__doc__)\\n+    parser = cli.parser(\\n+        description=__doc__.split(\\\"\\\\n\\\\n\\\\n\\\")[0], epilog=__doc__.split(\\\"\\\\n\\\\n\\\\n\\\")[1]\\n+    )\\n     parser.add_argument(\\\"column\\\", help=\\\"The column to filter.\\\", metavar=\\\"COLUMN\\\")\\n     parser.add_argument(\\\"filter\\\", help=\\\"An expression to filter by.\\\", metavar=\\\"FILTER\\\")\\n     parser.add_argument(\\n@@ -93,9 +96,9 @@ def parser():\\n         help=\\\"Output exactly the NON-matching lines\\\",\\n     )\\n     parser.add_argument(\\n-        \\\"--output-column\\\",\\n+        \\\"--output-columns\\\",\\n         \\\"-c\\\",\\n-        action=\\\"append\\\",\\n+        nargs=\\\"+\\\",\\n         default=[],\\n         help=\\\"Output only columns OUTPUT_COLUMN1,OUTPUT_COLUMN2,OUTPUT_COLUMN3,… in the same order as given.\\\",\\n     )\\n@@ -109,8 +112,8 @@ if __name__ == \\\"__main__\\\":\\n \\n     if not args.table:\\n         table = DictReader(sys.stdin)\\n-        if not args.output_column:\\n-            args.output_column = table.fieldnames\\n+        if not args.output_columns:\\n+            args.output_columns = table.fieldnames\\n     else:\\n         table = pycldf.Wordlist.from_metadata(args.metadata)[args.table]\\n \\n@@ -119,13 +122,13 @@ if __name__ == \\\"__main__\\\":\\n         for r, row in enumerate(\\n             filter(table, args.column, re.compile(args.filter), args.invert)\\n         ):\\n-            if not args.output_column:\\n-                args.output_column = row.keys()\\n+            if not args.output_columns:\\n+                args.output_columns = row.keys()\\n             if w is None:\\n-                w = DictWriter(sys.stdout, args.output_column)\\n+                w = DictWriter(sys.stdout, args.output_columns)\\n                 w.writeheader()\\n             row = {\\n-                key: value for key, value in row.items() if key in args.output_column\\n+                key: value for key, value in row.items() if key in args.output_columns\\n             }\\n             w.writerow(row)\\n     except KeyError:\\n\",\"diff --git a/src/lexedata/report/nonconcatenative_morphemes.py b/src/lexedata/report/nonconcatenative_morphemes.py\\nindex 8577f14..630417d 100644\\n--- a/src/lexedata/report/nonconcatenative_morphemes.py\\n+++ b/src/lexedata/report/nonconcatenative_morphemes.py\\n@@ -1,4 +1,3 @@\\n-import itertools\\n import typing as t\\n \\n import pycldf\\n@@ -16,7 +15,7 @@ def segment_to_cognateset(\\n         types.Cognate_ID,\\n         types.Cognateset_ID,\\n     ],\\n-    cognatesets: t.Optional[t.Iterable[types.Cognateset_ID]],\\n+    cognatesets: t.Optional[t.Container[types.Cognateset_ID]],\\n     logger: cli.logging.Logger = cli.logger,\\n     forms_by_cogset: t.Mapping[types.Cognateset_ID, t.List[t.Sequence[str]]] = {},\\n ) -> t.Set[t.Tuple[types.Cognateset_ID, types.Cognateset_ID]]:\\n@@ -30,26 +29,25 @@ def segment_to_cognateset(\\n     mergers: t.Set[t.Tuple[types.Cognateset_ID, types.Cognateset_ID]] = set()\\n \\n     forms = util.cache_table(dataset)\\n-    cognateset_cache: t.Mapping[t.Optional[types.Cognateset_ID], int]\\n+    cognateset_cache: t.Container[types.Cognateset_ID]\\n     if \\\"CognatesetTable\\\" in dataset:\\n+        c_s_id = dataset[\\\"CognatesetTable\\\", \\\"id\\\"].name\\n         cognateset_cache = {\\n-            cognateset[\\\"ID\\\"]: c\\n-            for c, cognateset in enumerate(dataset[\\\"CognatesetTable\\\"], 1)\\n+            cognateset[c_s_id]\\n+            for cognateset in dataset[\\\"CognatesetTable\\\"]\\n             if cognatesets is None or cognateset[\\\"ID\\\"] in cognatesets\\n         }\\n     else:\\n         if cognatesets is None:\\n-            cognateset_cache = t.DefaultDict(itertools.count().__next__)\\n+            cognateset_cache = types.WorldSet()\\n         else:\\n-            cognateset_cache = {c: i for i, c in enumerate(cognatesets, 1)}\\n-\\n-    cognateset_cache[None] = 0\\n+            cognateset_cache = cognatesets\\n \\n     which_segment_belongs_to_which_cognateset: t.Dict[\\n         types.Form_ID, t.List[t.Set[types.Cognateset_ID]]\\n     ] = {}\\n     for j in dataset[\\\"CognateTable\\\"]:\\n-        if j[c_cognate_form] in forms and cognateset_cache.get(j[c_cognate_cognateset]):\\n+        if j[c_cognate_form] in forms and j[c_cognate_cognateset] in cognateset_cache:\\n             form = forms[j[c_cognate_form]]\\n             if j[c_cognate_form] not in which_segment_belongs_to_which_cognateset:\\n                 which_segment_belongs_to_which_cognateset[j[c_cognate_form]] = [\\n@@ -113,10 +111,8 @@ if __name__ == \\\"__main__\\\":\\n     )\\n     parser.add_argument(\\n         \\\"--cognatesets\\\",\\n-        type=str,\\n-        nargs=\\\"*\\\",\\n-        default=None,\\n-        help=\\\"\\\",\\n+        action=cli.ListOrFromFile,\\n+        help=\\\"Only use these cognate sets as indication of overlapping morphemes.\\\",\\n     )\\n     args = parser.parse_args()\\n     logger = cli.setup_logging(args)\\n\",\"diff --git a/src/lexedata/report/segment_inventories.py b/src/lexedata/report/segment_inventories.py\\nindex e352b70..7897f92 100644\\n--- a/src/lexedata/report/segment_inventories.py\\n+++ b/src/lexedata/report/segment_inventories.py\\n@@ -66,20 +66,16 @@ if __name__ == \\\"__main__\\\":\\n         description=__doc__.split(\\\"\\\\n\\\\n\\\\n\\\")[0], epilog=__doc__.split(\\\"\\\\n\\\\n\\\\n\\\")[1]\\n     )\\n     parser.add_argument(\\n-        \\\"--language\\\",\\n-        action=\\\"append\\\",\\n-        default=[],\\n-        help=\\\"Restrict the report to these lanugage id(s). (default: All languages.)\\\",\\n+        \\\"--languages\\\",\\n+        action=cli.ListOrFromFile,\\n+        help=\\\"Restrict the report to these lanugage id(s).\\\",\\n     )\\n     args = parser.parse_args()\\n     logger = cli.setup_logging(args)\\n \\n     dataset = pycldf.Wordlist.from_metadata(args.metadata)\\n \\n-    if not args.language:\\n-        args.language = types.WorldSet()\\n-\\n-    counts = count_segments(dataset, args.language)\\n+    counts = count_segments(dataset, args.languages)\\n \\n     if len(counts) == 1:\\n         # A single language\\n\"]", "test_patch": "[\"diff --git a/test/test_various_parsers.py b/test/test_various_parsers.py\\nindex e02de76..cc918ea 100644\\n--- a/test/test_various_parsers.py\\n+++ b/test/test_various_parsers.py\\n@@ -1,14 +1,38 @@\\n import logging\\n+import tempfile\\n+import argparse\\n+from lexedata import cli\\n from lexedata.report.filter import parser as filter_parser\\n \\n \\n+def test_listorfromfile_list():\\n+    parser = argparse.ArgumentParser()\\n+    parser.add_argument(\\\"--objects\\\", action=cli.ListOrFromFile, help=\\\"Some objects.\\\")\\n+    parser.add_argument(\\\"--other\\\", action=\\\"store_true\\\", default=False)\\n+    parameters = parser.parse_args([\\\"--objects\\\", \\\"o1\\\", \\\"o2\\\", \\\"o3\\\", \\\"--other\\\"])\\n+    assert parameters.other\\n+    assert parameters.objects == [\\\"o1\\\", \\\"o2\\\", \\\"o3\\\"]\\n+\\n+\\n+def test_listorfromfile_file():\\n+    parser = argparse.ArgumentParser()\\n+    parser.add_argument(\\\"--objects\\\", action=cli.ListOrFromFile, help=\\\"Some objects.\\\")\\n+    parser.add_argument(\\\"--other\\\", action=\\\"store_true\\\", default=False)\\n+    _, fname = tempfile.mkstemp(\\\".csv\\\")\\n+    with open(fname, \\\"w\\\") as file:\\n+        file.write(\\\"ID,ignored\\\\no1,yes\\\\no2\\\\no3,\\\")\\n+    parameters = parser.parse_args([\\\"--objects\\\", fname, \\\"--other\\\"])\\n+    assert parameters.other\\n+    assert parameters.objects == [\\\"o1\\\", \\\"o2\\\", \\\"o3\\\"]\\n+\\n+\\n def test_filter_parser():\\n     parameters = filter_parser().parse_args([\\\"form\\\", \\\"a\\\", \\\"FormTable\\\", \\\"-V\\\"])\\n     assert parameters.table == \\\"FormTable\\\"\\n     assert parameters.column == \\\"form\\\"\\n     assert parameters.filter == \\\"a\\\"\\n     assert parameters.invert is True\\n-    assert parameters.output_column == []\\n+    assert parameters.output_columns == []\\n \\n \\n def test_loglevel_parser():\"]", "hints_text": ""}
