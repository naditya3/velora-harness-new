{"instance_id": "1292495488938679", "repo": "fitbenchmarking/fitbenchmarking", "base_commit": "2012bea1d077ce6f5e0e370cd0c73ddc5cdb9033", "problem_statement": "Allow access to Hessians:\\nSome minimizers (e.g, RALFit, Scipy) can use second derivative information.  We should make this available for appropriate problems classes (e.g., NIST)\\r\\n", "FAIL_TO_PASS": ["fitbenchmarking/parsing/tests/test_parsers.py::TestParsers::test_parsers[expected3-nist-/app/repo/fitbenchmarking/parsing/tests/nist/basic.dat]", "fitbenchmarking/parsing/tests/test_parsers.py::TestParsers::test_hessian_evaluation[/app/repo/fitbenchmarking/parsing/tests/nist/hessian_evaluations.json-nist]", "fitbenchmarking/core/tests/test_fitting_benchmarking_jacobians.py::LoopOverJacobiansTests::test_single_no_jacobian", "fitbenchmarking/core/tests/test_fitting_benchmarking_jacobians.py::LoopOverJacobiansTests::test_multiple_jacobian", "fitbenchmarking/core/tests/test_fitting_benchmarking_jacobians.py::LoopOverJacobiansTests::test_single_jacobian"], "PASS_TO_PASS": ["fitbenchmarking/utils/tests/test_create_dirs.py::CreateDirsTests::test_results_throw_correct_error", "fitbenchmarking/utils/tests/test_options_minimizers.py::UserMininimizerOptionTests::test_minimizer_bumps_invalid", "fitbenchmarking/cost_func/tests/test_cost_func.py::FactoryTests::test_imports", "fitbenchmarking/utils/tests/test_options_minimizers.py::MininimizerOptionTests::test_minimizer_gsl", "fitbenchmarking/utils/tests/test_options_fitting.py::FittingOptionTests::test_jac_method_default", "fitbenchmarking/cost_func/tests/test_cost_func.py::TestPoissonCostFunc::test_eval_cost_raise_error", "fitbenchmarking/results_processing/tests/test_plots.py::PlotTests::test_plot_initial_guess_create_files", "fitbenchmarking/cli/tests/test_exception_handler.py::TestExceptionHandler::test_fb_exception", "fitbenchmarking/core/tests/test_results_output.py::CreateDirectoriesTests::test_create_dirs_correct", "fitbenchmarking/cost_func/tests/test_cost_func.py::TestNLLSCostFunc::test_validate_algorithm_type_error", "fitbenchmarking/utils/tests/test_options_fitting.py::UserFittingOptionTests::test_minimizer_cost_func_type_invalid", "fitbenchmarking/utils/tests/test_options_plotting.py::UserPlottingOptionTests::test_minimizer_make_plots_valid", "fitbenchmarking/parsing/tests/test_fitting_problem.py::TestFittingProblem::test_set_value_ranges_incorrect_names", "fitbenchmarking/parsing/tests/test_parsers.py::TestParsers::test_factory[nist-/app/repo/fitbenchmarking/parsing/tests/nist/start_end_x.dat]", "fitbenchmarking/cost_func/tests/test_cost_func.py::TestHellingerNLLSCostFunc::test_eval_r_correct_evaluation", "fitbenchmarking/parsing/tests/test_fitting_problem.py::TestFittingProblem::test_verify_problem", "fitbenchmarking/utils/tests/test_create_dirs.py::CreateDirsTests::test_groupResults_create_correct_group_results", "fitbenchmarking/results_processing/tests/test_performance_profiler.py::PerformanceProfillerTests::test_correct_profile", "fitbenchmarking/core/tests/test_fitting_benchmarking_benchmark.py::BenchmarkTests::test_check_unselected_minimizers", "fitbenchmarking/cost_func/tests/test_cost_func.py::TestWeightedNLLSCostFunc::test_eval_r_raise_error", "fitbenchmarking/utils/tests/test_options_plotting.py::PlottingOptionTests::test_colour_ulim_default", "fitbenchmarking/utils/tests/test_options_minimizers.py::UserMininimizerOptionTests::test_minimizer_ralfit_invalid", "fitbenchmarking/utils/tests/test_options_generic.py::OptionsWriteTests::test_options_reset", "fitbenchmarking/utils/tests/test_fitbm_result.py::FitbmResultTests::test_sanitised_min_name", "fitbenchmarking/results_processing/tests/test_support_page.py::GetFigurePathsTests::test_with_links", "fitbenchmarking/utils/tests/test_misc.py::CreateDirsTests::test_getProblemFiles_get_correct_probs", "fitbenchmarking/utils/tests/test_options_jacobian.py::JacobianOptionTesJacobians::test_num_method_default", "fitbenchmarking/jacobian/tests/test_jacobian.py::TestJacobianClass::test_analytic_cutest_weighted", "fitbenchmarking/core/tests/test_results_output.py::PreproccessDataTests::test_preproccess_data", "fitbenchmarking/results_processing/tests/test_support_page.py::CreateProbGroupTests::test_file_name", "fitbenchmarking/cost_func/tests/test_cost_func.py::TestNLLSCostFunc::test_validate_algorithm_type_correct", "fitbenchmarking/utils/tests/test_options_logging.py::UserLoggingOptionTests::test_minimizer_log_append_invalid", "fitbenchmarking/core/tests/test_results_output.py::CreatePlotsTests::test_create_plots_without_params", "fitbenchmarking/utils/tests/test_options_logging.py::LoggingOptionTests::test_append_default", "fitbenchmarking/utils/tests/test_output_grabber.py::OutputGrabberTests::test_correct_stdout", "fitbenchmarking/jacobian/tests/test_jacobian.py::TestJacobianClass::test_scipy_two_point_eval", "fitbenchmarking/jacobian/tests/test_jacobian.py::TestDerivCostFunc::test_numdifftools_eval", "fitbenchmarking/core/tests/test_fitting_benchmarking_software.py::LoopOverSoftwareTests::test_run_software", "fitbenchmarking/utils/tests/test_options_logging.py::LoggingOptionTests::test_file_name_default", "fitbenchmarking/utils/tests/test_options_fitting.py::UserFittingOptionTests::test_minimizer_jac_method_invalid", "fitbenchmarking/parsing/tests/test_fitting_problem.py::TestFittingProblem::test_set_value_ranges", "fitbenchmarking/parsing/tests/test_parsers.py::TestParserFactory::test_parse_problem_file", "fitbenchmarking/utils/tests/test_options_fitting.py::UserFittingOptionTests::test_minimizer_algorithm_type_valid", "fitbenchmarking/parsing/tests/test_parsers.py::TestParsers::test_jacobian_evaluation[/app/repo/fitbenchmarking/parsing/tests/fitbenchmark/jacobian_evaluations.json-fitbenchmark]", "fitbenchmarking/core/tests/test_fitting_benchmarking_benchmark_problems.py::LoopOverBenchmarkProblemsTests::test_run_multiple_benchmark_problems", "fitbenchmarking/utils/tests/test_fitbm_result.py::FitbmResultTests::test_init_with_dataset_id", "fitbenchmarking/jacobian/tests/test_jacobian.py::TestJacobianClass::test_analytic_cutest_poisson", "fitbenchmarking/jacobian/tests/test_jacobian.py::TestFactory::test_imports", "fitbenchmarking/utils/tests/test_options_plotting.py::UserPlottingOptionTests::test_minimizer_colour_map_invalid", "fitbenchmarking/utils/tests/test_options_minimizers.py::MininimizerOptionTests::test_minimizer_scipy_ls", "fitbenchmarking/utils/tests/test_options_minimizers.py::UserMininimizerOptionTests::test_minimizer_minuit_valid", "fitbenchmarking/utils/tests/test_create_dirs.py::CreateDirsTests::test_results_create_correct_dir", "fitbenchmarking/utils/tests/test_logger.py::test_level", "fitbenchmarking/utils/tests/test_options_jacobian.py::UserJacobianOptionTests::test_minimizer_num_method_valid", "fitbenchmarking/utils/tests/test_options_plotting.py::UserPlottingOptionTests::test_minimizer_comparison_mode_invalid", "fitbenchmarking/utils/tests/test_options_minimizers.py::MininimizerOptionTests::test_minimizer_bumps", "fitbenchmarking/parsing/tests/test_fitting_problem.py::TestFittingProblem::test_correct_data_single_fit", "fitbenchmarking/utils/tests/test_logger.py::TestSetupLogger::test_file_creation_correct", "fitbenchmarking/utils/tests/test_options_plotting.py::PlottingOptionTests::test_make_plots_default", "fitbenchmarking/results_processing/tests/test_plots.py::PlotTests::test_plot_fit_create_files", "fitbenchmarking/utils/tests/test_options_jacobian.py::UserJacobianOptionTests::test_invalid_option_key", "fitbenchmarking/utils/tests/test_options_plotting.py::UserPlottingOptionTests::test_minimizer_comparison_mode_valid", "fitbenchmarking/cli/tests/test_main.py::TestMain::test_check_no_results_produced", "fitbenchmarking/utils/tests/test_options_plotting.py::PlottingOptionTests::test_colour_map_default", "fitbenchmarking/cost_func/tests/test_cost_func.py::TestNLLSCostFunc::test_eval_cost", "fitbenchmarking/utils/tests/test_options_minimizers.py::UserMininimizerOptionTests::test_minimizer_scipy_ls_valid", "fitbenchmarking/utils/tests/test_options_fitting.py::FittingOptionTests::test_num_runs_default", "fitbenchmarking/core/tests/test_fitting_benchmarking_benchmark.py::BenchmarkTests::test_check_no_unselected_minimizers", "fitbenchmarking/parsing/tests/test_parsers.py::TestParsers::test_factory[nist-/app/repo/fitbenchmarking/parsing/tests/nist/basic.dat]", "fitbenchmarking/utils/tests/test_options_logging.py::UserLoggingOptionTests::test_minimizer_file_name_valid", "fitbenchmarking/utils/tests/test_misc.py::CreateDirsTests::test_get_css", "fitbenchmarking/results_processing/tests/test_performance_profiler.py::PerformanceProfillerTests::test_correct_prepare_profile_data", "fitbenchmarking/utils/tests/test_options_minimizers.py::UserMininimizerOptionTests::test_minimizer_mantid_invalid", "fitbenchmarking/core/tests/test_fitting_benchmarking_benchmark_problems.py::LoopOverBenchmarkProblemsTests::test_run_multiple_failed_problems", "fitbenchmarking/jacobian/tests/test_jacobian.py::TestJacobianClass::test_analytic_raise_error", "fitbenchmarking/jacobian/tests/test_jacobian.py::TestDerivCostFunc::test_scipy_cs_point_eval", "fitbenchmarking/utils/tests/test_options_logging.py::LoggingOptionTests::test_external_output_default", "fitbenchmarking/utils/tests/test_options_minimizers.py::MininimizerOptionTests::test_minimizer_scipy", "fitbenchmarking/utils/tests/test_options_plotting.py::UserPlottingOptionTests::test_minimizer_results_dir_valid", "fitbenchmarking/utils/tests/test_options_minimizers.py::UserMininimizerOptionTests::test_minimizer_gsl_invalid", "fitbenchmarking/cost_func/tests/test_cost_func.py::TestPoissonCostFunc::test_eval_cost_correct", "fitbenchmarking/core/tests/test_fitting_benchmarking_minimizers.py::LoopOverMinimizersTests::test_run_minimzers_none_selected", "fitbenchmarking/utils/tests/test_options_fitting.py::UserFittingOptionTests::test_invalid_option_key", "fitbenchmarking/jacobian/tests/test_jacobian.py::TestJacobianClass::test_scipy_cs_eval", "fitbenchmarking/jacobian/tests/test_jacobian.py::TestDerivCostFunc::test_analytic_cutest", "fitbenchmarking/core/tests/test_results_output.py::CreateProblemLevelIndex::test_creation_index_page", "fitbenchmarking/utils/tests/test_options_minimizers.py::UserMininimizerOptionTests::test_minimizer_scipy_ls_invalid", "fitbenchmarking/utils/tests/test_options_logging.py::LoggingOptionTests::test_level_default", "fitbenchmarking/utils/tests/test_options_minimizers.py::UserMininimizerOptionTests::test_minimizer_dfo_invalid", "fitbenchmarking/jacobian/tests/test_jacobian.py::TestDerivCostFunc::test_scipy_two_point_eval", "fitbenchmarking/utils/tests/test_options_logging.py::UserLoggingOptionTests::test_invalid_option_key", "fitbenchmarking/utils/tests/test_options_logging.py::UserLoggingOptionTests::test_minimizer_log_append_valid", "fitbenchmarking/utils/tests/test_options_plotting.py::PlottingOptionTests::test_results_dir_default", "fitbenchmarking/cli/tests/test_exception_handler.py::TestExceptionHandler::test_non_fb_exception", "fitbenchmarking/utils/tests/test_output_grabber.py::OutputGrabberTests::test_correct_stderr", "fitbenchmarking/cost_func/tests/test_cost_func.py::TestNLLSCostFunc::test_eval_r_raise_error", "fitbenchmarking/utils/tests/test_options_minimizers.py::MininimizerOptionTests::test_minimizer_ralfit", "fitbenchmarking/results_processing/tests/test_plots.py::PlotTests::test_multivariate_plot", "fitbenchmarking/utils/tests/test_options_fitting.py::FittingOptionTests::test_software_default", "fitbenchmarking/utils/tests/test_options_minimizers.py::UserMininimizerOptionTests::test_invalid_option_key", "fitbenchmarking/utils/tests/test_logger.py::TestSetupLogger::test_append_mode_off", "fitbenchmarking/core/tests/test_fitting_benchmarking_starting_values.py::LoopOverStartingValuesTests::test_run_reports_unselected_minimizers", "fitbenchmarking/utils/tests/test_options_logging.py::UserLoggingOptionTests::test_minimizer_log_level_valid", "fitbenchmarking/utils/tests/test_options_minimizers.py::UserMininimizerOptionTests::test_minimizer_scipy_invalid", "fitbenchmarking/utils/tests/test_options_plotting.py::UserPlottingOptionTests::test_minimizer_table_type_invalid", "fitbenchmarking/utils/tests/test_options_fitting.py::UserFittingOptionTests::test_minimizer_num_runs_valid", "fitbenchmarking/cost_func/tests/test_cost_func.py::TestNLLSCostFunc::test_eval_r_correct_evaluation", "fitbenchmarking/utils/tests/test_options_logging.py::UserLoggingOptionTests::test_minimizer_external_output_valid", "fitbenchmarking/utils/tests/test_logger.py::TestSetupLogger::test_append_mode_on", "fitbenchmarking/utils/tests/test_options_minimizers.py::UserMininimizerOptionTests::test_minimizer_gsl_valid", "fitbenchmarking/utils/tests/test_options_plotting.py::PlottingOptionTests::test_comparison_mode_default", "fitbenchmarking/parsing/tests/test_parsers.py::TestParsers::test_jacobian_evaluation[/app/repo/fitbenchmarking/parsing/tests/nist/jacobian_evaluations.json-nist]", "fitbenchmarking/utils/tests/test_options_minimizers.py::UserMininimizerOptionTests::test_minimizer_ralfit_valid", "fitbenchmarking/cli/tests/test_exception_handler.py::TestExceptionHandler::test_return_unchanged", "fitbenchmarking/parsing/tests/test_parsers.py::TestParserFactory::test_unknown_parser", "fitbenchmarking/core/tests/test_results_output.py::CreatePlotsTests::test_plot_error", "fitbenchmarking/jacobian/tests/test_jacobian.py::TestJacobianClass::test_numdifftools_eval", "fitbenchmarking/utils/tests/test_options_fitting.py::UserFittingOptionTests::test_minimizer_cost_func_type_valid", "fitbenchmarking/core/tests/test_results_output.py::CreatePlotsTests::test_create_plots_with_params", "fitbenchmarking/parsing/tests/test_fitting_problem.py::TestFittingProblem::test_correct_data_multi_fit", "fitbenchmarking/utils/tests/test_options_fitting.py::UserFittingOptionTests::test_minimizer_algorithm_type_invalid", "fitbenchmarking/utils/tests/test_options_generic.py::OptionsWriteTests::test_create_config", "fitbenchmarking/utils/tests/test_create_dirs.py::CreateDirsTests::test_support_pages_create_correct_dir", "fitbenchmarking/jacobian/tests/test_jacobian.py::TestCachedFuncValues::test_check_none_cached_eval", "fitbenchmarking/results_processing/tests/test_plots.py::PlotTests::test_init_creates_line", "fitbenchmarking/cost_func/tests/test_cost_func.py::TestHellingerNLLSCostFunc::test_eval_r_raise_error", "fitbenchmarking/parsing/tests/test_fitting_problem.py::TestFittingProblem::test_eval_model_raise_error", "fitbenchmarking/core/tests/test_fitting_benchmarking_software.py::LoopOverSoftwareTests::test_incorrect_software", "fitbenchmarking/utils/tests/test_options_logging.py::UserLoggingOptionTests::test_minimizer_log_level_invalid", "fitbenchmarking/utils/tests/test_options_minimizers.py::MininimizerOptionTests::test_minimizer_dfo", "fitbenchmarking/utils/tests/test_options_fitting.py::UserFittingOptionTests::test_minimizer_jac_method_valid", "fitbenchmarking/parsing/tests/test_parsers.py::TestParsers::test_parsers[expected2-nist-/app/repo/fitbenchmarking/parsing/tests/nist/start_end_x.dat]", "fitbenchmarking/core/tests/test_fitting_benchmarking_software.py::LoopOverSoftwareTests::test_run_software_all_failed_minimizers", "fitbenchmarking/results_processing/tests/test_support_page.py::CreateTests::test_create_unique_files", "fitbenchmarking/core/tests/test_fitting_benchmarking_starting_values.py::LoopOverStartingValuesTests::test_run_multiple_starting_values", "fitbenchmarking/utils/tests/test_options_plotting.py::PlottingOptionTests::test_cmap_range_default", "fitbenchmarking/parsing/tests/test_parsers.py::TestParsers::test_factory[fitbenchmark-/app/repo/fitbenchmarking/parsing/tests/fitbenchmark/basic.txt]", "fitbenchmarking/core/tests/test_fitting_benchmarking_starting_values.py::LoopOverStartingValuesTests::test_run_reports_failed_problems", "fitbenchmarking/cost_func/tests/test_cost_func.py::TestHellingerNLLSCostFunc::test_eval_cost", "fitbenchmarking/utils/tests/test_options_minimizers.py::UserMininimizerOptionTests::test_minimizer_bumps_valid", "fitbenchmarking/parsing/tests/test_parsers.py::TestParsers::test_factory[fitbenchmark-/app/repo/fitbenchmarking/parsing/tests/fitbenchmark/start_end_x.txt]", "fitbenchmarking/utils/tests/test_options_minimizers.py::MininimizerOptionTests::test_minimizer_mantid", "fitbenchmarking/core/tests/test_fitting_benchmarking_minimizers.py::LoopOverMinimizersTests::test_no_bounds_minimizer", "fitbenchmarking/cli/tests/test_main.py::TestMain::test_all_dummy_results_produced", "fitbenchmarking/utils/tests/test_options_generic.py::OptionsWriteTests::test_user_section_valid", "fitbenchmarking/results_processing/tests/test_plots.py::PlotTests::test_plot_best_create_files", "fitbenchmarking/utils/tests/test_options_logging.py::UserLoggingOptionTests::test_minimizer_external_output_invalid", "fitbenchmarking/utils/tests/test_options_minimizers.py::UserMininimizerOptionTests::test_minimizer_dfo_valid", "fitbenchmarking/parsing/tests/test_fitting_problem.py::TestFittingProblem::test_get_function_params", "fitbenchmarking/utils/tests/test_options_fitting.py::UserFittingOptionTests::test_minimizer_num_runs_invalid", "fitbenchmarking/utils/tests/test_options_generic.py::OptionsWriteTests::test_write_to_stream", "fitbenchmarking/core/tests/test_fitting_benchmarking_software.py::LoopOverSoftwareTests::test_run_software_failed_minimizers", "fitbenchmarking/parsing/tests/test_parsers.py::TestParsers::test_function_evaluation[/app/repo/fitbenchmarking/parsing/tests/nist/function_evaluations.json-nist]", "fitbenchmarking/utils/tests/test_options_generic.py::OptionsWriteTests::test_user_section_invalid", "fitbenchmarking/parsing/tests/test_fitting_problem.py::TestFittingProblem::test_eval_model_correct_evaluation", "fitbenchmarking/results_processing/tests/test_support_page.py::GetFigurePathsTests::test_no_links", "fitbenchmarking/core/tests/test_fitting_benchmarking_starting_values.py::LoopOverStartingValuesTests::test_run_one_starting_values", "fitbenchmarking/utils/tests/test_options_jacobian.py::UserJacobianOptionTests::test_minimizer_num_method_invalid", "fitbenchmarking/cost_func/tests/test_cost_func.py::TestWeightedNLLSCostFunc::test_eval_r_correct_evaluation", "fitbenchmarking/utils/tests/test_options_plotting.py::UserPlottingOptionTests::test_minimizer_colour_map_valid", "fitbenchmarking/utils/tests/test_options_minimizers.py::UserMininimizerOptionTests::test_minimizer_mantid_valid", "fitbenchmarking/utils/tests/test_misc.py::CreateDirsTests::test_get_js", "fitbenchmarking/jacobian/tests/test_jacobian.py::TestJacobianClass::test_analytic_cutest_hellinger", "fitbenchmarking/utils/tests/test_options_minimizers.py::MininimizerOptionTests::test_minimizer_minuit", "fitbenchmarking/utils/tests/test_fitbm_result.py::FitbmResultTests::test_sanitised_name", "fitbenchmarking/utils/tests/test_options_plotting.py::UserPlottingOptionTests::test_minimizer_cmap_range_invalid", "fitbenchmarking/utils/tests/test_options_plotting.py::UserPlottingOptionTests::test_minimizer_make_plots_invalid", "fitbenchmarking/jacobian/tests/test_jacobian.py::TestJacobianClass::test_default", "fitbenchmarking/cost_func/tests/test_cost_func.py::TestPoissonCostFunc::test_safe_a_log_b", "fitbenchmarking/jacobian/tests/test_jacobian.py::TestJacobianClass::test_analytic_cutest_no_errors", "fitbenchmarking/utils/tests/test_create_dirs.py::CreateDirsTests::test_css_create_correct_dir", "fitbenchmarking/utils/tests/test_options_minimizers.py::UserMininimizerOptionTests::test_minimizer_minuit_invalid", "fitbenchmarking/utils/tests/test_options_fitting.py::FittingOptionTests::test_cost_func_default", "fitbenchmarking/utils/tests/test_options_plotting.py::UserPlottingOptionTests::test_minimizer_cmap_range_valid", "fitbenchmarking/jacobian/tests/test_jacobian.py::TestDerivCostFunc::test_scipy_three_point_eval", "fitbenchmarking/utils/tests/test_options_fitting.py::FittingOptionTests::test_algorithm_type_default", "fitbenchmarking/parsing/tests/test_parsers.py::TestParsers::test_parsers[expected0-cutest-/app/repo/fitbenchmarking/parsing/tests/cutest/start_end_x.SIF]", "fitbenchmarking/utils/tests/test_options_generic.py::OptionsWriteTests::test_write", "fitbenchmarking/core/tests/test_fitting_benchmarking_minimizers.py::LoopOverMinimizersTests::test_run_minimzers_all", "fitbenchmarking/results_processing/tests/test_plots.py::PlotTests::test_plot_data_creates_line", "fitbenchmarking/utils/tests/test_options_minimizers.py::UserMininimizerOptionTests::test_minimizer_scipy_valid", "fitbenchmarking/utils/tests/test_fitbm_result.py::FitbmResultTests::test_norm_acc", "fitbenchmarking/results_processing/tests/test_support_page.py::CreateProbGroupTests::test_create_files", "fitbenchmarking/utils/tests/test_options_plotting.py::UserPlottingOptionTests::test_minimizer_table_type_valid", "fitbenchmarking/cost_func/tests/test_cost_func.py::TestWeightedNLLSCostFunc::test_eval_cost", "fitbenchmarking/jacobian/tests/test_jacobian.py::TestJacobianClass::test_scipy_three_point_eval", "fitbenchmarking/results_processing/tests/test_plots.py::PlotTests::test_plot_data_updates_line", "fitbenchmarking/jacobian/tests/test_jacobian.py::TestCachedFuncValues::test_check_cached_eval", "fitbenchmarking/utils/tests/test_fitbm_result.py::FitbmResultTests::test_norm_runtime", "fitbenchmarking/utils/tests/test_options_plotting.py::PlottingOptionTests::test_table_type_default", "fitbenchmarking/utils/tests/test_options_plotting.py::UserPlottingOptionTests::test_invalid_option_key", "fitbenchmarking/utils/tests/test_create_dirs.py::CreateDirsTests::test_figures_create_correct_dir", "fitbenchmarking/core/tests/test_fitting_benchmarking_minimizers.py::LoopOverMinimizersTests::test_run_minimzers_selected"], "language": "python", "test_command": "source /saved/ENV || source /saved/*/ENV && pytest --no-header -rA --tb=no -p no:cacheprovider --continue-on-collection-errors", "test_output_parser": "python/parse_log_pytest_v3", "image_storage_uri": "vmvm-registry.fbinfra.net/repomate_image_activ_pytest/fitbenchmarking_fitbenchmarking:2012bea1d077ce6f5e0e370cd0c73ddc5cdb9033", "patch": "", "test_patch": "[\"diff --git a/docs/source/extending/hessian_extend.rst b/docs/source/extending/hessian_extend.rst\\nnew file mode 100644\\nindex 000000000..733aa3a02\\n--- /dev/null\\n+++ b/docs/source/extending/hessian_extend.rst\\n@@ -0,0 +1,57 @@\\n+.. _hessian_extend:\\n+\\n+###################\\n+Adding new Hessians\\n+###################\\n+\\n+*This section describes how to add further methods to approximate the\\n+Hessian within FitBenchmarking*\\n+\\n+In order to add a new Hessian evaluation method, ``<hes_method>``,\\n+you will need to:\\n+\\n+1. Create ``fitbenchmarking/hessian/<hes_method>_hessian.py``,\\n+   which contains a new subclass of\\n+   :class:`~fitbenchmarking.hessian.base_hessian.hessian`.\\n+   Then implement the methods:\\n+\\n+    -  .. automethod:: fitbenchmarking.hessian.base_hessian.Hessian.eval()\\n+              :noindex:\\n+    -  .. automethod:: fitbenchmarking.hessian.base_hessian.Hessian.eval_cost()\\n+              :noindex:\\n+\\n+   The method is set sequentially within\\n+   :meth:`~fitbenchmarking.core.fitting_benchmarking.loop_over_hessians()` by\\n+   using the ``method`` attribute of the class.\\n+\\n+2. Enable the new method as an option in :ref:`fitting_option`,\\n+   following the instructions in :ref:`options_extend`.  Specifically:\\n+   \\n+   * Amend the ``VALID_FITTING`` dictionary so that the element associated\\n+     with the ``hes_method`` key contains the new ``<hes_method>``.\\n+\\n+3. Document the available Hessians by:\\n+\\n+  * adding to the list of available ``method`` options under ``hes_method`` in :ref:`fitting_option`.\\n+  * updating any example files in the ``examples`` directory\\n+\\n+4. Create tests for the Hessian evaluation in\\n+   ``fitbenchmarking/hessian/tests/test_hessians.py``.\\n+\\n+\\n+The :class:`~fitbenchmarking.parsing.fitting_problem.FittingProblem` and :class:`~fitbenchmarking.cost_func.base_cost_func.CostFunc`\\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n+\\n+When adding new Hessian, you will find it helpful to make use of the\\n+following members of the :class:`~fitbenchmarking.parsing.fitting_problem.FittingProblem`\\n+and subclasses of :class:`~fitbenchmarking.cost_func.base_cost_func.CostFunc`:\\n+\\n+.. currentmodule:: fitbenchmarking.parsing.fitting_problem\\n+.. autoclass:: fitbenchmarking.parsing.fitting_problem.FittingProblem\\n+          :members: eval_model, cache_model_x, data_x, data_y, data_e\\n+          :noindex:\\n+\\n+.. currentmodule:: fitbenchmarking.cost_func.base_cost_func\\n+.. autoclass:: fitbenchmarking.cost_func.base_cost_func.CostFunc\\n+          :members: eval_cost, cache_cost_x\\n+          :noindex:\\n\\\\ No newline at end of file\\n\",\"diff --git a/docs/source/extending/parsers.rst b/docs/source/extending/parsers.rst\\nindex afdb8ef48..c981906de 100644\\n--- a/docs/source/extending/parsers.rst\\n+++ b/docs/source/extending/parsers.rst\\n@@ -74,7 +74,15 @@ To add a new fitting problem definition type, complete the following steps:\\n      information*, then in ``test_parsers.py`` add \\n      ``<format_name>`` to the ``JACOBIAN_ENABLED_PARSERS`` global variable.\\n      Then add a file ``jacobian_evaluations.json`` to\\n-     ``fitbenchmarking/parsing/tests/<format_name>/``, which tests that the Jacobian evaluation behaves is as expected.\\n+     ``fitbenchmarking/parsing/tests/<format_name>/``, which tests that the Jacobian evaluation behaves as expected.\\n+     This file should have the same file structure as `function_evaluations.json`,\\n+     and works in a similar way. \\n+\\n+   - **Hessian tests**: *If the parser you add has analytic Hessian\\n+     information*, then in ``test_parsers.py`` add \\n+     ``<format_name>`` to the ``HESSIAN_ENABLED_PARSERS`` global variable.\\n+     Then add a file ``hessian_evaluations.json`` to\\n+     ``fitbenchmarking/parsing/tests/<format_name>/``, which tests that the Hessian evaluation behaves as expected.\\n      This file should have the same file structure as `function_evaluations.json`,\\n      and works in a similar way. \\n \\n\",\"diff --git a/docs/source/users/options/fitting_option.rst b/docs/source/users/options/fitting_option.rst\\nindex 3c5318bf6..3d1fb50c3 100644\\n--- a/docs/source/users/options/fitting_option.rst\\n+++ b/docs/source/users/options/fitting_option.rst\\n@@ -120,6 +120,28 @@ Default is ``default``\\n    Currently analytic Jacobians are available are only available for\\n    problems that use the cutest and NIST parsers.\\n \\n+\\n+Hessian method (:code:`hes_method`)\\n+------------------------------------\\n+\\n+This sets the Hessian used. Current Hessian methods are:\\n+\\n+* ``default`` - Hessian information is not passed to minimizers\\n+* ``analytic`` - uses the analytic Hessian extracted from the fitting problem.\\n+  \\n+Default is ``default``\\n+\\n+.. code-block:: rst\\n+\\n+    [FITTING]\\n+    hes_method: default\\n+\\n+.. warning::\\n+\\n+   Currently analytic Hessians are available are only available for\\n+   problems that use the NIST parser and for the ``nlls`` and\\n+   ``weighted_nlls`` cost functions.\\n+\\n Cost function (:code:`cost_func_type`)\\n --------------------------------------\\n \\n\",\"diff --git a/fitbenchmarking/controllers/tests/test_controllers.py b/fitbenchmarking/controllers/tests/test_controllers.py\\nindex 8e9d43298..e982ad4f9 100644\\n--- a/fitbenchmarking/controllers/tests/test_controllers.py\\n+++ b/fitbenchmarking/controllers/tests/test_controllers.py\\n@@ -123,6 +123,22 @@ def check_jac_info(self, controller, expected_has_jac, expected_jac_list):\\n         assert has_jacobian == expected_has_jac\\n         assert jacobian_list == expected_jac_list\\n \\n+    def check_hes_info(self, controller, expected_has_hes, expected_hes_list):\\n+        \\\"\\\"\\\"\\n+        Utility function to check controller.jacobian_information() produces\\n+        a success flag\\n+\\n+        :param controller: Controller to test, with setup already completed\\n+        :type controller: Object derived from BaseSoftwareController\\n+        :param expected_has_hes: expected has_hessian value\\n+        :type expected_has_hes: bool\\n+        :param expected_hes_list: expected hessian_list value\\n+        :type expected_hes_list: list\\n+        \\\"\\\"\\\"\\n+        has_hessian, hessian_list = controller.hessian_information()\\n+        assert has_hessian == expected_has_hes\\n+        assert hessian_list == expected_hes_list\\n+\\n     def check_converged(self, controller):\\n         \\\"\\\"\\\"\\n         Utility function to check controller.cleanup() produces a success flag\\n@@ -375,6 +391,9 @@ def test_bumps(self):\\n         self.shared_tests.check_jac_info(controller,\\n                                          False,\\n                                          [])\\n+        self.shared_tests.check_hes_info(controller,\\n+                                         False,\\n+                                         [])\\n \\n         controller._status = 0\\n         self.shared_tests.check_converged(controller)\\n@@ -394,6 +413,10 @@ def test_dfo(self):\\n         self.shared_tests.check_jac_info(controller,\\n                                          False,\\n                                          [])\\n+        self.shared_tests.check_hes_info(controller,\\n+                                         False,\\n+                                         [])\\n+\\n         for minimizer in minimizers:\\n             controller.minimizer = minimizer\\n             self.shared_tests.controller_run_test(controller)\\n@@ -415,6 +438,9 @@ def test_minuit(self):\\n         self.shared_tests.check_jac_info(controller,\\n                                          False,\\n                                          [])\\n+        self.shared_tests.check_hes_info(controller,\\n+                                         False,\\n+                                         [])\\n \\n         controller._status = 0\\n         self.shared_tests.check_converged(controller)\\n@@ -432,6 +458,9 @@ def test_scipy(self):\\n         self.shared_tests.check_jac_info(controller,\\n                                          True,\\n                                          [\\\"Nelder-Mead\\\", \\\"Powell\\\"])\\n+        self.shared_tests.check_hes_info(controller,\\n+                                         True,\\n+                                         ['Newton-CG'])\\n         controller.result.success = True\\n         self.shared_tests.check_converged(controller)\\n         controller.result.success = False\\n@@ -451,6 +480,9 @@ def test_scipy_ls(self):\\n         self.shared_tests.check_jac_info(controller,\\n                                          True,\\n                                          [None])\\n+        self.shared_tests.check_hes_info(controller,\\n+                                         False,\\n+                                         [])\\n \\n         controller._status = 1\\n         self.shared_tests.check_converged(controller)\\n@@ -601,6 +633,9 @@ def test_levmar(self):\\n         self.shared_tests.check_jac_info(controller,\\n                                          True,\\n                                          [])\\n+        self.shared_tests.check_hes_info(controller,\\n+                                         False,\\n+                                         [])\\n \\n         controller._info = (0, 1, 2, \\\"Stop by small Dp\\\", 4, 5, 6)\\n         self.shared_tests.check_converged(controller)\\n@@ -625,6 +660,9 @@ def test_mantid(self):\\n         self.shared_tests.check_jac_info(controller,\\n                                          True,\\n                                          [\\\"Simplex\\\"])\\n+        self.shared_tests.check_hes_info(controller,\\n+                                         False,\\n+                                         [])\\n         controller._status = \\\"success\\\"\\n         self.shared_tests.check_converged(controller)\\n         controller._status = \\\"Failed to converge\\\"\\n@@ -712,6 +750,9 @@ def test_gsl(self):\\n         self.shared_tests.check_jac_info(controller,\\n                                          True,\\n                                          [\\\"nmsimplex\\\", \\\"nmsimplex2\\\"])\\n+        self.shared_tests.check_hes_info(controller,\\n+                                         False,\\n+                                         [])\\n         # test one from each class\\n         minimizers = ['lmsder',\\n                       'nmsimplex',\\n@@ -736,6 +777,9 @@ def test_ralfit(self):\\n         self.shared_tests.check_jac_info(controller,\\n                                          True,\\n                                          [])\\n+        self.shared_tests.check_hes_info(controller,\\n+                                         False,\\n+                                         [])\\n \\n         minimizers = ['gn', 'gn_reg', 'hybrid', 'hybrid_reg']\\n         for minimizer in minimizers:\\n@@ -791,6 +835,9 @@ def test_matlab(self):\\n         self.shared_tests.check_jac_info(controller,\\n                                          False,\\n                                          [])\\n+        self.shared_tests.check_hes_info(controller,\\n+                                         False,\\n+                                         [])\\n \\n         minimizers = ['Nelder-Mead Simplex']\\n         for minimizer in minimizers:\\n@@ -813,6 +860,9 @@ def test_matlab_opt(self):\\n         self.shared_tests.check_jac_info(controller,\\n                                          True,\\n                                          [])\\n+        self.shared_tests.check_hes_info(controller,\\n+                                         False,\\n+                                         [])\\n \\n         minimizers = ['levenberg-marquardt', 'trust-region-reflective']\\n         for minimizer in minimizers:\\n@@ -835,6 +885,9 @@ def test_matlab_stats(self):\\n         self.shared_tests.check_jac_info(controller,\\n                                          False,\\n                                          ['Levenberg-Marquardt'])\\n+        self.shared_tests.check_hes_info(controller,\\n+                                         False,\\n+                                         [])\\n \\n         minimizers = ['Levenberg-Marquardt']\\n         for minimizer in minimizers:\\n@@ -855,6 +908,9 @@ def test_matlab_curve(self):\\n         self.shared_tests.check_jac_info(controller,\\n                                          False,\\n                                          [])\\n+        self.shared_tests.check_hes_info(controller,\\n+                                         False,\\n+                                         [])\\n \\n         minimizers = ['Levenberg-Marquardt', 'Trust-Region']\\n         for minimizer in minimizers:\\n@@ -894,6 +950,9 @@ def test_scipy_go(self):\\n         self.shared_tests.check_jac_info(controller,\\n                                          True,\\n                                          ['differential_evolution'])\\n+        self.shared_tests.check_hes_info(controller,\\n+                                         False,\\n+                                         [])\\n \\n         controller._status = 0\\n         self.shared_tests.check_converged(controller)\\n@@ -912,6 +971,9 @@ def test_gradient_free(self):\\n         self.shared_tests.check_jac_info(controller,\\n                                          False,\\n                                          [])\\n+        self.shared_tests.check_hes_info(controller,\\n+                                         False,\\n+                                         [])\\n \\n         controller._status = 0\\n         self.shared_tests.check_converged(controller)\\n\",\"diff --git a/fitbenchmarking/core/tests/test_fitting_benchmarking_hessians.py b/fitbenchmarking/core/tests/test_fitting_benchmarking_hessians.py\\nnew file mode 100644\\nindex 000000000..f7b0e5ccf\\n--- /dev/null\\n+++ b/fitbenchmarking/core/tests/test_fitting_benchmarking_hessians.py\\n@@ -0,0 +1,221 @@\\n+\\\"\\\"\\\"\\n+Tests for fitbenchmarking.core.fitting_benchmarking.loop_over_hessians\\n+\\\"\\\"\\\"\\n+import inspect\\n+import os\\n+import unittest\\n+from unittest.mock import patch\\n+\\n+from fitbenchmarking import mock_problems\\n+from fitbenchmarking.controllers.base_controller import Controller\\n+from fitbenchmarking.core.fitting_benchmarking import loop_over_hessians\\n+from fitbenchmarking.cost_func.nlls_cost_func import NLLSCostFunc\\n+from fitbenchmarking.parsing.parser_factory import parse_problem_file\\n+from fitbenchmarking.utils import output_grabber\\n+from fitbenchmarking.utils.options import Options\\n+\\n+# Due to construction of the controllers two folder functions\\n+# pylint: disable=unnecessary-pass\\n+\\n+\\n+class DummyController(Controller):\\n+    \\\"\\\"\\\"\\n+    Minimal instantiatable subclass of Controller class for testing\\n+    \\\"\\\"\\\"\\n+\\n+    def __init__(self, cost_func):\\n+        \\\"\\\"\\\"\\n+        Initialize dummy controller\\n+\\n+        :param cost_func: cost function class\\n+        :type cost_func: CostFunc class\\n+        \\\"\\\"\\\"\\n+        super().__init__(cost_func)\\n+        self.algorithm_check = {'all': ['deriv_free_algorithm', 'general'],\\n+                                'ls': [None],\\n+                                'deriv_free': ['deriv_free_algorithm'],\\n+                                'general': ['general']}\\n+        self.final_params_expected = [[1, 2, 3, 4], [4, 3, 2, 1]]\\n+        self.flag_expected = [0, 1]\\n+        self.count = 0\\n+        self.has_jacobian = []\\n+        self.invalid_jacobians = []\\n+        self.has_hessian = []\\n+        self.valid_hessians = []\\n+\\n+    def setup(self):\\n+        \\\"\\\"\\\"\\n+        Mock controller setup function\\n+        \\\"\\\"\\\"\\n+        pass\\n+\\n+    def fit(self):\\n+        \\\"\\\"\\\"\\n+        Mock controller fit function\\n+        \\\"\\\"\\\"\\n+        pass\\n+\\n+    def jacobian_information(self):\\n+        \\\"\\\"\\\"\\n+        Mock controller jacobian_information function\\n+        \\\"\\\"\\\"\\n+        has_jacobian = self.has_jacobian[self.count]\\n+        invalid_jacobians = self.invalid_jacobians[self.count]\\n+        return has_jacobian, invalid_jacobians\\n+\\n+    def hessian_information(self):\\n+        \\\"\\\"\\\"\\n+        Mock controller hessian_information function\\n+        \\\"\\\"\\\"\\n+        has_hessian = self.has_hessian[self.count]\\n+        valid_hessians = self.valid_hessians[self.count]\\n+        return has_hessian, valid_hessians\\n+\\n+    def cleanup(self):\\n+        \\\"\\\"\\\"\\n+        Mock controller cleanup function\\n+        \\\"\\\"\\\"\\n+        self.final_params = self.final_params_expected[self.count]\\n+        self.flag = self.flag_expected[self.count]\\n+        self.count += 1\\n+        self.count = self.count % len(self.flag_expected)\\n+\\n+\\n+def make_cost_function(file_name='cubic.dat', minimizers=None):\\n+    \\\"\\\"\\\"\\n+    Helper function that returns a simple fitting problem\\n+    \\\"\\\"\\\"\\n+    options = Options()\\n+    if minimizers:\\n+        options.minimizers = minimizers\\n+\\n+    bench_prob_dir = os.path.dirname(inspect.getfile(mock_problems))\\n+    fname = os.path.join(bench_prob_dir, file_name)\\n+\\n+    fitting_problem = parse_problem_file(fname, options)\\n+    fitting_problem.correct_data()\\n+    cost_func = NLLSCostFunc(fitting_problem)\\n+    return cost_func\\n+\\n+\\n+class LoopOverHessiansTests(unittest.TestCase):\\n+    \\\"\\\"\\\"\\n+    loop_over_hessians tests\\n+    \\\"\\\"\\\"\\n+\\n+    def setUp(self):\\n+        \\\"\\\"\\\"\\n+        Setting up problem for tests\\n+        \\\"\\\"\\\"\\n+        self.minimizers = [\\\"deriv_free_algorithm\\\", \\\"general\\\"]\\n+        self.cost_func = make_cost_function(minimizers=self.minimizers)\\n+        self.problem = self.cost_func.problem\\n+        self.controller = DummyController(cost_func=self.cost_func)\\n+        self.options = self.problem.options\\n+        self.grabbed_output = output_grabber.OutputGrabber(self.options)\\n+        self.controller.parameter_set = 0\\n+\\n+    def test_single_hessian(self):\\n+        \\\"\\\"\\\"\\n+        Test to check that only one Hessian option has been added\\n+        \\\"\\\"\\\"\\n+        self.options.hes_method = [\\\"analytic\\\"]\\n+        self.controller.has_jacobian = [True]\\n+        self.controller.invalid_jacobians = [\\\"deriv_free_algorithm\\\"]\\n+        self.controller.minimizer = \\\"general\\\"\\n+        self.controller.has_hessian = [True]\\n+        self.controller.valid_hessians = [\\\"general\\\"]\\n+        new_name = ['general, analytic hessian']\\n+        jacobian = False\\n+        minimizer_name = \\\"general\\\"\\n+        _, _, new_minimizer_list = \\\\\\n+            loop_over_hessians(self.controller,\\n+                               self.options,\\n+                               minimizer_name,\\n+                               jacobian,\\n+                               self.grabbed_output)\\n+        assert new_minimizer_list == new_name\\n+\\n+    def test_single_no_hessian(self):\\n+        \\\"\\\"\\\"\\n+        Test that checks that the minimizer doesn't need Hessian information\\n+        and the name does not have Hessian information in it\\n+        \\\"\\\"\\\"\\n+        self.options.hes_method = [\\\"analytic\\\"]\\n+        self.controller.has_jacobian = [True]\\n+        self.controller.invalid_jacobians = [\\\"deriv_free_algorithm\\\"]\\n+        self.controller.minimizer = \\\"deriv_free_algorithm\\\"\\n+        self.controller.has_hessian = [False]\\n+        self.controller.valid_hessians = [\\\"general\\\"]\\n+        new_name = ['deriv_free_algorithm']\\n+        jacobian = False\\n+        minimizer_name = \\\"deriv_free_algorithm\\\"\\n+        _, _, new_minimizer_list = \\\\\\n+            loop_over_hessians(self.controller,\\n+                               self.options,\\n+                               minimizer_name,\\n+                               jacobian,\\n+                               self.grabbed_output)\\n+        assert new_minimizer_list == new_name\\n+\\n+    # pylint: disable=unused-argument\\n+    @patch.object(DummyController, \\\"cleanup\\\")\\n+    @patch.object(DummyController, \\\"check_bounds_respected\\\")\\n+    def test_bounds_respected_func_called(\\n+            self, check_bounds_respected, cleanup):\\n+        \\\"\\\"\\\"\\n+        Test that the check to verify that bounds are respected is called when\\n+        the controller runs succesfully.\\n+        \\\"\\\"\\\"\\n+        self.controller.problem.value_ranges = {'test': (0, 1)}\\n+        self.controller.has_jacobian = [True]\\n+        self.controller.invalid_jacobians = [\\\"deriv_free_algorithm\\\"]\\n+        self.controller.minimizer = \\\"deriv_free_algorithm\\\"\\n+        self.controller.has_hessian = [False]\\n+        self.controller.valid_hessians = [\\\"general\\\"]\\n+        jacobian = False\\n+        minimizer_name = \\\"deriv_free_algorithm\\\"\\n+\\n+        # Cleanup has been mocked out with a no-op, so set the outputs now.\\n+        self.controller.flag = 0\\n+        self.controller.final_params = [1, 2, 3, 4]\\n+\\n+        _ = loop_over_hessians(self.controller,\\n+                               self.options,\\n+                               minimizer_name,\\n+                               jacobian,\\n+                               self.grabbed_output)\\n+        check_bounds_respected.assert_called()\\n+\\n+    @patch.object(DummyController, \\\"cleanup\\\")\\n+    @patch.object(DummyController, \\\"check_bounds_respected\\\")\\n+    def test_bounds_respected_func_not_called(\\n+            self, check_bounds_respected, cleanup):\\n+        \\\"\\\"\\\"\\n+        Test that the check to verify that bounds are respected is not called\\n+        when the controller fails.\\n+        \\\"\\\"\\\"\\n+        self.controller.problem.value_ranges = {'test': (0, 1)}\\n+        self.controller.has_jacobian = [True]\\n+        self.controller.invalid_jacobians = [\\\"deriv_free_algorithm\\\"]\\n+        self.controller.minimizer = \\\"deriv_free_algorithm\\\"\\n+        self.controller.has_hessian = [False]\\n+        self.controller.valid_hessians = [\\\"general\\\"]\\n+        jacobian = False\\n+        minimizer_name = \\\"deriv_free_algorithm\\\"\\n+\\n+        # Cleanup has been mocked out with a no-op, so set the outputs now.\\n+        self.controller.flag = 3\\n+        self.controller.final_params = [1, 2, 3, 4]\\n+\\n+        _ = loop_over_hessians(self.controller,\\n+                               self.options,\\n+                               minimizer_name,\\n+                               jacobian,\\n+                               self.grabbed_output)\\n+        check_bounds_respected.assert_not_called()\\n+    # pylint: enable=unused-argument\\n+\\n+\\n+if __name__ == \\\"__main__\\\":\\n+    unittest.main()\\n\",\"diff --git a/fitbenchmarking/core/tests/test_fitting_benchmarking_jacobians.py b/fitbenchmarking/core/tests/test_fitting_benchmarking_jacobians.py\\nindex 07f739bc2..89e7efee8 100644\\n--- a/fitbenchmarking/core/tests/test_fitting_benchmarking_jacobians.py\\n+++ b/fitbenchmarking/core/tests/test_fitting_benchmarking_jacobians.py\\n@@ -4,6 +4,7 @@\\n import inspect\\n import os\\n import unittest\\n+from unittest.mock import patch\\n \\n from fitbenchmarking import mock_problems\\n from fitbenchmarking.controllers.base_controller import Controller\\n@@ -16,6 +17,9 @@\\n # Due to construction of the controllers two folder functions\\n # pylint: disable=unnecessary-pass\\n \\n+# Defines the module which we mock out certain function calls for\\n+FITTING_DIR = \\\"fitbenchmarking.core.fitting_benchmarking\\\"\\n+\\n \\n class DummyController(Controller):\\n     \\\"\\\"\\\"\\n@@ -39,6 +43,8 @@ def __init__(self, cost_func):\\n         self.count = 0\\n         self.has_jacobian = []\\n         self.invalid_jacobians = []\\n+        self.has_hessian = []\\n+        self.valid_hessians = []\\n \\n     def setup(self):\\n         \\\"\\\"\\\"\\n@@ -60,6 +66,14 @@ def jacobian_information(self):\\n         invalid_jacobians = self.invalid_jacobians[self.count]\\n         return has_jacobian, invalid_jacobians\\n \\n+    def hessian_information(self):\\n+        \\\"\\\"\\\"\\n+        Mock controller jacobian_information function\\n+        \\\"\\\"\\\"\\n+        has_hessian = self.has_hessian[self.count]\\n+        valid_hessians = self.valid_hessians[self.count]\\n+        return has_hessian, valid_hessians\\n+\\n     def cleanup(self):\\n         \\\"\\\"\\\"\\n         Mock controller cleanup function\\n@@ -102,28 +116,35 @@ def setUp(self):\\n         self.controller = DummyController(cost_func=self.cost_func)\\n         self.options = self.problem.options\\n         self.grabbed_output = output_grabber.OutputGrabber(self.options)\\n-        self.controller.parameter_set = 0\\n \\n-    def test_single_jacobian(self):\\n+    def mock_func_call(self, *args):\\n+        \\\"\\\"\\\"\\n+        Mock function to be used instead of loop_over_hessians\\n+        \\\"\\\"\\\"\\n+        minimizer_name = args[2]\\n+        return [], [], [minimizer_name]\\n+\\n+    @patch('{}.loop_over_hessians'.format(FITTING_DIR))\\n+    def test_single_jacobian(self, loop_over_hessians):\\n         \\\"\\\"\\\"\\n-        Test to check that only on Jacobian option has been added\\n+        Test to check that only one Jacobian option has been added\\n         \\\"\\\"\\\"\\n         self.options.jac_method = [\\\"scipy\\\"]\\n         self.options.num_method = {\\\"scipy\\\": [\\\"3-point\\\"]}\\n         self.controller.has_jacobian = [True]\\n         self.controller.invalid_jacobians = [\\\"deriv_free_algorithm\\\"]\\n         self.controller.minimizer = \\\"general\\\"\\n+        self.controller.has_hessian = [False]\\n+        loop_over_hessians.side_effect = self.mock_func_call\\n         new_name = ['general: scipy 3-point']\\n-        results, _, new_minimizer_list = \\\\\\n+        _, _, new_minimizer_list = \\\\\\n             loop_over_jacobians(self.controller,\\n                                 self.options,\\n                                 self.grabbed_output)\\n-        assert all(isinstance(x, dict) for x in results)\\n-        assert all(x[\\\"minimizer\\\"] == name for x,\\n-                   name in zip(results, new_name))\\n         assert new_minimizer_list == new_name\\n \\n-    def test_multiple_jacobian(self):\\n+    @patch('{}.loop_over_hessians'.format(FITTING_DIR))\\n+    def test_multiple_jacobian(self, loop_over_hessians):\\n         \\\"\\\"\\\"\\n         Test to check multiple Jacobian options are set correctly\\n         \\\"\\\"\\\"\\n@@ -132,17 +153,17 @@ def test_multiple_jacobian(self):\\n         self.controller.has_jacobian = [True]\\n         self.controller.invalid_jacobians = [\\\"deriv_free_algorithm\\\"]\\n         self.controller.minimizer = \\\"general\\\"\\n+        self.controller.has_hessian = [False]\\n+        loop_over_hessians.side_effect = self.mock_func_call\\n         new_name = ['general: scipy 3-point', 'general: scipy 2-point']\\n-        results, _, new_minimizer_list = \\\\\\n+        _, _, new_minimizer_list = \\\\\\n             loop_over_jacobians(self.controller,\\n                                 self.options,\\n                                 self.grabbed_output)\\n-        assert all(isinstance(x, dict) for x in results)\\n-        assert all(x[\\\"minimizer\\\"] == name for x,\\n-                   name in zip(results, new_name))\\n         assert new_minimizer_list == new_name\\n \\n-    def test_single_no_jacobian(self):\\n+    @patch('{}.loop_over_hessians'.format(FITTING_DIR))\\n+    def test_single_no_jacobian(self, loop_over_hessians):\\n         \\\"\\\"\\\"\\n         Test that checks that the minimizer doesn't need Jacobian information\\n         and the name does not have Jacobian information in it\\n@@ -152,62 +173,15 @@ def test_single_no_jacobian(self):\\n         self.controller.has_jacobian = [True]\\n         self.controller.invalid_jacobians = [\\\"deriv_free_algorithm\\\"]\\n         self.controller.minimizer = \\\"deriv_free_algorithm\\\"\\n+        self.controller.has_hessian = [False]\\n+        loop_over_hessians.side_effect = self.mock_func_call\\n         new_name = ['deriv_free_algorithm']\\n-        results, _, new_minimizer_list = \\\\\\n+        _, _, new_minimizer_list = \\\\\\n             loop_over_jacobians(self.controller,\\n                                 self.options,\\n                                 self.grabbed_output)\\n-        assert all(isinstance(x, dict) for x in results)\\n-        assert all(x[\\\"minimizer\\\"] == name for x,\\n-                   name in zip(results, new_name))\\n         assert new_minimizer_list == new_name\\n \\n-    # pylint: disable=unused-argument\\n-    @unittest.mock.patch.object(DummyController, \\\"cleanup\\\")\\n-    @unittest.mock.patch.object(DummyController, \\\"check_bounds_respected\\\")\\n-    def test_bounds_respected_func_called(\\n-            self, check_bounds_respected, cleanup):\\n-        \\\"\\\"\\\"\\n-        Test that the check to verify that bounds are respected is called when\\n-        the controller runs succesfully.\\n-        \\\"\\\"\\\"\\n-        self.controller.problem.value_ranges = {'test': (0, 1)}\\n-        self.controller.has_jacobian = [True]\\n-        self.controller.invalid_jacobians = [\\\"deriv_free_algorithm\\\"]\\n-        self.controller.minimizer = \\\"deriv_free_algorithm\\\"\\n-\\n-        # Cleanup has been mocked out with a no-op, so set the outputs now.\\n-        self.controller.flag = 0\\n-        self.controller.final_params = [1, 2, 3, 4]\\n-\\n-        _ = loop_over_jacobians(self.controller,\\n-                                self.options,\\n-                                self.grabbed_output)\\n-        check_bounds_respected.assert_called()\\n-\\n-    @unittest.mock.patch.object(DummyController, \\\"cleanup\\\")\\n-    @unittest.mock.patch.object(DummyController, \\\"check_bounds_respected\\\")\\n-    def test_bounds_respected_func_not_called(\\n-            self, check_bounds_respected, cleanup):\\n-        \\\"\\\"\\\"\\n-        Test that the check to verify that bounds are respected is not called\\n-        when the controller fails.\\n-        \\\"\\\"\\\"\\n-        self.controller.problem.value_ranges = {'test': (0, 1)}\\n-        self.controller.has_jacobian = [True]\\n-        self.controller.invalid_jacobians = [\\\"deriv_free_algorithm\\\"]\\n-        self.controller.minimizer = \\\"deriv_free_algorithm\\\"\\n-\\n-        # Cleanup has been mocked out with a no-op, so set the outputs now.\\n-        self.controller.flag = 3\\n-        self.controller.final_params = [1, 2, 3, 4]\\n-\\n-        _ = loop_over_jacobians(self.controller,\\n-                                self.options,\\n-                                self.grabbed_output)\\n-        check_bounds_respected.assert_not_called()\\n-    # pylint: enable=unused-argument\\n-\\n \\n if __name__ == \\\"__main__\\\":\\n     unittest.main()\\n\",\"diff --git a/fitbenchmarking/core/tests/test_fitting_benchmarking_minimizers.py b/fitbenchmarking/core/tests/test_fitting_benchmarking_minimizers.py\\nindex 3bbfe92f3..58861fd73 100644\\n--- a/fitbenchmarking/core/tests/test_fitting_benchmarking_minimizers.py\\n+++ b/fitbenchmarking/core/tests/test_fitting_benchmarking_minimizers.py\\n@@ -4,6 +4,7 @@\\n import inspect\\n import os\\n import unittest\\n+from unittest.mock import patch\\n \\n from fitbenchmarking import mock_problems\\n from fitbenchmarking.controllers.base_controller import Controller\\n@@ -59,6 +60,12 @@ def jacobian_information(self):\\n         \\\"\\\"\\\"\\n         pass\\n \\n+    def hessian_information(self):\\n+        \\\"\\\"\\\"\\n+        Mock controller jacobian_information function\\n+        \\\"\\\"\\\"\\n+        pass\\n+\\n     def cleanup(self):\\n         \\\"\\\"\\\"\\n         Mock controller cleanup function\\n@@ -131,8 +138,8 @@ def test_run_minimzers_none_selected(self):\\n         assert minimizer_failed == self.minimizers\\n         assert new_minimizer_list == []\\n \\n-    @unittest.mock.patch('{}.loop_over_jacobians'.format(FITTING_DIR))\\n-    def test_run_minimzers_selected(self, loop_over_jacobians):\\n+    @patch('{}.loop_over_jacobians'.format(FITTING_DIR))\\n+    def test_run_minimzers_selected(self, loop_over_hessians):\\n         \\\"\\\"\\\"\\n         Tests that some minimizers are selected\\n         \\\"\\\"\\\"\\n@@ -140,7 +147,7 @@ def test_run_minimzers_selected(self, loop_over_jacobians):\\n         self.results = [[self.result_args]]\\n         self.chi_sq = 1\\n         self.minimizer_list = [[\\\"general\\\"]]\\n-        loop_over_jacobians.side_effect = self.mock_func_call\\n+        loop_over_hessians.side_effect = self.mock_func_call\\n \\n         results_problem, minimizer_failed, new_minimizer_list = \\\\\\n             loop_over_minimizers(self.controller, self.minimizers,\\n@@ -150,15 +157,15 @@ def test_run_minimzers_selected(self, loop_over_jacobians):\\n         assert minimizer_failed == [\\\"deriv_free_algorithm\\\"]\\n         assert new_minimizer_list == [\\\"general\\\"]\\n \\n-    @unittest.mock.patch('{}.loop_over_jacobians'.format(FITTING_DIR))\\n-    def test_run_minimzers_all(self, loop_over_jacobians):\\n+    @patch('{}.loop_over_jacobians'.format(FITTING_DIR))\\n+    def test_run_minimzers_all(self, loop_over_hessians):\\n         \\\"\\\"\\\"\\n         Tests that all minimizers are selected\\n         \\\"\\\"\\\"\\n         self.results = [[self.result_args], [self.result_args]]\\n         self.chi_sq = [1]\\n         self.minimizer_list = [[\\\"general\\\"], [\\\"deriv_free_algorithm\\\"]]\\n-        loop_over_jacobians.side_effect = self.mock_func_call\\n+        loop_over_hessians.side_effect = self.mock_func_call\\n \\n         results_problem, minimizer_failed, new_minimizer_list = \\\\\\n             loop_over_minimizers(self.controller, self.minimizers,\\n\",\"diff --git a/fitbenchmarking/hessian/tests/__init__.py b/fitbenchmarking/hessian/tests/__init__.py\\nnew file mode 100644\\nindex 000000000..e69de29bb\\n\",\"diff --git a/fitbenchmarking/hessian/tests/test_hessian.py b/fitbenchmarking/hessian/tests/test_hessian.py\\nnew file mode 100644\\nindex 000000000..91e5e45b2\\n--- /dev/null\\n+++ b/fitbenchmarking/hessian/tests/test_hessian.py\\n@@ -0,0 +1,194 @@\\n+\\\"\\\"\\\"\\n+Unit testing for the hessian directory.\\n+\\\"\\\"\\\"\\n+from unittest import TestCase\\n+\\n+import numpy as np\\n+\\n+from fitbenchmarking.cost_func.nlls_cost_func import NLLSCostFunc\\n+from fitbenchmarking.hessian.analytic_hessian import Analytic\\n+from fitbenchmarking.jacobian.analytic_jacobian import Analytic\\\\\\n+    as JacobianClass\\n+from fitbenchmarking.hessian.hessian_factory import create_hessian\\n+from fitbenchmarking.parsing.fitting_problem import FittingProblem\\n+from fitbenchmarking.utils import exceptions\\n+from fitbenchmarking.utils.options import Options\\n+\\n+\\n+def f(x, p1, p2):\\n+    \\\"\\\"\\\"\\n+    Test function for numerical Hessians\\n+\\n+    :param x: x data points, defaults to self.data_x\\n+    :type x: numpy array, optional\\n+    :param p1: parameter 1\\n+    :type p1: float\\n+    :param p2: parameter 1\\n+    :type p2: float\\n+\\n+    :return: function evaluation\\n+    :rtype: numpy array\\n+    \\\"\\\"\\\"\\n+    return p1 * np.exp(p2 * x)\\n+\\n+\\n+def J(x, p):\\n+    \\\"\\\"\\\"\\n+    Analytic Jacobian evaluation\\n+\\n+    :param x: x data points, defaults to self.data_x\\n+    :type x: numpy array, optional\\n+    :param p: list of parameters to fit\\n+    :type p: list\\n+\\n+    :return: Jacobian evaluation\\n+    :rtype: numpy array\\n+    \\\"\\\"\\\"\\n+\\n+    return np.column_stack((-np.exp(p[1] * x),\\n+                            -x * p[0] * np.exp(p[1] * x)))\\n+\\n+\\n+def H(x, p):\\n+    \\\"\\\"\\\"\\n+    Analytic Hessian evaluation\\n+\\n+    :param x: x data points, defaults to self.data_x\\n+    :type x: numpy array, optional\\n+    :param p: list of parameters to fit\\n+    :type p: list\\n+\\n+    :return: Hessian evaluation\\n+    :rtype: numpy array\\n+    \\\"\\\"\\\"\\n+    return np.array([[0*(np.ones(x.shape[0])), x*np.exp(p[1]*x)],\\n+                    [x*np.exp(p[1]*x), p[0]*x**2*np.exp(p[1]*x)], ])\\n+\\n+\\n+class TestHessianClass(TestCase):\\n+    \\\"\\\"\\\"\\n+    Tests for Hessian classes\\n+    \\\"\\\"\\\"\\n+\\n+    def setUp(self):\\n+        \\\"\\\"\\\"\\n+        Setting up tests\\n+        \\\"\\\"\\\"\\n+        options = Options()\\n+        options.cost_func_type = \\\"nlls\\\"\\n+        self.fitting_problem = FittingProblem(options)\\n+        self.fitting_problem.function = f\\n+        self.fitting_problem.jacobian = J\\n+        self.fitting_problem.hessian = H\\n+        self.fitting_problem.data_x = np.array([1, 2, 3, 4, 5])\\n+        self.fitting_problem.data_y = np.array([1, 2, 4, 8, 16])\\n+        self.cost_func = NLLSCostFunc(self.fitting_problem)\\n+        self.jacobian = JacobianClass(self.cost_func)\\n+        self.params = [6, 0.1]\\n+        self.f_eval = self.fitting_problem.data_y\\\\\\n+            - f(x=self.fitting_problem.data_x,\\n+                p1=self.params[0],\\n+                p2=self.params[1])\\n+        self.actual = H(x=self.fitting_problem.data_x, p=self.params)\\n+\\n+    def test_analytic_cutest_no_errors(self):\\n+        \\\"\\\"\\\"\\n+        Test analytic Hessian\\n+        \\\"\\\"\\\"\\n+        self.fitting_problem.options.cost_func_type = \\\"nlls\\\"\\n+        self.fitting_problem.format = \\\"cutest\\\"\\n+        hes = Analytic(self.cost_func, self.jacobian)\\n+        eval_result, _ = hes.eval(params=self.params)\\n+        self.actual = np.matmul(self.actual, self.f_eval)\\n+        self.assertTrue(np.isclose(self.actual, eval_result).all())\\n+\\n+    def test_analytic_cutest_weighted(self):\\n+        \\\"\\\"\\\"\\n+        Test analytic Hessian\\n+        \\\"\\\"\\\"\\n+        self.fitting_problem.options.cost_func_type = \\\"weighted_nlls\\\"\\n+        e = np.array([1, 2, 1, 3, 1])\\n+        self.fitting_problem.data_e = e\\n+        self.fitting_problem.format = \\\"cutest\\\"\\n+        hes = Analytic(self.cost_func, self.jacobian)\\n+        eval_result, _ = hes.eval(params=self.params)\\n+        scaled_actual = self.actual\\n+        for i in range(len(e)):\\n+            scaled_actual[:, :, i] = self.actual[:, :, i] / e[i]\\n+        scaled_actual = np.matmul(scaled_actual, self.f_eval)\\n+        self.assertTrue(np.isclose(scaled_actual, eval_result).all())\\n+\\n+    def test_analytic_raise_error(self):\\n+        \\\"\\\"\\\"\\n+        Test analytic Hessian raises an exception when problem.hessian is\\n+        not callable\\n+        \\\"\\\"\\\"\\n+        self.fitting_problem.hessian = None\\n+        with self.assertRaises(exceptions.NoHessianError):\\n+            Analytic(self.cost_func, self.jacobian)\\n+\\n+\\n+class TestHesCostFunc(TestCase):\\n+    \\\"\\\"\\\"\\n+    Tests for Hessian classes\\n+    \\\"\\\"\\\"\\n+\\n+    def setUp(self):\\n+        \\\"\\\"\\\"\\n+        Setting up tests\\n+        \\\"\\\"\\\"\\n+        options = Options()\\n+        options.cost_func_type = \\\"nlls\\\"\\n+        self.fitting_problem = FittingProblem(options)\\n+        self.fitting_problem.function = f\\n+        self.fitting_problem.jacobian = J\\n+        self.fitting_problem.hessian = H\\n+        self.fitting_problem.data_x = np.array([1, 2, 3, 4, 5])\\n+        self.fitting_problem.data_y = np.array([1, 2, 4, 8, 16])\\n+        self.params = [6, 0.1]\\n+        self.cost_func = NLLSCostFunc(self.fitting_problem)\\n+        self.jacobian = JacobianClass(self.cost_func)\\n+        J_eval = J(x=self.fitting_problem.data_x,\\n+                   p=self.params)\\n+        H_eval = H(x=self.fitting_problem.data_x,\\n+                   p=self.params)\\n+        f_eval = self.fitting_problem.data_y - f(x=self.fitting_problem.data_x,\\n+                                                 p1=self.params[0],\\n+                                                 p2=self.params[1])\\n+        self.actual = np.matmul(H_eval, f_eval) + np.matmul(J_eval.T, J_eval)\\n+\\n+    def test_analytic_cutest(self):\\n+        \\\"\\\"\\\"\\n+        Test analytic hessian\\n+        \\\"\\\"\\\"\\n+        self.fitting_problem.format = \\\"cutest\\\"\\n+        hes = Analytic(self.cost_func, self.jacobian)\\n+        self.fitting_problem.hes = hes\\n+        eval_result = hes.eval_cost(params=self.params)\\n+        self.assertTrue(np.isclose(self.actual, eval_result).all())\\n+\\n+\\n+class TestFactory(TestCase):\\n+    \\\"\\\"\\\"\\n+    Tests for the Hessian factory\\n+    \\\"\\\"\\\"\\n+\\n+    def setUp(self):\\n+        self.options = Options()\\n+\\n+    def test_imports(self):\\n+        \\\"\\\"\\\"\\n+        Test that the factory returns the correct class for inputs\\n+        \\\"\\\"\\\"\\n+        valid = ['analytic']\\n+\\n+        invalid = ['random_hes']\\n+\\n+        for hes_method in valid:\\n+            hes = create_hessian(hes_method)\\n+            self.assertTrue(hes.__name__.lower().startswith(hes_method))\\n+\\n+        for hes_method in invalid:\\n+            self.assertRaises(exceptions.NoHessianError,\\n+                              create_hessian,\\n+                              hes_method)\\n\",\"diff --git a/fitbenchmarking/parsing/tests/nist/data_files/basic.hes b/fitbenchmarking/parsing/tests/nist/data_files/basic.hes\\nnew file mode 100644\\nindex 000000000..0c864eae6\\n--- /dev/null\\n+++ b/fitbenchmarking/parsing/tests/nist/data_files/basic.hes\\n@@ -0,0 +1,4 @@\\n+# hessian for test problem basic.dat\\n+\\n+H = [[0, p2*exp(-p2*x)],\\n+    [p2*exp(-p2*x), exp(-p2*x)*(p1-p1*p2*x)]]\\n\\\\ No newline at end of file\\n\",\"diff --git a/fitbenchmarking/parsing/tests/nist/hessian_evaluations.json b/fitbenchmarking/parsing/tests/nist/hessian_evaluations.json\\nnew file mode 100644\\nindex 000000000..3ae381184\\n--- /dev/null\\n+++ b/fitbenchmarking/parsing/tests/nist/hessian_evaluations.json\\n@@ -0,0 +1,10 @@\\n+{\\\"basic.dat\\\": [[[0.0, 1.0, 2.0], [1, 1],\\n+    [[[0.0, 0.0, 0.0],[1.0, 0.36787944, 0.13533528]],\\n+    [[1.0, 0.36787944, 0.13533528], [1.0, 0.0, -0.13533528]]],\\n+[1.0, [1, 1], [[0, 0.36787944117144233], [0.36787944117144233, 0.0]]],\\n+[2.0, [1, 1], [[0, 0.1353352832366127], [0.1353352832366127, -0.1353352832366127]]],\\n+[3.0, [1, 1], [[0, 0.049787068367863944], [0.049787068367863944, -0.09957413673572789]]],\\n+[2.1, [1, 1], [[0, 0.1224564282529819], [0.1224564282529819, -0.1347020710782801]]],\\n+[2.1, [20.6, 1], [[0, 0.1224564282529819], [0.1224564282529819, -2.7748626642125704]]],\\n+[2.1, [1, 0.1], [[0, 0.08105842459701872], [0.08105842459701872, 0.6403615543164478]]],\\n+[2.1, [12.8, 26.5], [[0, 1.7978686353778994e-23], [1.7978686353778994e-23, -4.74583044460207e-22]]]]]}\\n\\\\ No newline at end of file\\n\",\"diff --git a/fitbenchmarking/parsing/tests/test_parsers.py b/fitbenchmarking/parsing/tests/test_parsers.py\\nindex 99e6daa89..f49d2862b 100644\\n--- a/fitbenchmarking/parsing/tests/test_parsers.py\\n+++ b/fitbenchmarking/parsing/tests/test_parsers.py\\n@@ -19,6 +19,7 @@\\n \\n OPTIONS = Options()\\n JACOBIAN_ENABLED_PARSERS = ['cutest', 'nist']\\n+HESSIAN_ENABLED_PARSERS = ['nist']\\n BOUNDS_ENABLED_PARSERS = ['cutest', 'fitbenchmark']\\n \\n \\n@@ -50,7 +51,8 @@ def generate_test_cases():\\n     params = {'test_parsers': [],\\n               'test_factory': [],\\n               'test_function_evaluation': [],\\n-              'test_jacobian_evaluation': []}\\n+              'test_jacobian_evaluation': [],\\n+              'test_hessian_evaluation': []}\\n \\n     # get all parsers\\n     test_dir = os.path.dirname(__file__)\\n@@ -106,6 +108,14 @@ def generate_test_cases():\\n         test_jac_eval['evaluations_file'] = jac_eval\\n         params['test_jacobian_evaluation'].append(test_jac_eval)\\n \\n+        hes_eval = os.path.join(test_dir,\\n+                                file_format,\\n+                                'hessian_evaluations.json')\\n+        test_hes_eval = {}\\n+        test_hes_eval['file_format'] = file_format\\n+        test_hes_eval['evaluations_file'] = hes_eval\\n+        params['test_hessian_evaluation'].append(test_hes_eval)\\n+\\n     return params\\n \\n \\n@@ -218,6 +228,10 @@ def test_parsers(self, file_format, test_file, expected):\\n             # Check that the Jacobian is callable\\n             assert callable(fitting_problem.jacobian)\\n \\n+        if file_format in HESSIAN_ENABLED_PARSERS:\\n+            # Check that the Jacobian is callable\\n+            assert callable(fitting_problem.hessian)\\n+\\n     def test_function_evaluation(self, file_format, evaluations_file):\\n         \\\"\\\"\\\"\\n         Tests that the function evaluation is consistent with what would be\\n@@ -298,6 +312,47 @@ def test_jacobian_evaluation(self, file_format, evaluations_file):\\n                     actual = fitting_problem.jacobian(x, r[1])\\n                     assert np.isclose(actual, r[2]).all()\\n \\n+    def test_hessian_evaluation(self, file_format, evaluations_file):\\n+        \\\"\\\"\\\"\\n+        Tests that the Hessian evaluation is consistent with what would be\\n+        expected by comparing to some precomputed values with fixed params and\\n+        x values.\\n+\\n+        :param file_format: The name of the file format\\n+        :type file_format: string\\n+        :param evaluations_file: Path to a json file containing tests and\\n+                                 results\\n+                                 in the following format:\\n+                                 {\\\"test_file1\\\": [[x1, params1, results1],\\n+                                                 [x2, params2, results2],\\n+                                                  ...],\\n+                                  \\\"test_file2\\\": ...}\\n+        :type evaluations_file: string\\n+        \\\"\\\"\\\"\\n+        # Note that this test is optional so will only run if the file_format\\n+        # is added to the HESSIAN_ENABLED_PARSERS list.\\n+        if file_format in HESSIAN_ENABLED_PARSERS:\\n+            message = 'No function evaluations provided to test ' \\\\\\n+                'against for {}'.format(file_format)\\n+            assert (evaluations_file is not None), message\\n+\\n+            with open(evaluations_file, 'r') as ef:\\n+                results = load(ef)\\n+\\n+            format_dir = os.path.dirname(evaluations_file)\\n+\\n+            for f, tests in results.items():\\n+                f = os.path.join(format_dir, f)\\n+\\n+                parser = ParserFactory.create_parser(f)\\n+                with parser(f, OPTIONS) as p:\\n+                    fitting_problem = p.parse()\\n+\\n+                for r in tests:\\n+                    x = np.array(r[0])\\n+                    actual = fitting_problem.hessian(x, r[1])\\n+                    assert np.isclose(actual, r[2]).all()\\n+\\n     def test_factory(self, file_format, test_file):\\n         \\\"\\\"\\\"\\n         Tests that the factory selects the correct parser\\n\",\"diff --git a/fitbenchmarking/utils/options.py b/fitbenchmarking/utils/options.py\\nindex 3cf498396..852a3547f 100644\\n--- a/fitbenchmarking/utils/options.py\\n+++ b/fitbenchmarking/utils/options.py\\n@@ -63,6 +63,7 @@ class Options:\\n                       'matlab_stats', 'minuit', 'ralfit', 'scipy',\\n                       'scipy_ls', 'scipy_go'],\\n          'jac_method': ['scipy', 'analytic', 'default', 'numdifftools'],\\n+         'hes_method': ['default', 'analytic'],\\n          'cost_func_type': ['nlls', 'weighted_nlls', 'hellinger_nlls',\\n                             'poisson']}\\n     VALID_JACOBIAN = \\\\\\n@@ -126,6 +127,7 @@ class Options:\\n          'algorithm_type': ['all'],\\n          'software': ['scipy', 'scipy_ls'],\\n          'jac_method': ['scipy'],\\n+         'hes_method': ['default'],\\n          'cost_func_type': 'weighted_nlls'}\\n     DEFAULT_JACOBIAN = \\\\\\n         {'analytic': ['cutest'],\\n@@ -208,6 +210,7 @@ def __init__(self, file_name=None):\\n             fitting.getlist, 'algorithm_type')\\n         self.software = self.read_value(fitting.getlist, 'software')\\n         self.jac_method = self.read_value(fitting.getlist, 'jac_method')\\n+        self.hes_method = self.read_value(fitting.getlist, 'hes_method')\\n         self.cost_func_type = self.read_value(fitting.getstr, 'cost_func_type')\\n \\n         jacobian = config['JACOBIAN']\\n\",\"diff --git a/fitbenchmarking/utils/tests/test_options_fitting.py b/fitbenchmarking/utils/tests/test_options_fitting.py\\nindex 216974821..84e90a71c 100644\\n--- a/fitbenchmarking/utils/tests/test_options_fitting.py\\n+++ b/fitbenchmarking/utils/tests/test_options_fitting.py\\n@@ -53,6 +53,14 @@ def test_jac_method_default(self):\\n         actual = self.options.jac_method\\n         self.assertEqual(expected, actual)\\n \\n+    def test_hes_method_default(self):\\n+        \\\"\\\"\\\"\\n+        Checks hes_method default\\n+        \\\"\\\"\\\"\\n+        expected = ['default']\\n+        actual = self.options.hes_method\\n+        self.assertEqual(expected, actual)\\n+\\n     def test_cost_func_default(self):\\n         \\\"\\\"\\\"\\n         Checks cost_func default\\n@@ -188,6 +196,23 @@ def test_minimizer_jac_method_invalid(self):\\n             \\\"[FITTING]\\\\njac_method: NumPyFD\\\"\\n         self.shared_invalid('jac_method', config_str)\\n \\n+    def test_minimizer_hes_method_valid(self):\\n+        \\\"\\\"\\\"\\n+        Checks user set hes_method is valid\\n+        \\\"\\\"\\\"\\n+        set_option = [\\\"analytic\\\"]\\n+        config_str = \\\\\\n+            \\\"[FITTING]\\\\nhes_method: analytic\\\"\\n+        self.shared_valid('hes_method', set_option, config_str)\\n+\\n+    def test_minimizer_hes_method_invalid(self):\\n+        \\\"\\\"\\\"\\n+        Checks user set hes_method is invalid\\n+        \\\"\\\"\\\"\\n+        config_str = \\\\\\n+            \\\"[FITTING]\\\\nhes_method: numpy\\\"\\n+        self.shared_invalid('hes_method', config_str)\\n+\\n     def test_minimizer_cost_func_type_valid(self):\\n         \\\"\\\"\\\"\\n         Checks user set cost_func_type is valid\"]", "hints_text": ""}
