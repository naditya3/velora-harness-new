{"instance_id": "1015176567056626", "repo": "qazalbash/jaxampler", "base_commit": "d79974d49c38fa3f03ce7efc15b3099a7c6cfaed", "problem_statement": "Expression Tree to evaluate the algebraic combination of `rvs`:\\n# Description\\r\\nAs per the title.", "FAIL_TO_PASS": ["tests/rvs_test.py::TestRandomVariable::test_adding_rvs", "tests/rvs_test.py::TestRandomVariable::test_dividing_number", "tests/rvs_test.py::TestRandomVariable::test_negating_rvs", "tests/rvs_test.py::TestRandomVariable::test_powering_rvs", "tests/rvs_test.py::TestRandomVariable::test_dividing_rvs", "tests/rvs_test.py::TestRandomVariable::test_subtracting_number", "tests/rvs_test.py::TestRandomVariable::test_powering_number", "tests/rvs_test.py::TestRandomVariable::test_adding_number", "tests/rvs_test.py::TestRandomVariable::test_subtracting_rvs", "tests/rvs_test.py::TestRandomVariable::test_multiplying_rvs", "tests/rvs_test.py::TestRandomVariable::test_multiplying_number"], "PASS_TO_PASS": ["tests/unifrom_test.py::TestUniform::test_low_is_negative", "tests/rayleigh_test.py::TestRayleigh::test_incompatible_shapes", "tests/pareto_test.py::TestPareto::test_cdf", "tests/geometric_test.py::TestGeometric::test_small_p", "tests/utils_test.py::TestUtils::test_jx_cast_fail", "tests/triangular_test.py::TestTriangular::test_x_is_greater_than_mid", "tests/triangular_test.py::TestTriangular::test_cdf", "tests/truncated_power_law_test.py::TestTruncPowerLaw::test_shapes", "tests/unifrom_test.py::TestUniform::test_high_is_greater_than_low", "tests/binomial_test.py::TestBinomial::test_p_out_of_range", "tests/weibull_test.py::TestWeibull::test_cdf", "tests/triangular_test.py::TestTriangular::test_x_is_greater_than_high", "tests/unifrom_test.py::TestUniform::test_rvs", "tests/utils_test.py::TestUtils::test_nPr_not_exist", "tests/triangular_test.py::TestTriangular::test_x_is_less_than_mid", "tests/binomial_test.py::TestBinomial::test_large_n", "tests/weibull_test.py::TestWeibull::test_negative_lambda", "tests/pareto_test.py::TestPareto::test_rvs", "tests/triangular_test.py::TestTriangular::test_rvs", "tests/studentt_test.py::TestStudentT::test_invalid_scale", "tests/truncated_power_law_test.py::TestTruncPowerLaw::test_incompatible_shapes", "tests/triangular_test.py::TestTriangular::test_mid_is_greater_than_high", "tests/pareto_test.py::TestPareto::test_pdf", "tests/truncated_power_law_test.py::TestTruncPowerLaw::test_rvs", "tests/truncated_power_law_test.py::TestTruncPowerLaw::test_pdf", "tests/binomial_test.py::TestBinomial::test_incompatible_shapes", "tests/pareto_test.py::TestPareto::test_imcompatible_shapes", "tests/exponential_test.py::TestExponential::test_cdf", "tests/unifrom_test.py::TestUniform::test_low_is_equal_to_high", "tests/exponential_test.py::TestExponential::test_ppf", "tests/weibull_test.py::TestWeibull::test_negative_k", "tests/boltzmann_test.py::TestBoltzmann::test_cdf", "tests/geometric_test.py::TestGeometric::test_big_n", "tests/utils_test.py::TestUtils::test_jx_cast_success", "tests/triangular_test.py::TestTriangular::test_low_is_greater_than_mid", "tests/geometric_test.py::TestGeometric::test_shapes", "tests/utils_test.py::TestUtils::test_nCr_exist", "tests/weibull_test.py::TestWeibull::test_negative_lambda_and_k", "tests/geometric_test.py::TestGeometric::test_cdf", "tests/geometric_test.py::TestGeometric::test_rvs", "tests/studentt_test.py::TestStudentT::test_cdf", "tests/geometric_test.py::TestGeometric::test_positive_p", "tests/geometric_test.py::TestGeometric::test_p_greater_than_1", "tests/rayleigh_test.py::TestRayleigh::test_out_of_bound", "tests/weibull_test.py::TestWeibull::test_negative", "tests/rayleigh_test.py::TestRayleigh::test_cdf", "tests/binomial_test.py::TestBinomial::test_rvs", "tests/studentt_test.py::TestStudentT::test_invalid_df", "tests/triangular_test.py::TestTriangular::test_low_is_equal_to_high", "tests/rayleigh_test.py::TestRayleigh::test_shapes", "tests/triangular_test.py::TestTriangular::test_x_is_equal_to_mid", "tests/lognormal_test.py::TestLogNormal::test_invalid_params", "tests/binomial_test.py::TestBinomial::test_small_p", "tests/exponential_test.py::TestExponential::test_valid_params", "tests/weibull_test.py::TestWeibull::test_pdf", "tests/utils_test.py::TestUtils::test_incompatible_shapes", "tests/exponential_test.py::TestExponential::test_invalid_params", "tests/pareto_test.py::TestPareto::test_out_of_bound", "tests/unifrom_test.py::TestUniform::test_low_and_high_are_negative", "tests/rayleigh_test.py::TestRayleigh::test_rvs", "tests/unifrom_test.py::TestUniform::test_shape", "tests/triangular_test.py::TestTriangular::test_low_is_negative", "tests/truncated_power_law_test.py::TestTruncPowerLaw::test_cdf", "tests/binomial_test.py::TestBinomial::test_all_positive", "tests/triangular_test.py::TestTriangular::test_low_and_high_are_negative", "tests/weibull_test.py::TestWeibull::test_ppf", "tests/boltzmann_test.py::TestBoltzmann::test_logpdf", "tests/triangular_test.py::TestTriangular::test_x_is_less_than_low", "tests/utils_test.py::TestUtils::test_nCr_not_exist", "tests/weibull_test.py::TestWeibull::test_shapes", "tests/rayleigh_test.py::TestRayleigh::test_pdf", "tests/unifrom_test.py::TestUniform::test_cdf", "tests/boltzmann_test.py::TestBoltzmann::test_invalid_params", "tests/truncated_power_law_test.py::TestTruncPowerLaw::test_out_of_bound", "tests/weibull_test.py::TestWeibull::test_rvs", "tests/pareto_test.py::TestPareto::test_shapes", "tests/binomial_test.py::TestBinomial::test_shapes", "tests/binomial_test.py::TestBinomial::test_cdf", "tests/utils_test.py::TestUtils::test_nPr_exist"], "language": "python", "test_command": "source /saved/ENV || source /saved/*/ENV && pytest --no-header -rA --tb=no -p no:cacheprovider --continue-on-collection-errors", "test_output_parser": "python/parse_log_pytest_v3", "image_storage_uri": "vmvm-registry.fbinfra.net/repomate_image_activ_pytest/qazalbash_jaxampler:d79974d49c38fa3f03ce7efc15b3099a7c6cfaed", "patch": "[\"diff --git a/jaxampler/_src/jobj.py b/jaxampler/_src/jobj.py\\nindex e5c9a23..5c0ae33 100644\\n--- a/jaxampler/_src/jobj.py\\n+++ b/jaxampler/_src/jobj.py\\n@@ -27,6 +27,10 @@ class JObj(object):\\n     def __init__(self, name: Optional[str] = None) -> None:\\n         self._name = name\\n \\n+    @property\\n+    def name(self):\\n+        return self._name\\n+\\n     @staticmethod\\n     def get_key(key: Optional[Array] = None) -> Array:\\n         \\\"\\\"\\\"Get a new JAX random key.\\n\",\"diff --git a/jaxampler/_src/montecarlo/montecarlogeneric.py b/jaxampler/_src/montecarlo/montecarlogeneric.py\\nindex 72ed668..5382543 100644\\n--- a/jaxampler/_src/montecarlo/montecarlogeneric.py\\n+++ b/jaxampler/_src/montecarlo/montecarlogeneric.py\\n@@ -20,7 +20,7 @@\\n \\n from ..rvs.rvs import RandomVariable\\n from ..typing import Numeric\\n-from ..utils import jx_cast\\n+from ..utils import jxam_array_cast\\n from .integration import Integration\\n \\n \\n@@ -76,7 +76,7 @@ def compute_integral(self, *args, **kwargs) -> Array:\\n         key: Optional[Array] = kwargs.get(\\\"key\\\", None)\\n         if key is None:\\n             key = self.get_key()\\n-        param_shape, low, high = jx_cast(low, high)\\n+        param_shape, low, high = jxam_array_cast(low, high)\\n         p_rv = p.rvs(shape=(N,) + param_shape, key=key)\\n         p_rv = p_rv[(p_rv >= low) & (p_rv <= high)]\\n         hx = vmap(h)(p_rv)\\n\",\"diff --git a/jaxampler/_src/rvs/beta.py b/jaxampler/_src/rvs/beta.py\\nindex 065b72b..fade8f4 100644\\n--- a/jaxampler/_src/rvs/beta.py\\n+++ b/jaxampler/_src/rvs/beta.py\\n@@ -23,7 +23,7 @@\\n from tensorflow_probability.substrates import jax as tfp\\n \\n from ..typing import Numeric\\n-from ..utils import jx_cast\\n+from ..utils import jxam_array_cast\\n from .rvs import RandomVariable\\n \\n \\n@@ -45,7 +45,7 @@ def __init__(\\n         scale: Numeric | Any = 1.0,\\n         name: Optional[str] = None,\\n     ) -> None:\\n-        shape, self._alpha, self._beta, self._loc, self._scale = jx_cast(alpha, beta, loc, scale)\\n+        shape, self._alpha, self._beta, self._loc, self._scale = jxam_array_cast(alpha, beta, loc, scale)\\n         self.check_params()\\n         super().__init__(name=name, shape=shape)\\n \\n\",\"diff --git a/jaxampler/_src/rvs/binomial.py b/jaxampler/_src/rvs/binomial.py\\nindex d3e5dfb..347c185 100644\\n--- a/jaxampler/_src/rvs/binomial.py\\n+++ b/jaxampler/_src/rvs/binomial.py\\n@@ -23,7 +23,7 @@\\n from jax.scipy.stats import binom as jax_binom\\n \\n from ..typing import Numeric\\n-from ..utils import jx_cast\\n+from ..utils import jxam_array_cast\\n from .rvs import RandomVariable\\n \\n \\n@@ -39,7 +39,7 @@ def __init__(self, p: Numeric | Any, n: Numeric | Any, name: Optional[str] = Non\\n         :param n: Number of trials\\n         :param name: Name of the random variable\\n         \\\"\\\"\\\"\\n-        shape, self._p, self._n = jx_cast(p, n)\\n+        shape, self._p, self._n = jxam_array_cast(p, n)\\n         self.check_params()\\n         self._q = 1.0 - self._p\\n         super().__init__(name=name, shape=shape)\\n\",\"diff --git a/jaxampler/_src/rvs/boltzmann.py b/jaxampler/_src/rvs/boltzmann.py\\nindex 707ea5d..5956c4a 100644\\n--- a/jaxampler/_src/rvs/boltzmann.py\\n+++ b/jaxampler/_src/rvs/boltzmann.py\\n@@ -21,13 +21,13 @@\\n from jax.scipy.special import erf\\n \\n from ..typing import Numeric\\n-from ..utils import jx_cast\\n+from ..utils import jxam_array_cast\\n from .rvs import RandomVariable\\n \\n \\n class Boltzmann(RandomVariable):\\n     def __init__(self, a: Numeric | Any, name: Optional[str] = None) -> None:\\n-        shape, self._a = jx_cast(a)\\n+        shape, self._a = jxam_array_cast(a)\\n         self.check_params()\\n         super().__init__(name=name, shape=shape)\\n \\n\",\"diff --git a/jaxampler/_src/rvs/cauchy.py b/jaxampler/_src/rvs/cauchy.py\\nindex 1920887..b35fab8 100644\\n--- a/jaxampler/_src/rvs/cauchy.py\\n+++ b/jaxampler/_src/rvs/cauchy.py\\n@@ -22,13 +22,13 @@\\n from jax.scipy.stats import cauchy as jax_cauchy\\n \\n from ..typing import Numeric\\n-from ..utils import jx_cast\\n+from ..utils import jxam_array_cast\\n from .rvs import RandomVariable\\n \\n \\n class Cauchy(RandomVariable):\\n     def __init__(self, loc: Numeric | Any = 0, scale: Numeric | Any = 1.0, name: Optional[str] = None) -> None:\\n-        shape, self._loc, self._scale = jx_cast(loc, scale)\\n+        shape, self._loc, self._scale = jxam_array_cast(loc, scale)\\n         self.check_params()\\n         super().__init__(name=name, shape=shape)\\n \\n\",\"diff --git a/jaxampler/_src/rvs/chi2.py b/jaxampler/_src/rvs/chi2.py\\nindex 69f7dec..e7c5fbc 100644\\n--- a/jaxampler/_src/rvs/chi2.py\\n+++ b/jaxampler/_src/rvs/chi2.py\\n@@ -22,7 +22,7 @@\\n from jax.scipy.stats import chi2 as jax_chi2\\n \\n from ..typing import Numeric\\n-from ..utils import jx_cast\\n+from ..utils import jxam_array_cast\\n from .rvs import RandomVariable\\n \\n \\n@@ -34,7 +34,7 @@ def __init__(\\n         scale: Numeric | Any = 1.0,\\n         name: Optional[str] = None,\\n     ) -> None:\\n-        shape, self._nu, self._loc, self._scale = jx_cast(nu, loc, scale)\\n+        shape, self._nu, self._loc, self._scale = jxam_array_cast(nu, loc, scale)\\n         self.check_params()\\n         super().__init__(name=name, shape=shape)\\n \\n\",\"diff --git a/jaxampler/_src/rvs/exponential.py b/jaxampler/_src/rvs/exponential.py\\nindex 28ebe30..f53a0f1 100644\\n--- a/jaxampler/_src/rvs/exponential.py\\n+++ b/jaxampler/_src/rvs/exponential.py\\n@@ -22,7 +22,7 @@\\n from jax.scipy.stats import expon as jax_expon\\n \\n from ..typing import Numeric\\n-from ..utils import jx_cast\\n+from ..utils import jxam_array_cast\\n from .rvs import RandomVariable\\n \\n \\n@@ -33,7 +33,7 @@ def __init__(\\n         scale: Numeric | Any = 1.0,\\n         name: Optional[str] = None,\\n     ) -> None:\\n-        shape, self._loc, self._scale = jx_cast(loc, scale)\\n+        shape, self._loc, self._scale = jxam_array_cast(loc, scale)\\n         self.check_params()\\n         super().__init__(name=name, shape=shape)\\n \\n\",\"diff --git a/jaxampler/_src/rvs/gamma.py b/jaxampler/_src/rvs/gamma.py\\nindex f7af258..10d0df6 100644\\n--- a/jaxampler/_src/rvs/gamma.py\\n+++ b/jaxampler/_src/rvs/gamma.py\\n@@ -22,7 +22,7 @@\\n from jax.scipy.stats import gamma as jax_gamma\\n \\n from ..typing import Numeric\\n-from ..utils import jx_cast\\n+from ..utils import jxam_array_cast\\n from .rvs import RandomVariable\\n \\n \\n@@ -34,7 +34,7 @@ def __init__(\\n         scale: Numeric | Any = 1.0,\\n         name: Optional[str] = None,\\n     ) -> None:\\n-        shape, self._a, self._loc, self._scale = jx_cast(a, loc, scale)\\n+        shape, self._a, self._loc, self._scale = jxam_array_cast(a, loc, scale)\\n         self.check_params()\\n         super().__init__(name=name, shape=shape)\\n \\n\",\"diff --git a/jaxampler/_src/rvs/geometric.py b/jaxampler/_src/rvs/geometric.py\\nindex fb61bde..8a27998 100644\\n--- a/jaxampler/_src/rvs/geometric.py\\n+++ b/jaxampler/_src/rvs/geometric.py\\n@@ -22,7 +22,7 @@\\n from jax.scipy.stats import geom as jax_geom\\n \\n from ..typing import Numeric\\n-from ..utils import jx_cast\\n+from ..utils import jxam_array_cast\\n from .rvs import RandomVariable\\n \\n \\n@@ -30,7 +30,7 @@ class Geometric(RandomVariable):\\n     \\\"\\\"\\\"Geometric Random Variable\\\"\\\"\\\"\\n \\n     def __init__(self, p: Numeric | Any, loc: Numeric | Any = 0.0, name: Optional[str] = None) -> None:\\n-        shape, self._p, self._loc = jx_cast(p, loc)\\n+        shape, self._p, self._loc = jxam_array_cast(p, loc)\\n         self.check_params()\\n         self._q = 1.0 - self._p\\n         super().__init__(name=name, shape=shape)\\n\",\"diff --git a/jaxampler/_src/rvs/logistic.py b/jaxampler/_src/rvs/logistic.py\\nindex 2331854..5dd77f2 100644\\n--- a/jaxampler/_src/rvs/logistic.py\\n+++ b/jaxampler/_src/rvs/logistic.py\\n@@ -23,13 +23,13 @@\\n from jax.scipy.stats import logistic as jax_logistic\\n \\n from ..typing import Numeric\\n-from ..utils import jx_cast\\n+from ..utils import jxam_array_cast\\n from .rvs import RandomVariable\\n \\n \\n class Logistic(RandomVariable):\\n     def __init__(self, loc: Numeric | Any = 0.0, scale: Numeric | Any = 1.0, name: Optional[str] = None) -> None:\\n-        shape, self._loc, self._scale = jx_cast(loc, scale)\\n+        shape, self._loc, self._scale = jxam_array_cast(loc, scale)\\n         self.check_params()\\n         super().__init__(name=name, shape=shape)\\n \\n\",\"diff --git a/jaxampler/_src/rvs/lognormal.py b/jaxampler/_src/rvs/lognormal.py\\nindex 3b66ded..65c3284 100644\\n--- a/jaxampler/_src/rvs/lognormal.py\\n+++ b/jaxampler/_src/rvs/lognormal.py\\n@@ -22,13 +22,13 @@\\n from jax.scipy.special import log_ndtr, ndtr, ndtri\\n \\n from ..typing import Numeric\\n-from ..utils import jx_cast\\n+from ..utils import jxam_array_cast\\n from .rvs import RandomVariable\\n \\n \\n class LogNormal(RandomVariable):\\n     def __init__(self, loc: Numeric | Any = 0.0, scale: Numeric | Any = 1.0, name: Optional[str] = None) -> None:\\n-        shape, self._loc, self._scale = jx_cast(loc, scale)\\n+        shape, self._loc, self._scale = jxam_array_cast(loc, scale)\\n         self.check_params()\\n         super().__init__(name=name, shape=shape)\\n \\n\",\"diff --git a/jaxampler/_src/rvs/normal.py b/jaxampler/_src/rvs/normal.py\\nindex 1d16684..ef54b37 100644\\n--- a/jaxampler/_src/rvs/normal.py\\n+++ b/jaxampler/_src/rvs/normal.py\\n@@ -22,13 +22,13 @@\\n from jax.scipy.stats import norm as jax_norm\\n \\n from ..typing import Numeric\\n-from ..utils import jx_cast\\n+from ..utils import jxam_array_cast\\n from .rvs import RandomVariable\\n \\n \\n class Normal(RandomVariable):\\n     def __init__(self, loc: Numeric | Any = 0.0, scale: Numeric | Any = 1.0, name: Optional[str] = None) -> None:\\n-        shape, self._loc, self._scale = jx_cast(loc, scale)\\n+        shape, self._loc, self._scale = jxam_array_cast(loc, scale)\\n         self.check_params()\\n         self._logZ = 0.0\\n         super().__init__(name=name, shape=shape)\\n\",\"diff --git a/jaxampler/_src/rvs/pareto.py b/jaxampler/_src/rvs/pareto.py\\nindex 7d3a16c..5ac38f5 100644\\n--- a/jaxampler/_src/rvs/pareto.py\\n+++ b/jaxampler/_src/rvs/pareto.py\\n@@ -22,7 +22,7 @@\\n from jax.scipy.stats import pareto as jax_pareto\\n \\n from ..typing import Numeric\\n-from ..utils import jx_cast\\n+from ..utils import jxam_array_cast\\n from .rvs import RandomVariable\\n \\n \\n@@ -30,7 +30,7 @@ class Pareto(RandomVariable):\\n     def __init__(\\n         self, a: Numeric | Any, loc: Numeric | Any = 0.0, scale: Numeric | Any = 1.0, name: Optional[str] = None\\n     ) -> None:\\n-        shape, self._a, self._loc, self._scale = jx_cast(a, loc, scale)\\n+        shape, self._a, self._loc, self._scale = jxam_array_cast(a, loc, scale)\\n         self.check_params()\\n         super().__init__(name=name, shape=shape)\\n \\n\",\"diff --git a/jaxampler/_src/rvs/poisson.py b/jaxampler/_src/rvs/poisson.py\\nindex d248373..82c6ab4 100644\\n--- a/jaxampler/_src/rvs/poisson.py\\n+++ b/jaxampler/_src/rvs/poisson.py\\n@@ -22,13 +22,13 @@\\n from jax.scipy.stats import poisson as jax_poisson\\n \\n from ..typing import Numeric\\n-from ..utils import jx_cast\\n+from ..utils import jxam_array_cast\\n from .rvs import RandomVariable\\n \\n \\n class Poisson(RandomVariable):\\n     def __init__(self, mu: Numeric | Any, loc: Numeric | Any = 0.0, name: Optional[str] = None) -> None:\\n-        shape, self._mu, self._loc = jx_cast(mu, loc)\\n+        shape, self._mu, self._loc = jxam_array_cast(mu, loc)\\n         self.check_params()\\n         super().__init__(name=name, shape=shape)\\n \\n\",\"diff --git a/jaxampler/_src/rvs/rayleigh.py b/jaxampler/_src/rvs/rayleigh.py\\nindex 2cfb00b..5f732f9 100644\\n--- a/jaxampler/_src/rvs/rayleigh.py\\n+++ b/jaxampler/_src/rvs/rayleigh.py\\n@@ -21,13 +21,13 @@\\n from jax import Array, jit, numpy as jnp\\n \\n from ..typing import Numeric\\n-from ..utils import jx_cast\\n+from ..utils import jxam_array_cast\\n from .rvs import RandomVariable\\n \\n \\n class Rayleigh(RandomVariable):\\n     def __init__(self, loc: Numeric | Any = 0.0, sigma: Numeric | Any = 1.0, name: Optional[str] = None) -> None:\\n-        shape, self._loc, self._sigma = jx_cast(loc, sigma)\\n+        shape, self._loc, self._sigma = jxam_array_cast(loc, sigma)\\n         self.check_params()\\n         super().__init__(name=name, shape=shape)\\n \\n\",\"diff --git a/jaxampler/_src/rvs/rvs.py b/jaxampler/_src/rvs/rvs.py\\nindex 1508ebc..3b7fc8d 100644\\n--- a/jaxampler/_src/rvs/rvs.py\\n+++ b/jaxampler/_src/rvs/rvs.py\\n@@ -15,22 +15,22 @@\\n from __future__ import annotations\\n \\n from functools import partial\\n-from typing import Callable, Optional\\n+from typing_extensions import Any, Callable, Optional\\n \\n-import numpy as np\\n-from jax import jit, lax, numpy as jnp, vmap\\n-from jax._src import core\\n+from jax import jit, numpy as jnp, vmap\\n from jaxtyping import Array\\n \\n from ..jobj import JObj\\n from ..typing import Numeric\\n+from ..utils import jxam_shape_cast\\n \\n \\n class RandomVariable(JObj):\\n-    \\\"\\\"\\\"Generic random variable class.\\\"\\\"\\\"\\n+    \\\"\\\"\\\"Random variable class.\\\"\\\"\\\"\\n \\n     def __init__(self, name: Optional[str] = None, shape: tuple[int, ...] = ()) -> None:\\n         self._shape = shape\\n+        self._stack = []\\n         super().__init__(name=name)\\n \\n     def check_params(self) -> None:\\n@@ -39,17 +39,33 @@ def check_params(self) -> None:\\n     # POINT VALUED\\n \\n     @partial(jit, static_argnums=(0,))\\n-    def _logcdf_x(self, *x: Numeric) -> Numeric:\\n+    def _logpmf_x(self, *x: Numeric) -> Numeric:\\n         raise NotImplementedError\\n \\n     @partial(jit, static_argnums=(0,))\\n-    def _cdf_x(self, *x: Numeric) -> Numeric:\\n-        return jnp.exp(self._logcdf_x(*x))\\n+    def _logpdf_x(self, *x: Numeric) -> Numeric:\\n+        raise NotImplementedError\\n+\\n+    @partial(jit, static_argnums=(0,))\\n+    def _logcdf_x(self, *x: Numeric) -> Numeric:\\n+        raise NotImplementedError\\n \\n     @partial(jit, static_argnums=(0,))\\n     def _logppf_x(self, *x: Numeric) -> Numeric:\\n         raise NotImplementedError\\n \\n+    @partial(jit, static_argnums=(0,))\\n+    def _pmf_x(self, *x: Numeric) -> Numeric:\\n+        return jnp.exp(self._logpmf_x(*x))\\n+\\n+    @partial(jit, static_argnums=(0,))\\n+    def _pdf_x(self, *x: Numeric) -> Numeric:\\n+        return jnp.exp(self._logpdf_x(*x))\\n+\\n+    @partial(jit, static_argnums=(0,))\\n+    def _cdf_x(self, *x: Numeric) -> Numeric:\\n+        return jnp.exp(self._logcdf_x(*x))\\n+\\n     @partial(jit, static_argnums=(0,))\\n     def _ppf_x(self, *x: Numeric) -> Numeric:\\n         return jnp.exp(self._logppf_x(*x))\\n@@ -57,120 +73,195 @@ def _ppf_x(self, *x: Numeric) -> Numeric:\\n     # VECTOR VALUED\\n \\n     @partial(jit, static_argnums=(0,))\\n-    def _logcdf_v(self, *x: Numeric) -> Numeric:\\n-        return vmap(self._logcdf_x, in_axes=0)(*x)\\n+    def _logpmf_v(self, *x: Numeric) -> Numeric:\\n+        return vmap(self._logpmf_x, in_axes=0)(*x)\\n \\n     @partial(jit, static_argnums=(0,))\\n-    def _cdf_v(self, *x: Numeric) -> Numeric:\\n-        return jnp.exp(self._logcdf_v(*x))\\n+    def _logpdf_v(self, *x: Numeric) -> Numeric:\\n+        return vmap(self._logpdf_x, in_axes=0)(*x)\\n \\n     @partial(jit, static_argnums=(0,))\\n-    def _logppf_v(self, *x: Numeric) -> Numeric:\\n-        return vmap(self._logppf_x, in_axes=0)(*x)\\n+    def _logcdf_v(self, *x: Numeric) -> Numeric:\\n+        return vmap(self._logcdf_x, in_axes=0)(*x)\\n \\n     @partial(jit, static_argnums=(0,))\\n-    def _ppf_v(self, *x: Numeric) -> Numeric:\\n-        return jnp.exp(self._logppf_v(*x))\\n-\\n-    # XXF FACTORY METHODS\\n-    @staticmethod\\n-    def _pv_factory(\\n-        func_p: Callable[..., Numeric],\\n-        func_v: Callable[..., Numeric],\\n-        *x: Numeric,\\n-    ) -> Numeric:\\n-        # partially taken from the implementation of `jnp.broadcast_arrays`\\n-        shapes = [np.shape(arg) for arg in x]\\n-        if not shapes or all(core.definitely_equal_shape(shapes[0], s) for s in shapes):\\n-            shape = shapes[0]\\n-        else:\\n-            shape: tuple[int, ...] = lax.broadcast_shapes(*shapes)\\n-\\n-        if len(shape) < 2:\\n-            return func_p(*x)\\n-        return func_v(*x)\\n+    def _logppf_v(self, *x: Numeric) -> Numeric:\\n+        return vmap(self._logppf_x, in_axes=0)(*x)\\n \\n     @partial(jit, static_argnums=(0,))\\n-    def logcdf(self, *x: Numeric) -> Numeric:\\n-        return self._pv_factory(self._logcdf_x, self._logcdf_v, *x)\\n+    def _pmf_v(self, *x: Numeric) -> Numeric:\\n+        return jnp.exp(self._logpmf_v(*x))\\n \\n     @partial(jit, static_argnums=(0,))\\n-    def cdf(self, *x: Numeric) -> Numeric:\\n-        return self._pv_factory(self._cdf_x, self._cdf_v, *x)\\n+    def _cdf_v(self, *x: Numeric) -> Numeric:\\n+        return jnp.exp(self._logcdf_v(*x))\\n \\n     @partial(jit, static_argnums=(0,))\\n-    def logppf(self, *x: Numeric) -> Numeric:\\n-        return self._pv_factory(self._logppf_x, self._logppf_v, *x)\\n+    def _pdf_v(self, *x: Numeric) -> Numeric:\\n+        return jnp.exp(self._logpdf_v(*x))\\n \\n     @partial(jit, static_argnums=(0,))\\n-    def ppf(self, *x: Numeric) -> Numeric:\\n-        return self._pv_factory(self._ppf_x, self._ppf_v, *x)\\n-\\n-    def rvs(self, shape: tuple[int, ...], key: Optional[Array] = None) -> Array:\\n-        if key is None:\\n-            key = self.get_key()\\n-        new_shape = shape + self._shape\\n-        return self._rvs(shape=new_shape, key=key)\\n+    def _ppf_v(self, *x: Numeric) -> Numeric:\\n+        return jnp.exp(self._logppf_v(*x))\\n \\n     def _rvs(self, shape: tuple[int, ...], key: Array) -> Array:\\n         raise NotImplementedError\\n \\n-    # POINT VALUED\\n-\\n-    @partial(jit, static_argnums=(0,))\\n-    def _logpdf_x(self, *x: Numeric) -> Numeric:\\n-        raise NotImplementedError\\n+    # XXF FACTORY METHODS\\n \\n-    @partial(jit, static_argnums=(0,))\\n-    def _pdf_x(self, *x: Numeric) -> Numeric:\\n-        return jnp.exp(self._logpdf_x(*x))\\n+    def _pv_factory(\\n+        self,\\n+        func_p_repr: Callable[[RandomVariable], Callable[[Numeric], Numeric]],\\n+        func_v_repr: Callable[[RandomVariable], Callable[[Numeric], Numeric]],\\n+        shape: tuple[int, ...],\\n+    ) -> Callable:\\n+        if len(shape) < 2:\\n+            fn = func_p_repr\\n+        else:\\n+            fn = func_v_repr\\n \\n-    # VECTOR VALUED\\n+        if len(self._stack) == 0:\\n+            return lambda *args: fn(self)(*args)\\n+        return lambda *args: self._evaulate(fn, *args)\\n \\n     @partial(jit, static_argnums=(0,))\\n-    def _logpdf_v(self, *x: Numeric) -> Numeric:\\n-        return vmap(self._logpdf_x, in_axes=0)(*x)\\n+    def pmf(self, *x: Numeric) -> Numeric:\\n+        shape = jxam_shape_cast(*x)\\n+        fn = self._pv_factory(lambda x: x._pmf_x, lambda x: x._pmf_v, shape)\\n+        return fn(*x)\\n \\n     @partial(jit, static_argnums=(0,))\\n-    def _pdf_v(self, *x: Numeric) -> Numeric:\\n-        return jnp.exp(self._logpdf_v(*x))\\n-\\n-    # FACTORY METHODS\\n+    def pdf(self, *x: Numeric) -> Numeric:\\n+        shape = jxam_shape_cast(*x)\\n+        fn = self._pv_factory(lambda x: x._pdf_x, lambda x: x._pdf_v, shape)\\n+        return fn(*x)\\n \\n     @partial(jit, static_argnums=(0,))\\n-    def logpdf(self, *x: Numeric) -> Numeric:\\n-        return self._pv_factory(self._logpdf_x, self._logpdf_v, *x)\\n+    def cdf(self, *x: Numeric) -> Numeric:\\n+        shape = jxam_shape_cast(*x)\\n+        fn = self._pv_factory(lambda x: x._cdf_x, lambda x: x._cdf_v, shape)\\n+        return fn(*x)\\n \\n     @partial(jit, static_argnums=(0,))\\n-    def pdf(self, *x: Numeric) -> Numeric:\\n-        return self._pv_factory(self._pdf_x, self._pdf_v, *x)\\n-\\n-    # POINT VALUED\\n+    def ppf(self, *x: Numeric) -> Numeric:\\n+        shape = jxam_shape_cast(*x)\\n+        fn = self._pv_factory(lambda x: x._ppf_x, lambda x: x._ppf_v, shape)\\n+        return fn(*x)\\n \\n     @partial(jit, static_argnums=(0,))\\n-    def _logpmf_x(self, *k: Numeric) -> Numeric:\\n-        raise NotImplementedError\\n+    def logpmf(self, *x: Numeric) -> Numeric:\\n+        shape = jxam_shape_cast(*x)\\n+        fn = self._pv_factory(lambda x: x._logpmf_x, lambda x: x._logpmf_v, shape)\\n+        return fn(*x)\\n \\n     @partial(jit, static_argnums=(0,))\\n-    def _pmf_x(self, *k: Numeric) -> Numeric:\\n-        return jnp.exp(self._logpmf_x(*k))\\n-\\n-    # VECTOR VALUED\\n+    def logpdf(self, *x: Numeric) -> Numeric:\\n+        shape = jxam_shape_cast(*x)\\n+        fn = self._pv_factory(lambda x: x._logpdf_x, lambda x: x._logpdf_v, shape)\\n+        return fn(*x)\\n \\n     @partial(jit, static_argnums=(0,))\\n-    def _logpmf_v(self, *k: Numeric) -> Numeric:\\n-        return vmap(self._logpmf_x, in_axes=0)(*k)\\n+    def logcdf(self, *x: Numeric) -> Numeric:\\n+        shape = jxam_shape_cast(*x)\\n+        fn = self._pv_factory(lambda x: x._logcdf_x, lambda x: x._logcdf_v, shape)\\n+        return fn(*x)\\n \\n     @partial(jit, static_argnums=(0,))\\n-    def _pmf_v(self, *k: Numeric) -> Numeric:\\n-        return jnp.exp(self._logpmf_v(*k))\\n-\\n-    # FACTORY METHODS\\n+    def logppf(self, *x: Numeric) -> Numeric:\\n+        shape = jxam_shape_cast(*x)\\n+        fn = self._pv_factory(lambda x: x._logppf_x, lambda x: x._logppf_v, shape)\\n+        return fn(*x)\\n \\n-    @partial(jit, static_argnums=(0,))\\n-    def logpmf(self, *k: Numeric) -> Numeric:\\n-        return self._pv_factory(self._logpmf_x, self._logpmf_v, *k)\\n+    def rvs(self, shape: tuple[int, ...], key: Optional[Array] = None) -> Array:\\n+        if key is None:\\n+            key = self.get_key()\\n+        new_shape = shape + self._shape\\n+        return self._rvs(shape=new_shape, key=key)\\n \\n-    @partial(jit, static_argnums=(0,))\\n-    def pmf(self, *k: Numeric) -> Numeric:\\n-        return self._pv_factory(self._pmf_x, self._pmf_v, *k)\\n+    # postfix notation methods\\n+\\n+    def _add_expression(self, op: Callable, *args: Any) -> None:\\n+        stack: list[Any] = [op]\\n+        for arg in args:\\n+            if isinstance(arg, RandomVariable) and len(arg._stack) != 0:\\n+                stack.extend(arg._stack)\\n+            else:\\n+                stack.append(arg)\\n+        self._stack = stack\\n+\\n+    def _evaulate(self, func: Callable, *args, **kwargs) -> Any:\\n+        s = []\\n+        s_ = []\\n+        for i in range(len(self._stack)):\\n+            if isinstance(self._stack[i], RandomVariable):\\n+                s_.append(func(self._stack[i])(*args, **kwargs))\\n+            else:\\n+                s_.append(self._stack[i])\\n+        for item in s_[::-1]:\\n+            if isinstance(item, Callable):\\n+                s.append(item(*[s.pop() for _ in range(item.__code__.co_argcount)]))\\n+            else:\\n+                s.append(item)\\n+        return s[-1]\\n+\\n+    # arithmetic operations\\n+\\n+    def __add__(self, other) -> RandomVariable:\\n+        new_variable = RandomVariable(name=f\\\"({self.__repr__()} + {other.__repr__()})\\\")\\n+        new_variable._add_expression(lambda x, y: x + y, self, other)\\n+        return new_variable\\n+\\n+    def __sub__(self, other) -> RandomVariable:\\n+        new_variable = RandomVariable(name=f\\\"({self.__repr__()} - {other.__repr__()})\\\")\\n+        new_variable._add_expression(lambda x, y: x - y, self, other)\\n+        return new_variable\\n+\\n+    def __neg__(self):\\n+        new_variable = RandomVariable(name=f\\\"(-{self.__repr__()})\\\")\\n+        new_variable._add_expression(lambda x: -x, self)\\n+        return new_variable\\n+\\n+    def __mul__(self, other) -> RandomVariable:\\n+        new_variable = RandomVariable(name=f\\\"({self.__repr__()} * {other.__repr__()})\\\")\\n+        new_variable._add_expression(lambda x, y: x * y, self, other)\\n+        return new_variable\\n+\\n+    def __truediv__(self, other) -> RandomVariable:\\n+        new_variable = RandomVariable(name=f\\\"({self.__repr__()} / {other.__repr__()})\\\")\\n+        new_variable._add_expression(lambda x, y: x / y, self, other)\\n+        return new_variable\\n+\\n+    def __pow__(self, power, modulo=None) -> RandomVariable:\\n+        new_variable = RandomVariable(name=f\\\"({self.__repr__()}**{power})\\\")\\n+        new_variable._add_expression(lambda x, y: jnp.power(x, y), self, power)\\n+        return new_variable\\n+\\n+    # reverse arithmetic operations\\n+\\n+    def __radd__(self, other) -> RandomVariable:\\n+        new_variable = RandomVariable(name=f\\\"({other.__repr__()} + {self.__repr__()})\\\")\\n+        new_variable._add_expression(lambda x, y: x + y, other, self)\\n+        return new_variable\\n+\\n+    def __rsub__(self, other) -> RandomVariable:\\n+        new_variable = RandomVariable(name=f\\\"({other.__repr__()} - {self.__repr__()})\\\")\\n+        new_variable._add_expression(lambda x, y: x - y, other, self)\\n+        return new_variable\\n+\\n+    def __rmul__(self, other) -> RandomVariable:\\n+        new_variable = RandomVariable(name=f\\\"({other.__repr__()} * {self.__repr__()})\\\")\\n+        new_variable._add_expression(lambda x, y: x * y, other, self)\\n+        return new_variable\\n+\\n+    def __rtruediv__(self, other) -> RandomVariable:\\n+        new_variable = RandomVariable(name=f\\\"({other.__repr__()} / {self.__repr__()})\\\")\\n+        new_variable._add_expression(lambda x, y: x / y, other, self)\\n+        return new_variable\\n+\\n+    def __repr__(self) -> str:\\n+        if self._name is None:\\n+            return \\\"\\\"\\n+        return self._name\\n+\\n+    def __str__(self) -> str:\\n+        return self.__repr__()\\n\",\"diff --git a/jaxampler/_src/rvs/studentt.py b/jaxampler/_src/rvs/studentt.py\\nindex ee5615e..6ec80e5 100644\\n--- a/jaxampler/_src/rvs/studentt.py\\n+++ b/jaxampler/_src/rvs/studentt.py\\n@@ -23,7 +23,7 @@\\n from jax.scipy.stats import t as jax_t\\n \\n from ..typing import Numeric\\n-from ..utils import jx_cast\\n+from ..utils import jxam_array_cast\\n from .rvs import RandomVariable\\n \\n \\n@@ -35,7 +35,7 @@ def __init__(\\n         scale: Numeric | Any = 1.0,\\n         name: Optional[str] = None,\\n     ) -> None:\\n-        shape, self._df, self._loc, self._scale = jx_cast(df, loc, scale)\\n+        shape, self._df, self._loc, self._scale = jxam_array_cast(df, loc, scale)\\n         self.check_params()\\n         super().__init__(name=name, shape=shape)\\n \\n\",\"diff --git a/jaxampler/_src/rvs/triangular.py b/jaxampler/_src/rvs/triangular.py\\nindex 5186c6d..cc80eef 100644\\n--- a/jaxampler/_src/rvs/triangular.py\\n+++ b/jaxampler/_src/rvs/triangular.py\\n@@ -21,7 +21,7 @@\\n from jax import Array, jit, lax, numpy as jnp\\n \\n from ..typing import Numeric\\n-from ..utils import jx_cast\\n+from ..utils import jxam_array_cast\\n from .rvs import RandomVariable\\n \\n \\n@@ -33,7 +33,7 @@ def __init__(\\n         high: Numeric | Any = 1,\\n         name: Optional[str] = None,\\n     ) -> None:\\n-        shape, self._low, self._mode, self._high = jx_cast(low, mode, high)\\n+        shape, self._low, self._mode, self._high = jxam_array_cast(low, mode, high)\\n         self.check_params()\\n         super().__init__(name=name, shape=shape)\\n \\n\",\"diff --git a/jaxampler/_src/rvs/truncnormal.py b/jaxampler/_src/rvs/truncnormal.py\\nindex 3e6bf9d..300d9f8 100644\\n--- a/jaxampler/_src/rvs/truncnormal.py\\n+++ b/jaxampler/_src/rvs/truncnormal.py\\n@@ -22,7 +22,7 @@\\n from jax.scipy.stats import truncnorm as jax_truncnorm\\n \\n from ..typing import Numeric\\n-from ..utils import jx_cast\\n+from ..utils import jxam_array_cast\\n from .rvs import RandomVariable\\n \\n \\n@@ -35,7 +35,7 @@ def __init__(\\n         high: Numeric | Any = 1.0,\\n         name: Optional[str] = None,\\n     ) -> None:\\n-        shape, self._loc, self._scale, self._low, self._high = jx_cast(loc, scale, low, high)\\n+        shape, self._loc, self._scale, self._low, self._high = jxam_array_cast(loc, scale, low, high)\\n         self.check_params()\\n         self._alpha = (self._low - self._loc) / self._scale\\n         self._beta = (self._high - self._loc) / self._scale\\n\",\"diff --git a/jaxampler/_src/rvs/truncpowerlaw.py b/jaxampler/_src/rvs/truncpowerlaw.py\\nindex 7d7f252..3c9157f 100644\\n--- a/jaxampler/_src/rvs/truncpowerlaw.py\\n+++ b/jaxampler/_src/rvs/truncpowerlaw.py\\n@@ -22,7 +22,7 @@\\n from jaxtyping import Array\\n \\n from ..typing import Numeric\\n-from ..utils import jx_cast\\n+from ..utils import jxam_array_cast\\n from .rvs import RandomVariable\\n \\n \\n@@ -34,7 +34,7 @@ def __init__(\\n         high: Numeric | Any,\\n         name: Optional[str] = None,\\n     ) -> None:\\n-        shape, self._alpha, self._low, self._high = jx_cast(alpha, low, high)\\n+        shape, self._alpha, self._low, self._high = jxam_array_cast(alpha, low, high)\\n         self.check_params()\\n         self._beta = 1.0 + self._alpha\\n         self._logZ = self.logZ()\\n\",\"diff --git a/jaxampler/_src/rvs/uniform.py b/jaxampler/_src/rvs/uniform.py\\nindex 3009958..18b5a2d 100644\\n--- a/jaxampler/_src/rvs/uniform.py\\n+++ b/jaxampler/_src/rvs/uniform.py\\n@@ -23,13 +23,13 @@\\n from jaxtyping import Array\\n \\n from ..typing import Numeric\\n-from ..utils import jx_cast\\n+from ..utils import jxam_array_cast\\n from .rvs import RandomVariable\\n \\n \\n class Uniform(RandomVariable):\\n     def __init__(self, low: Numeric | Any, high: Numeric | Any, name: Optional[str] = None) -> None:\\n-        shape, self._low, self._high = jx_cast(low, high)\\n+        shape, self._low, self._high = jxam_array_cast(low, high)\\n         self.check_params()\\n         super().__init__(name, shape)\\n \\n\",\"diff --git a/jaxampler/_src/rvs/weibull.py b/jaxampler/_src/rvs/weibull.py\\nindex 402f482..8d21c58 100644\\n--- a/jaxampler/_src/rvs/weibull.py\\n+++ b/jaxampler/_src/rvs/weibull.py\\n@@ -21,7 +21,7 @@\\n from jax import Array, jit, numpy as jnp\\n \\n from ..typing import Numeric\\n-from ..utils import jx_cast\\n+from ..utils import jxam_array_cast\\n from .rvs import RandomVariable\\n \\n \\n@@ -33,7 +33,7 @@ def __init__(\\n         scale: Numeric | Any = 1.0,\\n         name: Optional[str] = None,\\n     ) -> None:\\n-        shape, self._k, self._loc, self._scale = jx_cast(k, loc, scale)\\n+        shape, self._k, self._loc, self._scale = jxam_array_cast(k, loc, scale)\\n         self.check_params()\\n         super().__init__(name=name, shape=shape)\\n \\n\",\"diff --git a/jaxampler/_src/typing.py b/jaxampler/_src/typing.py\\nindex 2760194..2c96847 100644\\n--- a/jaxampler/_src/typing.py\\n+++ b/jaxampler/_src/typing.py\\n@@ -14,7 +14,7 @@\\n \\n from __future__ import annotations\\n \\n-from typing import Union\\n+from typing_extensions import Union\\n \\n import numpy as np\\n from jaxtyping import Array\\n\",\"diff --git a/jaxampler/_src/utils.py b/jaxampler/_src/utils.py\\nindex 8213b5f..471c6fc 100644\\n--- a/jaxampler/_src/utils.py\\n+++ b/jaxampler/_src/utils.py\\n@@ -14,8 +14,7 @@\\n \\n from __future__ import annotations\\n \\n-from typing import Any\\n-from typing_extensions import Unpack\\n+from typing_extensions import Any, Unpack\\n \\n import numpy as np\\n from jax import lax, numpy as jnp\\n@@ -23,7 +22,19 @@\\n from jaxtyping import Integer\\n \\n \\n-def jx_cast(\\n+def jxam_shape_cast(\\n+    *args: Any,\\n+) -> tuple[int, ...]:\\n+    # partially taken from the implementation of `jnp.broadcast_arrays`\\n+    shapes = [np.shape(arg) for arg in args]\\n+    if not shapes or all(core.definitely_equal_shape(shapes[0], s) for s in shapes):\\n+        result_shape = shapes[0]\\n+    else:\\n+        result_shape: tuple[int, ...] = lax.broadcast_shapes(*shapes)\\n+    return result_shape\\n+\\n+\\n+def jxam_array_cast(\\n     *args: Any,\\n ) -> tuple[tuple[int, ...], Unpack[tuple[Any, ...]]]:\\n     \\\"\\\"\\\"Cast provided arguments to `jnp.array` and checks if they can be\\n@@ -39,13 +50,7 @@ def jx_cast(\\n     list[Array]\\n         List of cast arguments.\\n     \\\"\\\"\\\"\\n-    # partially taken from the implementation of `jnp.broadcast_arrays`\\n-    shapes = [np.shape(arg) for arg in args]\\n-    if not shapes or all(core.definitely_equal_shape(shapes[0], s) for s in shapes):\\n-        result_shape = shapes[0]\\n-    else:\\n-        result_shape: tuple[int, ...] = lax.broadcast_shapes(*shapes)\\n-    return result_shape, *tuple(jnp.asarray(arg) for arg in args)\\n+    return jxam_shape_cast(*args), *tuple(jnp.asarray(arg) for arg in args)\\n \\n \\n fact = [1, 1, 2, 6, 24, 120, 720, 5_040, 40_320, 362_880, 3_628_800]\\n\",\"diff --git a/jaxampler/utils.py b/jaxampler/utils.py\\nindex d3ad686..a0ffd8e 100644\\n--- a/jaxampler/utils.py\\n+++ b/jaxampler/utils.py\\n@@ -14,4 +14,9 @@\\n \\n from __future__ import annotations\\n \\n-from jaxampler._src.utils import jx_cast as jx_cast, nCr as nCr, nPr as nPr\\n+from jaxampler._src.utils import (\\n+    jxam_array_cast as jxam_array_cast,\\n+    jxam_shape_cast as jxam_shape_cast,\\n+    nCr as nCr,\\n+    nPr as nPr,\\n+)\\n\"]", "test_patch": "[\"diff --git a/tests/rvs_test.py b/tests/rvs_test.py\\nnew file mode 100644\\nindex 0000000..06dd43b\\n--- /dev/null\\n+++ b/tests/rvs_test.py\\n@@ -0,0 +1,71 @@\\n+#  Copyright 2023 The Jaxampler Authors\\n+#\\n+#  Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\\n+#  you may not use this file except in compliance with the License.\\n+#  You may obtain a copy of the License at\\n+#\\n+#      http://www.apache.org/licenses/LICENSE-2.0\\n+#\\n+#  Unless required by applicable law or agreed to in writing, software\\n+#  distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\\n+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n+#  See the License for the specific language governing permissions and\\n+#  limitations under the License.\\n+\\n+\\n+import sys\\n+\\n+from jax import numpy as jnp\\n+\\n+sys.path.append(\\\"../jaxampler\\\")\\n+from jaxampler.rvs import Normal, Beta\\n+\\n+\\n+class TestRandomVariable:\\n+    beta = Beta(alpha=2, beta=2)\\n+    norms = [Normal(loc=i, scale=1) for i in jnp.linspace(-5, 5, 30)]\\n+    xx = jnp.linspace(-5, 5, 1000)\\n+\\n+    def test_adding_rvs(self):\\n+        Z = sum(self.norms)\\n+        assert jnp.allclose(Z.pdf(self.xx), sum(norm.pdf(self.xx) for norm in self.norms))\\n+\\n+    def test_adding_number(self):\\n+        Z = self.norms[0] + 2.6\\n+        assert jnp.allclose(Z.pdf(self.xx), self.norms[0].pdf(self.xx) + 2.6)\\n+\\n+    def test_subtracting_rvs(self):\\n+        Z = self.norms[0] - self.norms[1]\\n+        assert jnp.allclose(Z.pdf(self.xx), self.norms[0].pdf(self.xx) - self.norms[1].pdf(self.xx))\\n+\\n+    def test_subtracting_number(self):\\n+        Z = self.norms[0] - 2.6\\n+        assert jnp.allclose(Z.pdf(self.xx), self.norms[0].pdf(self.xx) - 2.6)\\n+\\n+    def test_multiplying_rvs(self):\\n+        Z = self.norms[0] * self.norms[1]\\n+        assert jnp.allclose(Z.pdf(self.xx), self.norms[0].pdf(self.xx) * self.norms[1].pdf(self.xx))\\n+\\n+    def test_multiplying_number(self):\\n+        Z = self.norms[0] * 2.6\\n+        assert jnp.allclose(Z.pdf(self.xx), self.norms[0].pdf(self.xx) * 2.6)\\n+\\n+    def test_dividing_rvs(self):\\n+        Z = self.norms[0] / self.norms[1]\\n+        assert jnp.allclose(Z.pdf(self.xx), self.norms[0].pdf(self.xx) / self.norms[1].pdf(self.xx))\\n+\\n+    def test_dividing_number(self):\\n+        Z = self.norms[0] / 2.6\\n+        assert jnp.allclose(Z.pdf(self.xx), self.norms[0].pdf(self.xx) / 2.6)\\n+\\n+    def test_powering_rvs(self):\\n+        Z = self.norms[0] ** self.norms[1]\\n+        assert jnp.allclose(Z.pdf(self.xx), self.norms[0].pdf(self.xx) ** self.norms[1].pdf(self.xx))\\n+\\n+    def test_powering_number(self):\\n+        Z = self.norms[0] ** 2.6\\n+        assert jnp.allclose(Z.pdf(self.xx), self.norms[0].pdf(self.xx) ** 2.6)\\n+\\n+    def test_negating_rvs(self):\\n+        Z = -self.norms[0]\\n+        assert jnp.allclose(Z.pdf(self.xx), -self.norms[0].pdf(self.xx))\\n\",\"diff --git a/tests/utils_test.py b/tests/utils_test.py\\nindex c8925c9..1b781b8 100644\\n--- a/tests/utils_test.py\\n+++ b/tests/utils_test.py\\n@@ -19,14 +19,14 @@\\n \\n \\n sys.path.append(\\\"../jaxampler\\\")\\n-from jaxampler.utils import jx_cast, nCr, nPr\\n+from jaxampler.utils import jxam_array_cast, nCr, nPr\\n \\n \\n class TestUtils:\\n     def test_jx_cast_success(self):\\n         a = 1\\n         b = [1, 2, 3]\\n-        shape, casted_a, casted_b = jx_cast(a, b)\\n+        shape, casted_a, casted_b = jxam_array_cast(a, b)\\n         assert shape == (3,)\\n         assert isinstance(casted_a, jnp.ndarray)\\n         assert isinstance(casted_b, jnp.ndarray)\\n@@ -39,11 +39,11 @@ def test_jx_cast_fail(self):\\n         g = jnp.array([[1, 2, 3], [4, 5, 6]])\\n         h = jnp.array([[1, 2]])\\n         with pytest.raises(ValueError):\\n-            jx_cast(g, h)\\n+            jxam_array_cast(g, h)\\n \\n     def test_incompatible_shapes(self):\\n         with pytest.raises(ValueError):\\n-            jx_cast([[0.3], 0.5], [0.5])\\n+            jxam_array_cast([[0.3], 0.5], [0.5])\\n \\n     def test_nPr_exist(self):\\n         assert nPr(5, 3) == 60\"]", "hints_text": ""}
