{"instance_id": "1303775021454963", "repo": "bamresearch/bayesianinference", "base_commit": "4ebc0084961da9c3921d7c064ca64f70023ce7df", "problem_statement": "\"impure\" ModelErrorInterface.__call__():\\nIn computer programming, a [pure function](https://en.wikipedia.org/wiki/Pure_function) is a function that (roughly) has no side-effects and, given the same arguments, has the same return value. The benefit is that those functions are easy to reason about, do not require much knowledge of their context and are easy to test. An example would be the obvious\\r\\n`error = model_error(parameter_list)`\\r\\n\\r\\nIn our implementation, however, we do\\r\\n~~~py\\r\\nmodel_error.parameter_list[...] = something...\\r\\nerror = model_error()\\r\\n~~~\\r\\n\\r\\n* You need to know that every model error has a member `parameter_list`\\r\\n* Another function, like `latent.update(...)` [here](https://github.com/BAMresearch/BayesianInference/blob/master/bayes/inference_problem.py#L57-L62), may actually change that list without you expecting/noticing it.\\r\\n\\r\\nVery unintuitive. \\r\\n\\r\\nI honestly do not know why I chose the impure version. One reason _could_ have been the distinction between the (implementation-wise) _completely different (!!!)_ methods `ModelErrorInterface.__call__` and `VariationalBayesInterface.__call__`. \\r\\n\\r\\nAnyways, I'll try to revert that to the pure version.", "FAIL_TO_PASS": ["tests/test_parameters.py::TestParameters::test_overwrite_one", "tests/test_parameters.py::TestParameters::test_overwrite"], "PASS_TO_PASS": ["tests/test_correlation.py::TestCorrelation::test_with_MNV", "tests/test_vb_stop.py::TestFreeEnergy::test_stopped_by_max_iter", "tests/test_correlation.py::TestCorrelation::test_uncorrelated", "tests/test_correlation.py::TestCorrelatedVB::test_demonstrate_solution", "tests/test_MVN.py::TestMVN::test_dim_mismatch", "tests/test_parameters.py::TestParameters::test_has", "tests/test_vb.py::Test_VB::test_vb_with_given_jac", "tests/test_vb_noises.py::TestTwoNoises::test_proper_noise", "tests/test_MVN.py::TestMVN::test_named_print", "tests/test_json.py::TestJSON::test_json", "tests/test_vb_stop.py::TestFreeEnergy::test_increase_in_free_energy_at_max_iter", "tests/test_vb_noises.py::TestTwoNoises::test_wrong_number_of_noises", "tests/test_MVN.py::TestMVN::test_named_access", "tests/test_json.py::TestJSON::test_vb_result", "tests/test_vb_noises.py::TestTwoNoises::test_noninformative_noise", "tests/test_vb_minimal.py::Test_VB::test_vector", "tests/test_noise.py::TestNoiseLoglike::test_loglike", "tests/test_parameters.py::TestParameters::test_define", "tests/test_noise.py::TestNoiseSingle::test_uncorrelated_noise", "tests/test_correlation.py::TestCorrelation::test_fully_correlated", "tests/test_parameters.py::TestParameters::test_concat", "tests/test_correlation.py::TestCorrelatedVB::test_demonstrate_issue", "tests/test_noise.py::TestNoiseSensor::test_uncorrelated_noise", "tests/test_parameters.py::TestParameters::test_copy", "tests/test_vb_minimal.py::Test_VB::test_dict", "tests/test_vb_stop.py::TestFreeEnergy::test_no_change_in_free_energy", "tests/test_vb.py::Test_VB::test_vb_with_numeric_jac", "tests/test_noise.py::TestNoiseJacobian::test_jacobian", "tests/test_correlation.py::TestCorrelation::test_invalid_2d", "tests/test_noise.py::TestNoiseTerms::test_uncorrelated_noise", "tests/test_vb_stop.py::TestFreeEnergy::test_reached_tolerance_return_stored", "tests/test_vb_ARD.py::Test_VB::test_vb_with_given_jac", "tests/test_parameters.py::TestParameters::test_iterate"], "language": "python", "test_command": "source /saved/ENV || source /saved/*/ENV && pytest --no-header -rA --tb=no -p no:cacheprovider --continue-on-collection-errors", "test_output_parser": "python/parse_log_pytest_v3", "image_storage_uri": "vmvm-registry.fbinfra.net/repomate_image_activ_pytest/bamresearch_bayesianinference:4ebc0084961da9c3921d7c064ca64f70023ce7df", "patch": "[\"diff --git a/bayes/latent.py b/bayes/latent.py\\nindex 47c7645..c62bd9b 100644\\n--- a/bayes/latent.py\\n+++ b/bayes/latent.py\\n@@ -1,248 +1,70 @@\\n-from collections import OrderedDict\\n-from operator import itemgetter\\n+import copy\\n+import collections\\n+from typing import Hashable\\n+import numpy as np\\n \\n-\\n-def len_or_one(vector_or_scalar):\\n-    \\\"\\\"\\\"\\n-    Returns the length of a vector or 1 for a scalar\\n-    \\\"\\\"\\\"\\n-    try:\\n-        return len(vector_or_scalar)\\n-    except TypeError:  # \\\"has no __len__\\\"\\n-        return 1\\n+from .parameters import ParameterList\\n \\n \\n class LatentParameter(list):\\n-    \\\"\\\"\\\"\\n-    Represents a single latent parameter that is mapped to one or more\\n-    individual (ParameterList, name) pairs stored in this list.\\n-    \\\"\\\"\\\"\\n-\\n-    def __init__(self, latent_parameters, name):\\n-        # we first need an instance of the global list to update the indices!\\n-        self._latent_parameters = latent_parameters\\n-        self._name = name  # for better RuntimeErrors\\n-\\n-        self.N = None  # number of (scalar) parameters of this\\n-        self.start_idx = None  # start index in the\\n-\\n-    def add(self, parameter_list, parameter_name):\\n-        \\\"\\\"\\\"\\n-        Adds `parameter_name` of `parameter_list` to this LatentParameter. So\\n-        if we change the value of this LatentParameter in `update`,\\n-        `parameter_name` of `parameter_list` will be changed as well.\\n-        \\\"\\\"\\\"\\n-        if parameter_name not in parameter_list:\\n-            raise RuntimeError(\\n-                f\\\"Parameter {parameter_name} is not part of {parameter_list},\\\"\\n-                f\\\"so it cannot be added to the latent variable {self._name}.\\\"\\n-            )\\n-\\n-        if self.has(parameter_list, parameter_name):\\n-            raise RuntimeError(\\n-                f\\\"Parameter {parameter_name} of {parameter_list} is already\\\"\\n-                f\\\"associated with latent parameter {self._name}!\\\"\\n-            )\\n-\\n-        N = len_or_one(parameter_list[parameter_name])\\n-\\n-        if self.N is None:\\n-            self.N = N\\n-            self._update_idx()\\n-        else:\\n-            if self.N != N:\\n-                raise RuntimeError(\\n-                    f\\\"The latent parameter {self._name} is defined with length\\\"\\n-                    f\\\"{self.N}. This does not match length {N} of parameter\\\"\\n-                    f\\\"of {parameter_name} in {parameter_list}!\\\"\\n-                )\\n-\\n-        self.append((parameter_list, parameter_name))\\n-\\n-    def _update_idx(self):\\n-        \\\"\\\"\\\"\\n-        As soon as a new latent parameter is added, the start indices of the\\n-        existing parameters must be updated. \\n-        \\\"\\\"\\\"\\n-        length = 0\\n-        for key, latent in self._latent_parameters.items():\\n-            latent.start_idx = length\\n-            length += latent.N\\n-\\n-    def global_index_range(self):\\n-        \\\"\\\"\\\"\\n-        Returns a list of global vector indices associated with this \\n-        LatentParameter.\\n-\\n-        Example:\\n-            The parameters \\\"A\\\" and \\\"B\\\" of parameter list\\n-                p[\\\"A\\\"] = [6., 1., 7., 4.]\\n-                p[\\\"B\\\"] = 42.\\n-            are added via `self.add`.\\n-\\n-            Thus, the _global vector_ of latent variables has length 5 and \\n-            could look like: [A, B, B, B, B].\\n-\\n-            This means:\\n-                latent[\\\"A\\\"].global_index_range() == [0, 1, 2, 3]\\n-                latent[\\\"B\\\"].global_index_range() == [4]\\n-\\n-        Remark:\\n-            At some point, e.g. for defining prior distributions for latent\\n-            parameters, the user can use this method to define individual \\n-            distributions like:\\n-\\n-            for latent_parameter in latent.values():\\n-                for index in latent_parameter.global_index_range():\\n-                    prior[index] = SomeScalarDistribution()\\n-\\n-            Alternatively, vector valued distributions could be defined like:\\n-\\n-            for latent_parameter in latent.values():\\n-                prior.append(MultivariateDistribution(length=latent_parameter.N))\\n-\\n-        \\\"\\\"\\\"\\n-        return list(range(self.start_idx, self.start_idx + self.N))\\n-\\n-    def update(self, number_vector):\\n-        \\\"\\\"\\\"\\n-        Extracts the `global_index_range` from `number_vector` and assigns it\\n-        to all (parameter_list,parameter_name) pairs in `self`.\\n-        \\\"\\\"\\\"\\n-        values = self.values(number_vector)\\n-        for parameter_list, parameter_name in self:\\n-            parameter_list[parameter_name] = values\\n-\\n-    def set_value(self, value):\\n-        \\\"\\\"\\\"\\n-        Updates the parameters associated to self in all parameter lists.\\n-        Checks, if the dimensions match.\\n-        \\\"\\\"\\\"\\n-        if self.N != len_or_one(value):\\n-            raise RuntimeError(\\n-                f\\\"Dimension mismatch: Global parameter '{self._name}' is of length {self.N} and you provided length {len_or_one(value)}!\\\"\\n-            )\\n-\\n-        for parameter_list, parameter_name in self:\\n-            parameter_list[parameter_name] = value\\n-\\n-    def values(self, number_vector):\\n-        return itemgetter(*self.global_index_range())(number_vector)\\n-\\n-    def has(self, parameter_list, parameter_name):\\n-        return (parameter_list, parameter_name) in self\\n-\\n-    def value(self, number_vector=None):\\n-        \\\"\\\"\\\"\\n-        Returns the value this latent parameter either from (one of) its \\n-        parameter list(s) or from `number_vector`, if provided.\\n-        \\\"\\\"\\\"\\n-        if number_vector is None:\\n-            if not self:  # = self is empty\\n-                raise RuntimeError(f\\\"There is no parameter associated to {self._name}!\\\")\\n-\\n-            parameter_list, parameter_name = self[0]\\n-            return parameter_list[parameter_name]\\n-\\n-        else:\\n-            return self.values(number_vector)\\n-\\n-    def unambiguous_value(self):\\n-        \\\"\\\"\\\"\\n-        Returns the value this latent parameter either from (one of) its \\n-        parameter list(s), checking that _all_ values are equal.\\n-        \\\"\\\"\\\"\\n-        p_list0, p_name0 = self[0]\\n-        value0 = p_list0[p_name0]\\n-        for p_list, p_name in self:\\n-            value = p_list[p_name]\\n-            if value != value0:\\n-                msg = f\\\"The values of the shared global parameter '{self._name}' differ in the individual parameter lists: \\\\n Parameter '{p_name0, value0}' in \\\\n{p_list0} vs. parameter '{p_name, value}' in \\\\n{p_list}\\\"\\n-                raise RuntimeError(msg)\\n-\\n-        return value0\\n-\\n-\\n-class LatentParameters(OrderedDict):\\n-    def update(self, number_vector):\\n-        n_parameters = self.vector_length\\n-        if n_parameters != len(number_vector):\\n-            raise RuntimeError(\\n-                f\\\"Dimension mismatch: There are {n_parameters} global parameters, but you provided {len(number_vector)}!\\\"\\n-            )\\n-\\n-        for latent in self.values():\\n-            latent.update(number_vector)\\n-\\n-    @property\\n-    def vector_length(self):\\n-        \\\"\\\"\\\"\\n-        Returns the total number of latent parameters, considering vector \\n-        valued ones.\\n-        \\\"\\\"\\\"\\n-        return sum(l.N for l in self.values())\\n-\\n-    def __missing__(self, latent_name):\\n-        \\\"\\\"\\\"\\n-        This method is called whenever you access a new latent parameter,\\n-        mainly resulting in an `DefaultDict`.\\n-\\n-        To update the global start indices when parameters are added, we pass\\n-        `self` - so the LatentParameters - to the newly created\\n-        LatentParameter.\\n-        \\\"\\\"\\\"\\n-        self[latent_name] = LatentParameter(self, latent_name)\\n-        return self[latent_name]\\n-\\n-    def __str__(self):\\n-        l_max = max(len(name) for name in self)\\n-        s = \\\"\\\"\\n-        for latent_name, latent in self.items():\\n-            s += f\\\"{latent_name:{l_max}} = {latent.value()}\\\\n\\\"\\n-        return s\\n-\\n-    def latent_names(self, parameter_list):\\n-        \\\"\\\"\\\"\\n-        Returns the latent parameter names of `parameter_list`.\\n-        \\\"\\\"\\\"\\n-        names = []\\n+    def __init__(self, N):\\n+        self.N = N\\n+\\n+    def add(self, model_error_key, local_name):\\n+        self.append((model_error_key, local_name))\\n+\\n+\\n+class InconsistentLengthException(Exception):\\n+    pass\\n+\\n+\\n+class LatentParameters(collections.OrderedDict):\\n+    def __init__(self):\\n+        super().__init__()\\n+        # This member is just for convenience/performance such that the\\n+        # `updated_parameters` do not have to build it again and can\\n+        # just copy it.\\n+        self._empty_parameter_lists: dict[Hashable, ParameterList] = {}\\n+\\n+    def add(\\n+        self, global_name: str, local_name: str, model_error_key: Hashable, N: int = 1\\n+    ) -> None:\\n+        self._empty_parameter_lists[model_error_key] = ParameterList()\\n+\\n+        if global_name not in self:\\n+            self[global_name] = LatentParameter(N)\\n+\\n+        latent = self[global_name]\\n+        if latent.N != N:\\n+            raise InconsistentLengthException(\\\"TODO: some info\\\")\\n+        latent.add(model_error_key, local_name)\\n+\\n+    def updated_parameters(\\n+        self, number_vector: np.ndarray\\n+    ) -> dict[Hashable, ParameterList]:\\n+\\n+        # maybe make that a member?\\n+        current_length = sum(l.N for l in self.values())\\n+\\n+        if current_length != len(number_vector):\\n+            msg = f\\\"These latent parameters have a total length of \\\"\\n+            msg += f\\\"{current_length}, you provided a vector of length \\\"\\n+            msg += f\\\"{len(number_vector)}\\\"\\n+            raise InconsistentLengthException(msg)\\n+\\n+        lists = copy.deepcopy(self._empty_parameter_lists)\\n+\\n+        start_index = 0\\n+\\n         for global_name, latent in self.items():\\n-            for (prm_list, local_name) in latent:\\n-                if prm_list == parameter_list:\\n-                    names.append((local_name, global_name))\\n-        return names\\n-\\n-    def get_vector(self, overwrite={}):\\n-        \\\"\\\"\\\"\\n-        Extracts a vector of global parameters from the individual parameter \\n-        lists. If a global parameters is inside the `overwrite`, that value\\n-        is used instead.\\n-\\n-        overwrite:\\n-            dict {global parameter name: parameter value}\\n-            Note that the dimension of the parameter value must match the\\n-            the dimension of the parameter.\\n-        \\\"\\\"\\\"\\n-        v = []\\n-        for name, latent in self.items():\\n-            if name in overwrite:\\n-                value = overwrite[name]\\n-                N_value = len_or_one(value)\\n-                if latent.N != N_value:\\n-                    raise RuntimeError(\\n-                        f\\\"Dimension mismatch: Global parameter '{name}' has length {latent.N}, you provided {value} of length {N_value}.\\\"\\n-                    )\\n-            else:\\n-                try:\\n-                    value = latent.unambiguous_value()\\n-                except RuntimeError as e:\\n-                    msg_add = f\\\"You can fix that adding the dict entry '{name}: your_value' to the optional 'overwrite' argument of this method.\\\"\\n-                    raise RuntimeError(str(e) + msg_add)\\n \\n             if latent.N == 1:\\n-                v.append(value)\\n+                value = number_vector[start_index]\\n             else:\\n-                for i in value:\\n-                    v.append(i)\\n+                value = number_vector[start_index : start_index + latent.N]\\n+\\n+            for model_error_key, local_name in latent:\\n+                lists[model_error_key].define(local_name, value)\\n+            start_index += latent.N\\n \\n-        return v\\n+        return lists\\n\",\"diff --git a/bayes/inference_problem.py b/bayes/inference_problem.py\\nindex e06677b..9457be4 100644\\n--- a/bayes/inference_problem.py\\n+++ b/bayes/inference_problem.py\\n@@ -1,19 +1,16 @@\\n import numpy as np\\n from .parameters import ParameterList\\n-from .latent import LatentParameters\\n+# from .latent import LatentParameters\\n from collections import OrderedDict\\n from .vb import MVN, Gamma, variational_bayes, VariationalBayesInterface\\n from .jacobian import d_model_error_d_named_parameter\\n+from typing import Hashable\\n \\n \\n class ModelErrorInterface:\\n-    def __init__(self):\\n-        self.parameter_list = ParameterList()\\n-\\n-    def __call__(self):\\n+    def __call__(self, latent_parameter_list: ParameterList) -> dict[Hashable: np.ndarray]: \\n         \\\"\\\"\\\"\\n-        Evaluate the model error based on `self.parameter_list` as a dict of\\n-        {some_key: numpy.array}.\\n+        Evaluate the model error based on the `latent_parameter_list`.\\n         \\\"\\\"\\\"\\n         raise NotImplementedError(\\\"Override this!\\\")\\n \\n@@ -30,10 +27,27 @@ def jacobian(self, latent_names=None):\\n \\n         return jac\\n \\n+    def get_length(self, parameter_name):\\n+        \\\"\\\"\\\"\\n+        Overwrite for more complex behaviours, e.g. to \\n+\\n+        ~~~py\\n+            if parameter_name == \\\"displacement_field\\\":\\n+                return len(self.u)\\n+            if parameter_name == \\\"force_vector\\\":\\n+                return 3\\n+            raise UnknownParameter()\\n+            # or\\n+            return 1\\n+        ~~~\\n+        \\\"\\\"\\\"\\n+        return 1\\n+\\n+\\n \\n class InferenceProblem:\\n     def __init__(self):\\n-        self.latent = LatentParameters()\\n+        self._latent = LatentParameters()\\n         self.model_errors = OrderedDict()  # key : model_error\\n         self._noise_models = OrderedDict()  # key : noise_model\\n \\n@@ -68,7 +82,17 @@ def __call__(self, number_vector):\\n             result[key] = me()\\n         return result\\n \\n-    def define_shared_latent_parameter_by_name(self, name):\\n+    def set_latent(self, global_and_local_name):\\n+        pass\\n+\\n+    def set_latent_individually(self, global_name, model_error_key, local_name):\\n+        pass\\n+\\n+\\n+    def define_latent_parameter(self, global_and_local_name):\\n+        \\\"\\\"\\\"\\n+\\n+        \\\"\\\"\\\"\\n         for model_error in self.model_errors.values():\\n             try:\\n                 prm = model_error.parameter_list\\n@@ -80,6 +104,7 @@ def define_shared_latent_parameter_by_name(self, name):\\n             if name in model_error.parameter_list:\\n                 self.latent[name].add(model_error.parameter_list, name)\\n \\n+\\n     def loglike(self, number_vector):\\n         self.latent.update(number_vector)\\n         raw_me = {}\\n\",\"diff --git a/bayes/latent.py b/bayes/latent.py\\nindex c62bd9b..61aeaee 100644\\n--- a/bayes/latent.py\\n+++ b/bayes/latent.py\\n@@ -1,7 +1,9 @@\\n import copy\\n import collections\\n from typing import Hashable\\n+\\n import numpy as np\\n+from tabulate import tabulate\\n \\n from .parameters import ParameterList\\n \\n@@ -68,3 +70,11 @@ def updated_parameters(\\n             start_index += latent.N\\n \\n         return lists\\n+\\n+    def __str__(self):\\n+        to_print = []\\n+        for global_name, latent in self.items():\\n+            for model_error_key, local_name in latent:\\n+                to_print.append((global_name, latent.N, model_error_key, local_name))\\n+\\n+        return tabulate(to_print, headers=[\\\"global name\\\", \\\"length\\\", \\\"model error\\\", \\\"local name\\\"])\\n\",\"diff --git a/bayes/inference_problem.py b/bayes/inference_problem.py\\nindex 9457be4..91d0126 100644\\n--- a/bayes/inference_problem.py\\n+++ b/bayes/inference_problem.py\\n@@ -1,6 +1,6 @@\\n import numpy as np\\n from .parameters import ParameterList\\n-# from .latent import LatentParameters\\n+from .latent import LatentParameters\\n from collections import OrderedDict\\n from .vb import MVN, Gamma, variational_bayes, VariationalBayesInterface\\n from .jacobian import d_model_error_d_named_parameter\\n@@ -47,7 +47,7 @@ def get_length(self, parameter_name):\\n \\n class InferenceProblem:\\n     def __init__(self):\\n-        self._latent = LatentParameters()\\n+        self.latent = LatentParameters()\\n         self.model_errors = OrderedDict()  # key : model_error\\n         self._noise_models = OrderedDict()  # key : noise_model\\n \\n@@ -75,41 +75,36 @@ def add_noise_model(self, noise_model, key=None):\\n         self._noise_models[key] = noise_model\\n         return key\\n \\n-    def __call__(self, number_vector):\\n-        self.latent.update(number_vector)\\n+    def evaluate_model_errors(self, number_vector):\\n+        updated_latent_parameters = self.latent.updated_parameters(number_vector)\\n         result = {}\\n-        for key, me in self.model_errors.items():\\n-            result[key] = me()\\n+        for me_key, prms in updated_latent_parameters.items():\\n+            result[me_key] = self.model_errors[me_key](prms)\\n         return result\\n \\n     def set_latent(self, global_and_local_name):\\n-        pass\\n-\\n-    def set_latent_individually(self, global_name, model_error_key, local_name):\\n-        pass\\n+        \\\"\\\"\\\"\\n+        There is no way to check if a model error actually has a\\n+        `global_and_local_name` parameter, so we pass it to all of them.\\n+        \\\"\\\"\\\"\\n+        for me_key in self.model_errors:\\n+            self.set_latent_individually(global_and_local_name, me_key)\\n \\n \\n-    def define_latent_parameter(self, global_and_local_name):\\n-        \\\"\\\"\\\"\\n+    def set_latent_individually(self, global_name, model_error_key, local_name=None):\\n+        local_name = global_name if local_name is None else local_name\\n+        model_error = self.model_errors[model_error_key]\\n \\n-        \\\"\\\"\\\"\\n-        for model_error in self.model_errors.values():\\n-            try:\\n-                prm = model_error.parameter_list\\n-            except AttributeError:\\n-                raise AttributeError(\\n-                    \\\"This method requires the `model_error` to have a `parameter_list` attribute!\\\"\\n-                )\\n+        try:\\n+            N = model_error.get_shape(local_name)\\n+        except AttributeError:\\n+            N = 1\\n \\n-            if name in model_error.parameter_list:\\n-                self.latent[name].add(model_error.parameter_list, name)\\n+        self.latent.add(global_name, local_name, model_error_key, N)\\n \\n \\n     def loglike(self, number_vector):\\n-        self.latent.update(number_vector)\\n-        raw_me = {}\\n-        for key, me in self.model_errors.items():\\n-            raw_me[key] = me()\\n+        model_errors = self.evaluate_model_errors(number_vector)\\n \\n         log_like = 0.0\\n         for noise_key, noise_term in self.noise_models.items():\\n@@ -222,7 +217,7 @@ def __call__(self, number_vector, concatenate=True):\\n         \\\"\\\"\\\"\\n         overwrites VariationalBayesInterface.__call__\\n         \\\"\\\"\\\"\\n-        me = super().__call__(number_vector)\\n+        me = super().evaluate_model_errors(number_vector)\\n \\n         errors_by_noise = {}\\n         for key, noise in self.noise_models.items():\\n\",\"diff --git a/bayes/latent.py b/bayes/latent.py\\nindex 61aeaee..52e9ba7 100644\\n--- a/bayes/latent.py\\n+++ b/bayes/latent.py\\n@@ -1,6 +1,7 @@\\n import copy\\n import collections\\n from typing import Hashable\\n+import logging\\n \\n import numpy as np\\n from tabulate import tabulate\\n@@ -8,13 +9,13 @@\\n from .parameters import ParameterList\\n \\n \\n+logger = logging.getLogger(__name__)\\n+\\n+\\n class LatentParameter(list):\\n     def __init__(self, N):\\n         self.N = N\\n \\n-    def add(self, model_error_key, local_name):\\n-        self.append((model_error_key, local_name))\\n-\\n \\n class InconsistentLengthException(Exception):\\n     pass\\n@@ -39,7 +40,13 @@ def add(\\n         latent = self[global_name]\\n         if latent.N != N:\\n             raise InconsistentLengthException(\\\"TODO: some info\\\")\\n-        latent.add(model_error_key, local_name)\\n+\\n+        parameter_reference = (model_error_key, local_name)\\n+        if parameter_reference in latent:\\n+            msg = f\\\"{parameter_reference} is already associated to the global \\\"\\n+            msg += f\\\"parameter {global_name}. No need to add it again!\\\"\\n+            logger.warning(msg)\\n+        latent.append((model_error_key, local_name))\\n \\n     def updated_parameters(\\n         self, number_vector: np.ndarray\\n@@ -77,4 +84,6 @@ def __str__(self):\\n             for model_error_key, local_name in latent:\\n                 to_print.append((global_name, latent.N, model_error_key, local_name))\\n \\n-        return tabulate(to_print, headers=[\\\"global name\\\", \\\"length\\\", \\\"model error\\\", \\\"local name\\\"])\\n+        return tabulate(\\n+            to_print, headers=[\\\"global name\\\", \\\"length\\\", \\\"model error\\\", \\\"local name\\\"]\\n+        )\\n\",\"diff --git a/bayes/inference_problem.py b/bayes/inference_problem.py\\nindex 91d0126..ee2954f 100644\\n--- a/bayes/inference_problem.py\\n+++ b/bayes/inference_problem.py\\n@@ -3,33 +3,25 @@\\n from .latent import LatentParameters\\n from collections import OrderedDict\\n from .vb import MVN, Gamma, variational_bayes, VariationalBayesInterface\\n-from .jacobian import d_model_error_d_named_parameter\\n+from .jacobian import jacobian\\n from typing import Hashable\\n \\n \\n class ModelErrorInterface:\\n-    def __call__(self, latent_parameter_list: ParameterList) -> dict[Hashable: np.ndarray]: \\n+    def __call__(\\n+        self, latent_parameter_list: ParameterList\\n+    ) -> dict[Hashable : np.ndarray]:\\n         \\\"\\\"\\\"\\n         Evaluate the model error based on the `latent_parameter_list`.\\n         \\\"\\\"\\\"\\n         raise NotImplementedError(\\\"Override this!\\\")\\n \\n-    def jacobian(self, latent_names=None):\\n-        jac = dict()\\n-        latent_names = latent_names or self.parameter_list.names\\n-        for prm_name in latent_names:\\n-            prm_jac = d_model_error_d_named_parameter(self, prm_name)\\n-            for key in prm_jac:\\n-                if key not in jac:\\n-                    jac[key] = dict()\\n-\\n-                jac[key][prm_name] = prm_jac[key]\\n-\\n-        return jac\\n+    def jacobian(self, latent_parameter_list, w_r_t_what=None):\\n+        return jacobian(self, latent_parameter_list, w_r_t_what)\\n \\n     def get_length(self, parameter_name):\\n         \\\"\\\"\\\"\\n-        Overwrite for more complex behaviours, e.g. to \\n+        Overwrite for more complex behaviours, e.g. to\\n \\n         ~~~py\\n             if parameter_name == \\\"displacement_field\\\":\\n@@ -44,7 +36,6 @@ def get_length(self, parameter_name):\\n         return 1\\n \\n \\n-\\n class InferenceProblem:\\n     def __init__(self):\\n         self.latent = LatentParameters()\\n@@ -90,7 +81,6 @@ def set_latent(self, global_and_local_name):\\n         for me_key in self.model_errors:\\n             self.set_latent_individually(global_and_local_name, me_key)\\n \\n-\\n     def set_latent_individually(self, global_name, model_error_key, local_name=None):\\n         local_name = global_name if local_name is None else local_name\\n         model_error = self.model_errors[model_error_key]\\n@@ -102,7 +92,6 @@ def set_latent_individually(self, global_name, model_error_key, local_name=None)\\n \\n         self.latent.add(global_name, local_name, model_error_key, N)\\n \\n-\\n     def loglike(self, number_vector):\\n         model_errors = self.evaluate_model_errors(number_vector)\\n \\n@@ -151,58 +140,44 @@ def jacobian(self, number_vector, concatenate=True):\\n         \\\"\\\"\\\"\\n         overwrites VariationalBayesInterface.jacobian\\n         \\\"\\\"\\\"\\n-        self.latent.update(number_vector)\\n+        updated_latent_parameters = self.latent.updated_parameters(number_vector)\\n         jac = {}\\n-        for key, me in self.model_errors.items():\\n-\\n-            # For each global latent parameter, we now need to find its\\n-            # _local_ name, so the name in the parameter_list of the\\n-            # model_error ...\\n-            latent_names = self.latent.latent_names(me.parameter_list)\\n-            local_latent_names = [n[0] for n in latent_names]\\n-            # ... and only request the jacobian for the latent parameters.\\n-            sensor_parameter_jac = me.jacobian(local_latent_names)\\n+        for me_key, me in self.model_errors.items():\\n+            me_parameter_list = updated_latent_parameters[me_key]\\n+            sensor_parameter_jac = me.jacobian(me_parameter_list)\\n+            \\n             \\\"\\\"\\\"\\n             sensor_parameter_jac contains a \\n                 dict (sensor) of \\n                 dict (parameter)\\n             \\n-            We now flatten the last dict (parameter) in the order of the \\n-            latent parameters for a valid VB input.\\n-\\n-            This is challenging/ugly because:\\n-                * The \\\"parameter\\\" in sensor_parameter_jac is not the same\\n-                  as the corresponding _global_ parameter in the latent\\n-                  parameters.\\n-                * Some of the latent parameters may not be part of \\n-                  sensor_parameter_jac, because it only is a parameter of a \\n-                  different model error. We have to fill it with zeros of the\\n-                  right dimension\\n+            We now \\\"flatten\\\" the last dict (parameter) in the order of the \\n+            latent parameters for a valid VB input. \\n \\n             \\\"\\\"\\\"\\n+            \\n             sensor_jac = {}\\n             for sensor, parameter_jac in sensor_parameter_jac.items():\\n                 first_jac = list(parameter_jac.values())[0]\\n                 N = len(first_jac)\\n-\\n-                # We allocate \\\"stacked_jac\\\" where each column corresponds\\n-                # to a number in the \\\"number_vector\\\".\\n+                \\n                 stacked_jac = np.zeros((N, len(number_vector)))\\n \\n-                for (local_name, global_name) in latent_names:\\n-                    J = parameter_jac[local_name]\\n+                for local_name in me_parameter_list.names:\\n+                    global_name = self.latent.global_name(local_name)\\n+                    indices = self.latent.global_indices(global_name)\\n \\n+                    J = parameter_jac[local_name]\\n                     # If it is a scalar parameter, the user may have\\n                     # defined as a vector of length N. We need to\\n                     # transform it to a matrix Nx1.\\n                     if len(J.shape) == 1:\\n                         J = np.atleast_2d(J).T\\n-\\n-                    stacked_jac[:, self.latent[global_name].global_index_range()] += J\\n-\\n+                    \\n+                    stacked_jac[:, indices] += J\\n                 sensor_jac[sensor] = stacked_jac\\n \\n-            jac[key] = sensor_jac\\n+            jac[me_key] = sensor_jac\\n \\n         jacs_by_noise = {}\\n         for key, noise in self.noise_models.items():\\n@@ -243,4 +218,9 @@ def prior_MVN(self):\\n                 means.append(mean)\\n                 precs.append(1.0 / sd ** 2)\\n \\n-        return MVN(means, np.diag(precs), name=\\\"MVN prior\\\", parameter_names=list(self.latent.keys()))\\n+        return MVN(\\n+            means,\\n+            np.diag(precs),\\n+            name=\\\"MVN prior\\\",\\n+            parameter_names=list(self.latent.keys()),\\n+        )\\n\",\"diff --git a/bayes/jacobian.py b/bayes/jacobian.py\\nindex 9045b03..e6435dc 100644\\n--- a/bayes/jacobian.py\\n+++ b/bayes/jacobian.py\\n@@ -4,79 +4,70 @@\\n def delta_x(x0, delta=None):\\n     if delta is not None:\\n         return delta\\n-    dx = x0 * 1.0e-7 + 1.0e-7  # approx x0 * sqrt(machine precision)\\n-    if dx == 0:\\n-        dx = 1.0e-7\\n-    return dx\\n-\\n-\\n-def d_model_error_d_named_parameter(model_error, prm_name):\\n-    if hasattr(model_error.parameter_list[prm_name], \\\"__len__\\\"):\\n-        return d_model_error_d_vector_parameter(model_error, prm_name)\\n-    else:\\n-        return d_model_error_d_scalar_parameter(model_error, prm_name)\\n-\\n-\\n-def d_model_error_d_scalar_parameter(model_error, prm_name):\\n-    \\\"\\\"\\\"\\n-    Calculates the derivative of `model_error` w.r.t the named parameter \\n-    `prm_name`.\\n-\\n-    model_error:\\n-        object that has an attribute `parameter_list` and a __call__() method\\n-        without arguments that returns a dict of type \\n-        {key : numpy_vector of length N}\\n-    prm_name:\\n-        name of a named scalar parameter in `model_error.parameter_list` \\n-    returns:\\n-        dict of type {key : numpy_vector of length N}\\n-    \\\"\\\"\\\"\\n-    prm0 = model_error.parameter_list[prm_name]\\n-    dx = delta_x(prm0)\\n-\\n-    model_error.parameter_list[prm_name] = prm0 - dx\\n-    me0 = model_error()\\n-    model_error.parameter_list[prm_name] = prm0 + dx\\n-    me1 = model_error()\\n-    model_error.parameter_list[prm_name] = prm0\\n \\n+    if x0 == 0:\\n+        return 1.0e-6\\n+\\n+    return x0 * 1.0e-6  # approx x0 * sqrt(machine precision)\\n+\\n+\\n+def jacobian(f, latent_parameter_list, w_r_t_what=None):\\n     jac = dict()\\n-    for key in me0:\\n-        jac[key] = (me1[key] - me0[key]) / (2 * dx)\\n+    w_r_t_what = w_r_t_what or latent_parameter_list.names\\n+    for prm_name in w_r_t_what:\\n+        if hasattr(latent_parameter_list[prm_name], \\\"__len__\\\"):\\n+            prm_jac = _vector_jac(f, latent_parameter_list, prm_name)\\n+        else:\\n+            prm_jac = _scalar_jac(f, latent_parameter_list, prm_name)\\n+\\n+        for sensor_key, sensor_jac in prm_jac.items():\\n+            if sensor_key not in jac:\\n+                jac[sensor_key] = dict()\\n+            jac[sensor_key][prm_name] = sensor_jac\\n+\\n+    return jac\\n+\\n+\\n+def _vector_jac(f, latent_parameter_list, prm_name):\\n+    jac = None\\n+    value = latent_parameter_list[prm_name]\\n+    N = len(value)\\n+\\n+    for row in range(N):\\n+        dx = delta_x(value[row])\\n+\\n+        # generate N zeros with a single 1 at position row\\n+        mask = np.eye(1, N, k=row).flatten()\\n+\\n+        prm_plus = latent_parameter_list.with_value(prm_name, value + mask * 0.5 * dx)\\n+        prm_minus = latent_parameter_list.with_value(prm_name, value - mask * 0.5 * dx)\\n+\\n+        me_plus = f(prm_plus)\\n+        me_minus = f(prm_minus)\\n+\\n+        if jac is None:\\n+            jac = dict()\\n+            for sensor_key, me_values in me_plus.items():\\n+                jac[sensor_key] = np.empty((len(me_values), N))\\n+\\n+        for sensor_key in jac:\\n+            jac[sensor_key][:, row] = (me_plus[sensor_key] - me_minus[sensor_key]) / dx\\n+\\n     return jac\\n \\n \\n-def d_model_error_d_vector_parameter(model_error, prm_name):\\n-    \\\"\\\"\\\"\\n-    Calculates the derivative of `model_error` w.r.t the named parameter \\n-    `prm_name`.\\n-\\n-    model_error:\\n-        object that has an attribute `parameter_list` and a __call__() method\\n-        without arguments that returns a dict of type \\n-        {key : numpy_vector of length N}\\n-    prm_name:\\n-        name of a named vector parameter in `model_error.parameter_list` of \\n-        length M\\n-    returns:\\n-        dict of type {key : numpy_matrix of length NxM}\\n-    \\\"\\\"\\\"\\n-    prm0 = np.copy(model_error.parameter_list[prm_name])\\n-    M = len(prm0)\\n+def _scalar_jac(f, latent_parameter_list, prm_name):\\n     jac = dict()\\n+    value = latent_parameter_list[prm_name]\\n \\n-    for row in range(M):\\n-        dx = delta_x(prm0[row])\\n+    dx = delta_x(value)\\n+    prm_plus = latent_parameter_list.with_value(prm_name, value + 0.5 * dx)\\n+    prm_minus = latent_parameter_list.with_value(prm_name, value - 0.5 * dx)\\n \\n-        model_error.parameter_list[prm_name][row] = prm0[row] - dx\\n-        me0 = model_error()\\n-        model_error.parameter_list[prm_name][row] = prm0[row] + dx\\n-        me1 = model_error()\\n-        model_error.parameter_list[prm_name][row] = prm0[row]\\n+    me_plus = f(prm_plus)\\n+    me_minus = f(prm_minus)\\n \\n-        for key in me0:\\n-            if key not in jac:\\n-                jac[key] = np.empty((len(me0[key]), M))\\n+    for sensor_key in me_plus:\\n+        jac[sensor_key] = (me_plus[sensor_key] - me_minus[sensor_key]) / dx\\n \\n-            jac[key][:, row] = (me1[key] - me0[key]) / (2 * dx)\\n     return jac\\n\",\"diff --git a/bayes/latent.py b/bayes/latent.py\\nindex 52e9ba7..3847ae8 100644\\n--- a/bayes/latent.py\\n+++ b/bayes/latent.py\\n@@ -63,21 +63,37 @@ def updated_parameters(\\n \\n         lists = copy.deepcopy(self._empty_parameter_lists)\\n \\n-        start_index = 0\\n+        start_idx = 0\\n \\n         for global_name, latent in self.items():\\n \\n             if latent.N == 1:\\n-                value = number_vector[start_index]\\n+                value = number_vector[start_idx]\\n             else:\\n-                value = number_vector[start_index : start_index + latent.N]\\n+                value = number_vector[start_idx : start_idx + latent.N]\\n \\n             for model_error_key, local_name in latent:\\n                 lists[model_error_key].define(local_name, value)\\n-            start_index += latent.N\\n+            start_idx += latent.N\\n \\n         return lists\\n \\n+    def global_name(self, local_name):\\n+        for global_name, latent in self.items():\\n+            for _, ref_local_name in latent:\\n+                if ref_local_name == local_name:\\n+                    return global_name\\n+        return None \\n+\\n+    def global_indices(self, global_name):\\n+        start_idx = 0\\n+        for _global_name, _latent in self.items():\\n+            if global_name == _global_name:\\n+                return list(range(start_idx, start_idx + _latent.N))\\n+            start_idx += _latent.N\\n+        assert False\\n+\\n+\\n     def __str__(self):\\n         to_print = []\\n         for global_name, latent in self.items():\\n\",\"diff --git a/bayes/parameters.py b/bayes/parameters.py\\nindex 017fb73..7e5bcb6 100644\\n--- a/bayes/parameters.py\\n+++ b/bayes/parameters.py\\n@@ -1,6 +1,9 @@\\n+import copy\\n+\\n+\\n class ParameterList:\\n     \\\"\\\"\\\"\\n-    The ParameterList serves as an input to the user-defined models. It is \\n+    The ParameterList serves as an input to the user-defined models. It is\\n     basically a name:value-dict that allows the user to access the parameters\\n     by name instead of some vector index, which could read:\\n \\n@@ -33,10 +36,10 @@ def __setitem__(self, name, value):\\n \\n     def __add__(self, other):\\n         \\\"\\\"\\\"\\n-        Adding two ParameterLists can be convenient of nested models. An \\n+        Adding two ParameterLists can be convenient of nested models. An\\n         example could be a model error that combines a forward_model and\\n         a sensor_data_model like:\\n-            \\n+\\n             class MyModelError:\\n                 def __init__(self, forward_model, sensor_data_model):\\n                     self.fw = forward_model\\n@@ -45,13 +48,25 @@ def __init__(self, forward_model, sensor_data_model):\\n                                                                  ^\\n                                     this \\\"+\\\" is defined here ----|\\n         \\\"\\\"\\\"\\n-        concat = ParameterList()\\n-        for name, value in self.p.items():\\n+        concat = copy.deepcopy(self)\\n+        for name, value in other.p.items():\\n             concat.define(name, value)\\n+        return concat\\n+\\n+    def overwrite_with(self, other):\\n+        \\\"\\\"\\\" \\\"\\\"\\\"\\n+        concat = copy.deepcopy(self)\\n         for name, value in other.p.items():\\n+            assert name in concat\\n             concat.define(name, value)\\n         return concat\\n-    \\n+\\n+    def with_value(self, name, value):\\n+        \\\"\\\"\\\" \\\"\\\"\\\"\\n+        new = copy.deepcopy(self)\\n+        new[name] = value\\n+        return new\\n+\\n     def __str__(self):\\n         s = \\\"\\\"\\n         for name, value in self.p.items():\\n\",\"diff --git a/bayes/inference_problem.py b/bayes/inference_problem.py\\nindex ee2954f..7d2a9c2 100644\\n--- a/bayes/inference_problem.py\\n+++ b/bayes/inference_problem.py\\n@@ -97,7 +97,7 @@ def loglike(self, number_vector):\\n \\n         log_like = 0.0\\n         for noise_key, noise_term in self.noise_models.items():\\n-            log_like += noise_term.loglike_contribution(raw_me)\\n+            log_like += noise_term.loglike_contribution(model_errors)\\n \\n         return log_like\\n \\n\",\"diff --git a/bayes/inference_problem.py b/bayes/inference_problem.py\\nindex 7d2a9c2..c3817ee 100644\\n--- a/bayes/inference_problem.py\\n+++ b/bayes/inference_problem.py\\n@@ -4,13 +4,14 @@\\n from collections import OrderedDict\\n from .vb import MVN, Gamma, variational_bayes, VariationalBayesInterface\\n from .jacobian import jacobian\\n-from typing import Hashable\\n+from typing import Hashable, Dict\\n+\\n \\n \\n class ModelErrorInterface:\\n     def __call__(\\n         self, latent_parameter_list: ParameterList\\n-    ) -> dict[Hashable : np.ndarray]:\\n+    ) -> Dict[Hashable, np.ndarray]:\\n         \\\"\\\"\\\"\\n         Evaluate the model error based on the `latent_parameter_list`.\\n         \\\"\\\"\\\"\\n\",\"diff --git a/bayes/latent.py b/bayes/latent.py\\nindex 3847ae8..42eecd1 100644\\n--- a/bayes/latent.py\\n+++ b/bayes/latent.py\\n@@ -1,6 +1,6 @@\\n import copy\\n import collections\\n-from typing import Hashable\\n+from typing import Hashable, Dict\\n import logging\\n \\n import numpy as np\\n@@ -27,7 +27,7 @@ def __init__(self):\\n         # This member is just for convenience/performance such that the\\n         # `updated_parameters` do not have to build it again and can\\n         # just copy it.\\n-        self._empty_parameter_lists: dict[Hashable, ParameterList] = {}\\n+        self._empty_parameter_lists: Dict[Hashable, ParameterList] = {}\\n \\n     def add(\\n         self, global_name: str, local_name: str, model_error_key: Hashable, N: int = 1\\n@@ -50,7 +50,7 @@ def add(\\n \\n     def updated_parameters(\\n         self, number_vector: np.ndarray\\n-    ) -> dict[Hashable, ParameterList]:\\n+    ) -> Dict[Hashable, ParameterList]:\\n \\n         # maybe make that a member?\\n         current_length = sum(l.N for l in self.values())\\n\",\"diff --git a/bayes/inference_problem.py b/bayes/inference_problem.py\\nindex c3817ee..843abb0 100644\\n--- a/bayes/inference_problem.py\\n+++ b/bayes/inference_problem.py\\n@@ -1,11 +1,12 @@\\n-import numpy as np\\n-from .parameters import ParameterList\\n-from .latent import LatentParameters\\n from collections import OrderedDict\\n-from .vb import MVN, Gamma, variational_bayes, VariationalBayesInterface\\n-from .jacobian import jacobian\\n-from typing import Hashable, Dict\\n+from typing import Dict, Hashable\\n \\n+import numpy as np\\n+\\n+from .jacobian import jacobian\\n+from .latent import LatentParameters\\n+from .parameters import ParameterList\\n+from .vb import MVN, Gamma, VariationalBayesInterface, variational_bayes\\n \\n \\n class ModelErrorInterface:\\n\",\"diff --git a/bayes/latent.py b/bayes/latent.py\\nindex 42eecd1..b007d1b 100644\\n--- a/bayes/latent.py\\n+++ b/bayes/latent.py\\n@@ -1,19 +1,18 @@\\n-import copy\\n import collections\\n-from typing import Hashable, Dict\\n+import copy\\n import logging\\n+from typing import Dict, Hashable, List, Tuple\\n \\n-import numpy as np\\n+import numpy as np  # type: ignore\\n from tabulate import tabulate\\n \\n from .parameters import ParameterList\\n \\n-\\n logger = logging.getLogger(__name__)\\n \\n \\n-class LatentParameter(list):\\n-    def __init__(self, N):\\n+class ParameterListReferences(list):\\n+    def __init__(self, N: int):\\n         self.N = N\\n \\n \\n@@ -35,7 +34,7 @@ def add(\\n         self._empty_parameter_lists[model_error_key] = ParameterList()\\n \\n         if global_name not in self:\\n-            self[global_name] = LatentParameter(N)\\n+            self[global_name] = ParameterListReferences(N)\\n \\n         latent = self[global_name]\\n         if latent.N != N:\\n@@ -78,23 +77,24 @@ def updated_parameters(\\n \\n         return lists\\n \\n-    def global_name(self, local_name):\\n+    def global_name(self, local_name: str) -> str:\\n         for global_name, latent in self.items():\\n             for _, ref_local_name in latent:\\n                 if ref_local_name == local_name:\\n                     return global_name\\n-        return None \\n+        raise RuntimeError(\\n+            f\\\"{local_name} has no global_name, since it is not defined as latent!\\\"\\n+        )\\n \\n-    def global_indices(self, global_name):\\n+    def global_indices(self, global_name: str) -> str:\\n         start_idx = 0\\n         for _global_name, _latent in self.items():\\n             if global_name == _global_name:\\n                 return list(range(start_idx, start_idx + _latent.N))\\n             start_idx += _latent.N\\n-        assert False\\n-\\n+        raise RuntimeError(f\\\"{global_name} is not defined as latent!\\\")\\n \\n-    def __str__(self):\\n+    def __str__(self) -> str:\\n         to_print = []\\n         for global_name, latent in self.items():\\n             for model_error_key, local_name in latent:\\n\",\"diff --git a/bayes/parameters.py b/bayes/parameters.py\\nindex 7e5bcb6..cecaea6 100644\\n--- a/bayes/parameters.py\\n+++ b/bayes/parameters.py\\n@@ -1,4 +1,8 @@\\n import copy\\n+from typing import Union, List\\n+from numpy import typing as ndt\\n+\\n+ParameterValue = Union[float, ndt.ArrayLike]\\n \\n \\n class ParameterList:\\n@@ -12,19 +16,19 @@ def my_model(prm):\\n \\n     \\\"\\\"\\\"\\n \\n-    def __init__(self):\\n+    def __init__(self) -> None:\\n         self.p = {}\\n \\n-    def define(self, name, value=None):\\n+    def define(self, name: str, value: ParameterValue = None) -> None:\\n         self.p[name] = value\\n \\n-    def __getitem__(self, name):\\n+    def __getitem__(self, name: str) -> ParameterValue:\\n         return self.p[name]\\n \\n-    def __contains__(self, name):\\n+    def __contains__(self, name: str) -> bool:\\n         return name in self.p\\n \\n-    def __setitem__(self, name, value):\\n+    def __setitem__(self, name: str, value: ParameterValue) -> None:\\n         \\\"\\\"\\\"\\n         Calling parameter_list[\\\"A\\\"]=0. when there is no parameter \\\"A\\\" defined\\n         may hide some bugs in the user code. Thus, we force parameters to be\\n@@ -34,7 +38,7 @@ def __setitem__(self, name, value):\\n             raise Exception(\\\"Call .define to define new parameters.\\\")\\n         self.define(name, value)\\n \\n-    def __add__(self, other):\\n+    def __add__(self, other: \\\"ParameterList\\\") -> \\\"ParameterList\\\":\\n         \\\"\\\"\\\"\\n         Adding two ParameterLists can be convenient of nested models. An\\n         example could be a model error that combines a forward_model and\\n@@ -53,7 +57,7 @@ def __init__(self, forward_model, sensor_data_model):\\n             concat.define(name, value)\\n         return concat\\n \\n-    def overwrite_with(self, other):\\n+    def overwrite_with(self, other: \\\"ParameterList\\\") -> \\\"ParameterList\\\":\\n         \\\"\\\"\\\" \\\"\\\"\\\"\\n         concat = copy.deepcopy(self)\\n         for name, value in other.p.items():\\n@@ -61,18 +65,18 @@ def overwrite_with(self, other):\\n             concat.define(name, value)\\n         return concat\\n \\n-    def with_value(self, name, value):\\n+    def with_value(self, name: str, value: ParameterValue) -> \\\"ParameterList\\\":\\n         \\\"\\\"\\\" \\\"\\\"\\\"\\n         new = copy.deepcopy(self)\\n         new[name] = value\\n         return new\\n \\n-    def __str__(self):\\n+    def __str__(self) -> str:\\n         s = \\\"\\\"\\n         for name, value in self.p.items():\\n             s += f\\\"{name:20s} {value}\\\\n\\\"\\n         return s\\n \\n     @property\\n-    def names(self):\\n+    def names(self) -> List[str]:\\n         return list(self.p.keys())\\n\"]", "test_patch": "[\"diff --git a/tests/test_latent.py b/tests/test_latent.py\\nindex 139e856..8e313f9 100644\\n--- a/tests/test_latent.py\\n+++ b/tests/test_latent.py\\n@@ -1,96 +1,42 @@\\n import unittest\\n-from bayes.parameters import ParameterList\\n-from bayes.latent import LatentParameters\\n+from bayes.latent import LatentParameters, InconsistentLengthException\\n \\n \\n class TestLatentParameters(unittest.TestCase):\\n-    def setUp(self):\\n-        self.pA = ParameterList()\\n-        self.pA.define(\\\"A\\\", 0)\\n-        self.pA.define(\\\"shared\\\", 2)\\n-\\n-        self.pB = ParameterList()\\n-        self.pB.define(\\\"B\\\", 1)\\n-        self.pB.define(\\\"shared\\\", 2)\\n-        self.pB.define(\\\"list\\\", [3, 4])\\n-\\n-    def test_add(self):\\n-        latent = LatentParameters()\\n-        latent[\\\"latentA\\\"].add(self.pA, \\\"A\\\")\\n-\\n-    def test_add_twice(self):\\n-        latent = LatentParameters()\\n-        latent[\\\"latentA\\\"].add(self.pA, \\\"A\\\")\\n-\\n-        another_pA = ParameterList()\\n-        another_pA.define(\\\"A\\\", 0)\\n-        another_pA.define(\\\"shared\\\", 2)\\n-        latent[\\\"latentA\\\"].add(another_pA, \\\"A\\\")\\n-\\n-    def test_shared(self):\\n-        latent = LatentParameters()\\n-        latent[\\\"shared\\\"].add(self.pA, \\\"shared\\\")\\n-        latent[\\\"shared\\\"].add(self.pB, \\\"shared\\\")\\n-\\n-        self.assertTrue(latent[\\\"shared\\\"].has(self.pA, \\\"shared\\\"))\\n-\\n-        self.assertListEqual(latent[\\\"shared\\\"].global_index_range(), [0])\\n-        latent.update([42])\\n-        self.assertEqual(self.pA[\\\"shared\\\"], 42)\\n-        self.assertEqual(self.pB[\\\"shared\\\"], 42)\\n-\\n-        with self.assertRaises(Exception) as e:\\n-            latent.update([42, 42])\\n-        print(\\\"Expected exception: \\\\n\\\", e.exception)\\n-\\n-    def test_set_value(self):\\n-        latent = LatentParameters()\\n-        latent[\\\"shared\\\"].add(self.pA, \\\"shared\\\")\\n-        latent[\\\"shared\\\"].add(self.pB, \\\"shared\\\")\\n-        latent[\\\"shared\\\"].set_value(42)\\n-\\n-        self.assertEqual(self.pA[\\\"shared\\\"], 42)\\n-        self.assertEqual(self.pA[\\\"shared\\\"], 42)\\n-\\n-        with self.assertRaises(Exception) as e:\\n-            latent[\\\"shared\\\"].set_value([1, 2, 3])\\n-        print(\\\"Expected exception: \\\\n\\\", e.exception)\\n-\\n-    def test_start_vector(self):\\n-        latent = LatentParameters()\\n-        latent[\\\"A\\\"].add(self.pA, \\\"A\\\")\\n-        latent[\\\"B\\\"].add(self.pB, \\\"B\\\")\\n-        latent[\\\"shared\\\"].add(self.pA, \\\"shared\\\")\\n-        latent[\\\"shared\\\"].add(self.pB, \\\"shared\\\")\\n-        latent[\\\"list\\\"].add(self.pB, \\\"list\\\")\\n-\\n-        v = latent.get_vector()\\n-        self.assertListEqual(v, [0, 1, 2, 3, 4])\\n-\\n-        v = latent.get_vector({\\\"A\\\": 42, \\\"list\\\": [61, 74]})\\n-        self.assertListEqual(v, [42, 1, 2, 61, 74])\\n-\\n-        # check for dimensions errors\\n-        with self.assertRaises(Exception) as e:\\n-            v = latent.get_vector({\\\"A\\\": [1, 2, 3]})\\n-        print(\\\"Expected exception: \\\\n\\\", e.exception)\\n-\\n-        with self.assertRaises(Exception) as e:\\n-            v = latent.get_vector({\\\"list\\\": [1, 2, 3]})\\n-        print(\\\"Expected exception: \\\\n\\\", e.exception)\\n-\\n-        # We expect an exception, if a shared parameter is not defined\\n-        # unambiguously\\n-        self.pA[\\\"shared\\\"] = 20\\n-        with self.assertRaises(RuntimeError) as e:\\n-            v = latent.get_vector()\\n-        print(\\\"Expected exception: \\\\n\\\", e.exception)\\n-\\n-        # Provinding a default value for that case is fine though:\\n-        v = latent.get_vector({\\\"shared\\\": 42})\\n-\\n-        self.assertEqual(len(v), 5)\\n-        self.assertEqual(latent.vector_length, 5)\\n+    def test_update(self):\\n+        l = LatentParameters()\\n+\\n+        l.add(\\\"shared\\\", \\\"A1\\\", \\\"model1\\\")\\n+        l.add(\\\"shared\\\", \\\"A2\\\", \\\"model2\\\")\\n+        l.add(\\\"B\\\", \\\"B\\\", \\\"model1\\\", 2)\\n+\\n+        new = l.updated_parameters([42, 6174, 84])\\n+\\n+        self.assertEqual(new[\\\"model1\\\"][\\\"A1\\\"], 42)\\n+        self.assertEqual(new[\\\"model2\\\"][\\\"A2\\\"], 42)\\n+        self.assertEqual(new[\\\"model1\\\"][\\\"B\\\"], [6174, 84])\\n+\\n+    def test_inconsistent_length(self):\\n+        l = LatentParameters()\\n+        l.add(\\\"GlobalName\\\", \\\"A\\\", \\\"model\\\", N=42)\\n+        with self.assertRaises(InconsistentLengthException) as e:\\n+            l.add(\\\"GlobalName\\\", \\\"who\\\", \\\"cares\\\", N=6174)\\n+\\n+        # TODO?\\n+        # msg = str(e.exception)\\n+        # print(msg)\\n+        # self.assertIn(\\\"42\\\", msg)\\n+        # self.assertIn(\\\"6174\\\", msg)\\n+        # self.assertIn(\\\"GlobalName\\\", msg)\\n+\\n+        with self.assertRaises(InconsistentLengthException) as e:\\n+            some_vector_with_not_length_42 = [1, 2, 3]\\n+            l.updated_parameters(some_vector_with_not_length_42)\\n+\\n+        msg = str(e.exception)\\n+        print(msg)\\n+        self.assertIn(\\\"42\\\", msg)\\n+        self.assertIn(\\\"3\\\", msg)\\n \\n \\n if __name__ == \\\"__main__\\\":\\n\",\"diff --git a/tests/test_latent.py b/tests/test_latent.py\\nindex 8e313f9..7749bf0 100644\\n--- a/tests/test_latent.py\\n+++ b/tests/test_latent.py\\n@@ -38,6 +38,14 @@ def test_inconsistent_length(self):\\n         self.assertIn(\\\"42\\\", msg)\\n         self.assertIn(\\\"3\\\", msg)\\n \\n+    def test_pretty_print(self):\\n+        l = LatentParameters()\\n+        l.add(\\\"shared\\\", \\\"A1\\\", \\\"model1\\\")\\n+        l.add(\\\"shared\\\", \\\"A2\\\", \\\"model2\\\")\\n+        l.add(\\\"B\\\", \\\"B\\\", \\\"model1\\\", 2)\\n+\\n+        print(l)\\n+\\n \\n if __name__ == \\\"__main__\\\":\\n     unittest.main()\\n\",\"diff --git a/tests/test_inference_problem.py b/tests/test_inference_problem.py\\nindex cebec63..9a4146c 100644\\n--- a/tests/test_inference_problem.py\\n+++ b/tests/test_inference_problem.py\\n@@ -6,42 +6,46 @@\\n from bayes.inference_problem import VariationalBayesProblem, InferenceProblem\\n \\n \\n-class ModelError:\\n-    def __init__(self):\\n-        self.parameter_list = ParameterList()\\n-        self.parameter_list.define(\\\"B\\\")\\n-\\n-    def __call__(self):\\n-        x = np.linspace(0, 1, 10)\\n-        return {\\\"dummy_sensor\\\": x * self.parameter_list[\\\"B\\\"]}\\n+def dummy_model_error(prms):\\n+    x = np.r_[1, 2]\\n+    return {\\\"dummy_sensor\\\": x * prms[\\\"B\\\"]}\\n \\n \\n class TestProblem(unittest.TestCase):\\n     def test_add(self):\\n         p = InferenceProblem()\\n-        me = ModelError()\\n-        p.add_model_error(me, key=\\\"0\\\")\\n-        self.assertRaises(Exception, p.add_model_error, me, key=\\\"0\\\")\\n+        p.add_model_error(dummy_model_error, key=\\\"0\\\")\\n+        with self.assertRaises(Exception):\\n+            p.add_model_error(dummy_model_error, key=\\\"0\\\")\\n+\\n+    def test_latent_parameters(self):\\n+        p = InferenceProblem()\\n+        me_key = p.add_model_error(dummy_model_error)\\n+        p.set_latent_individually(\\\"B\\\", me_key, \\\"B\\\")\\n+        # or\\n+        p.set_latent_individually(\\\"B\\\", me_key)\\n+\\n+        me = p.evaluate_model_errors([42])\\n+        self.assertListEqual([42, 84], list(me[me_key][\\\"dummy_sensor\\\"]))\\n \\n     def test_shared_latent_evaluate(self):\\n         p = InferenceProblem()\\n         N = 3\\n         for _ in range(N):\\n-            p.add_model_error(ModelError())\\n-        p.define_shared_latent_parameter_by_name(\\\"B\\\")\\n+            p.add_model_error(dummy_model_error)\\n+        p.set_latent(\\\"B\\\")\\n         self.assertEqual(len(p.latent[\\\"B\\\"]), N)\\n \\n-        result = p([0.1])\\n-        for key, model_error in p.model_errors.items():\\n-            self.assertAlmostEqual(model_error.parameter_list[\\\"B\\\"], 0.1)\\n-            self.assertListEqual(list(result[key]), list(model_error()))\\n+        result = p.evaluate_model_errors([42])\\n+        for key, me in p.model_errors.items():\\n+            self.assertListEqual(list(result[key]), list(dummy_model_error({\\\"B\\\": 42})))\\n \\n \\n class TestVBProblem(unittest.TestCase):\\n     def test_prior(self):\\n         p = VariationalBayesProblem()\\n-        p.add_model_error(ModelError())\\n-        p.define_shared_latent_parameter_by_name(\\\"B\\\")\\n+        p.add_model_error(dummy_model_error)\\n+        p.set_latent(\\\"B\\\")\\n         p.set_normal_prior(\\\"B\\\", 0.0, 1.0)\\n         self.assertRaises(Exception, p.set_normal_prior, \\\"not B\\\", 0.0, 1.0)\\n \\n@@ -52,7 +56,7 @@ def test_prior(self):\\n \\n         result = p([0.1])\\n         self.assertEqual(len(result), 1)  # one noise group\\n-        self.assertEqual(len(result[\\\"noise\\\"]), 10)\\n+        self.assertEqual(len(result[\\\"noise\\\"]), 2)\\n \\n \\n if __name__ == \\\"__main__\\\":\\n\",\"diff --git a/tests/test_jacobian.py b/tests/test_jacobian.py\\nindex 97f331a..0f175ab 100644\\n--- a/tests/test_jacobian.py\\n+++ b/tests/test_jacobian.py\\n@@ -1,93 +1,88 @@\\n import unittest\\n import numpy as np\\n+from bayes.jacobian import jacobian\\n from bayes.inference_problem import ModelErrorInterface, VariationalBayesProblem\\n+from bayes.parameters import ParameterList\\n from bayes.noise import UncorrelatedSingleNoise\\n \\n CHECK = np.testing.assert_array_almost_equal  # just to make it shorter\\n \\n \\n-class DummyME(ModelErrorInterface):\\n+class DummyME:\\n     def __init__(self):\\n-        super().__init__()\\n-        self.parameter_list.define(\\\"A\\\", 42.0)\\n-        self.parameter_list.define(\\\"B\\\", 0.0)\\n         self.xs = np.linspace(0.0, 1.0, 3)\\n \\n-    def __call__(self):\\n-        A, B = self.parameter_list[\\\"A\\\"], self.parameter_list[\\\"B\\\"]\\n+    def __call__(self, prm):\\n+        A, B = prm[\\\"A\\\"], prm[\\\"B\\\"]\\n         return {\\\"out1\\\": self.xs * A + B ** 2, \\\"out2\\\": self.xs * A ** 2 + B * self.xs}\\n \\n \\n class DummyMEPartial(ModelErrorInterface):\\n     def __init__(self):\\n-        super().__init__()\\n-        self.parameter_list.define(\\\"A\\\", 42.0)\\n-        self.parameter_list.define(\\\"B\\\", 0.0)\\n         self.xs = np.linspace(0.0, 1.0, 3)\\n \\n-    def __call__(self):\\n-        A, B = self.parameter_list[\\\"A\\\"], self.parameter_list[\\\"B\\\"]\\n+    def __call__(self, prm):\\n+        A, B = prm[\\\"A\\\"], prm[\\\"B\\\"]\\n         return {\\\"out1\\\": self.xs * A + B ** 2, \\\"out2\\\": self.xs * A ** 2 + B * self.xs}\\n \\n-    def jacobian(self):\\n+    def jacobian(self, prm):\\n         \\\"\\\"\\\"\\n         We can provide the derivative w.r.t. \\\"A\\\" analytically and use the\\n         central differences of the superclass for the parameter \\\"B\\\".\\n         \\\"\\\"\\\"\\n-        jac = super().jacobian([\\\"B\\\"])\\n+        jac = super().jacobian(prm, [\\\"B\\\"])\\n         jac[\\\"out1\\\"][\\\"A\\\"] = self.xs\\n-        jac[\\\"out2\\\"][\\\"A\\\"] = 2 * self.parameter_list[\\\"A\\\"] * self.xs\\n+        jac[\\\"out2\\\"][\\\"A\\\"] = 2 * prm[\\\"A\\\"] * self.xs\\n         return jac\\n \\n \\n-class DummyMEVectorPrm(ModelErrorInterface):\\n-    def __init__(self):\\n-        super().__init__()\\n-        self.parameter_list.define(\\\"X\\\", [1.0, 2.0, 3.0])\\n-\\n-    def __call__(self):\\n-        x = np.asarray(self.parameter_list[\\\"X\\\"])\\n-        return {\\\"out\\\": np.concatenate([x ** 2, x ** 3])}\\n+def dummy_me_vector_prm(prm):\\n+    x = np.asarray(prm[\\\"X\\\"])\\n+    return {\\\"out\\\": np.concatenate([x ** 2, x ** 3])}\\n \\n \\n-class DummyMEVectorPrm2(ModelErrorInterface):\\n-    def __init__(self):\\n-        super().__init__()\\n-        self.parameter_list.define(\\\"X\\\", [0.0, 42.0])\\n-\\n-    def __call__(self):\\n-        return {\\\"out\\\": np.r_[6174 + sum(self.parameter_list[\\\"X\\\"])]}\\n+def dummy_me_vector_prm2(prm):\\n+    return {\\\"out\\\": np.r_[6174 + sum(prm[\\\"X\\\"])]}\\n \\n \\n class TestJacobian(unittest.TestCase):\\n     def test_scalar_prm(self):\\n+        A, B = 42.0, 0.0\\n+\\n         me = DummyME()\\n-        A, B = me.parameter_list[\\\"A\\\"], me.parameter_list[\\\"B\\\"]\\n-        jac = me.jacobian()\\n+        prm = ParameterList()\\n+        prm.define(\\\"A\\\", A)\\n+        prm.define(\\\"B\\\", B)\\n+        jac = jacobian(me, prm)\\n         CHECK(jac[\\\"out1\\\"][\\\"A\\\"], me.xs)\\n         CHECK(jac[\\\"out1\\\"][\\\"B\\\"], 2 * B * np.ones_like(me.xs))\\n         CHECK(jac[\\\"out2\\\"][\\\"A\\\"], me.xs * 2 * A)\\n         CHECK(jac[\\\"out2\\\"][\\\"B\\\"], me.xs)\\n \\n     def test_vector_prm(self):\\n-        me = DummyMEVectorPrm()\\n-        x = np.asarray(me.parameter_list[\\\"X\\\"])\\n-        jac = me.jacobian()\\n+        x = np.r_[1, 2, 3]\\n+        prm = ParameterList()\\n+        prm.define(\\\"X\\\", x)\\n+        jac = jacobian(dummy_me_vector_prm, prm)\\n \\n         jac_correct = np.concatenate([np.diag(2 * x), np.diag(3 * x ** 2)])\\n         CHECK(jac[\\\"out\\\"][\\\"X\\\"], jac_correct)\\n \\n     def test_vector_prm2(self):\\n-        me = DummyMEVectorPrm2()\\n-        jac = me.jacobian()\\n+        prm = ParameterList()\\n+        prm.define(\\\"X\\\", [0.0, 42.0])\\n+        jac = jacobian(dummy_me_vector_prm2, prm)\\n \\n         jac_correct = np.array([[1, 1]])\\n         CHECK(jac[\\\"out\\\"][\\\"X\\\"], jac_correct)\\n \\n     def test_partial_jacobian_definition(self):\\n+        A, B = 42.0, 0.0\\n         me = DummyMEPartial()\\n-        A, B = me.parameter_list[\\\"A\\\"], me.parameter_list[\\\"B\\\"]\\n-        jac = me.jacobian()\\n+        prm = ParameterList()\\n+        prm.define(\\\"A\\\", A)\\n+        prm.define(\\\"B\\\", B)\\n+        jac = me.jacobian(prm)\\n \\n         CHECK(jac[\\\"out1\\\"][\\\"A\\\"], me.xs)\\n         CHECK(jac[\\\"out1\\\"][\\\"B\\\"], 2 * B * np.ones_like(me.xs))\\n@@ -97,26 +92,29 @@ def test_partial_jacobian_definition(self):\\n \\n class OddEvenME(ModelErrorInterface):\\n     def __init__(self):\\n-        super().__init__()\\n-        self.parameter_list.define(\\\"E_odd\\\", 42.0)\\n-        self.parameter_list.define(\\\"E_even\\\", 4.0)\\n-        self.parameter_list.define(\\\"E_all\\\", 4.0)\\n         self.x_odd = np.r_[0, 2, 0, 2, 0, 2]\\n         self.x_even = np.r_[1, 0, 1, 0, 1, 0]\\n         self.x_all = np.r_[3, 3, 3, 3, 3, 3]\\n \\n-    def __call__(self):\\n+        self.prms = ParameterList()\\n+        self.prms.define(\\\"E_odd\\\", 42.0)\\n+        self.prms.define(\\\"E_even\\\", 4.0)\\n+        self.prms.define(\\\"E_all\\\", 4.0)\\n+\\n+    def __call__(self, latent_prm):\\n+        prm = self.prms.overwrite_with(latent_prm)\\n         return {\\n-            \\\"out\\\": self.x_odd * self.parameter_list[\\\"E_odd\\\"]\\n-            + self.x_even * self.parameter_list[\\\"E_even\\\"]\\n-            + self.x_all * self.parameter_list[\\\"E_all\\\"]\\n+            \\\"out\\\": self.x_odd * prm[\\\"E_odd\\\"]\\n+            + self.x_even * prm[\\\"E_even\\\"]\\n+            + self.x_all * prm[\\\"E_all\\\"]\\n         }\\n \\n \\n class TestJacobianJointGlobal(unittest.TestCase):\\n     def test_individual(self):\\n         me = OddEvenME()\\n-        jac = me.jacobian()\\n+\\n+        jac = me.jacobian(me.prms)\\n         CHECK(jac[\\\"out\\\"][\\\"E_odd\\\"], me.x_odd)\\n         CHECK(jac[\\\"out\\\"][\\\"E_even\\\"], me.x_even)\\n         CHECK(jac[\\\"out\\\"][\\\"E_all\\\"], me.x_all)\\n@@ -124,13 +122,10 @@ def test_individual(self):\\n     def test_three_joints(self):\\n         me = OddEvenME()\\n         p = VariationalBayesProblem()\\n-        p.add_model_error(me)\\n-        p.latent[\\\"E\\\"].add(me.parameter_list, \\\"E_odd\\\")\\n-        p.latent[\\\"E\\\"].add(me.parameter_list, \\\"E_even\\\")\\n-        p.latent[\\\"E\\\"].add(me.parameter_list, \\\"E_all\\\")\\n-\\n-        with self.assertRaises(Exception):\\n-            p.jacobian([42.0])  # we have not defined a noise model yet!\\n+        me_key = p.add_model_error(me)\\n+        p.set_latent_individually(\\\"E\\\", me_key, \\\"E_odd\\\")\\n+        p.set_latent_individually(\\\"E\\\", me_key, \\\"E_even\\\")\\n+        p.set_latent_individually(\\\"E\\\", me_key, \\\"E_all\\\")\\n \\n         noise_key = p.add_noise_model(UncorrelatedSingleNoise())\\n \\n@@ -138,6 +133,8 @@ def test_three_joints(self):\\n         self.assertEqual(J.shape, (6, 1))\\n         CHECK(J[:, 0], me.x_odd + me.x_even + me.x_all)\\n \\n+\\n+\\n     def test_two_joints(self):\\n         \\\"\\\"\\\"\\n         Checks if the joint jacobian is caluclated correctly, if only\\n@@ -145,9 +142,9 @@ def test_two_joints(self):\\n         \\\"\\\"\\\"\\n         me = OddEvenME()\\n         p = VariationalBayesProblem()\\n-        p.add_model_error(me)\\n-        p.latent[\\\"E\\\"].add(me.parameter_list, \\\"E_odd\\\")\\n-        p.latent[\\\"E\\\"].add(me.parameter_list, \\\"E_even\\\")\\n+        me_key = p.add_model_error(me)\\n+        p.set_latent_individually(\\\"E\\\", me_key, \\\"E_odd\\\")\\n+        p.set_latent_individually(\\\"E\\\", me_key, \\\"E_even\\\")\\n         noise_key = p.add_noise_model(UncorrelatedSingleNoise())\\n \\n         J = p.jacobian([42.0])[noise_key]\\n\",\"diff --git a/tests/test_multiple_models.py b/tests/test_multiple_models.py\\nindex faf39f6..1fd524e 100644\\n--- a/tests/test_multiple_models.py\\n+++ b/tests/test_multiple_models.py\\n@@ -50,12 +50,9 @@ def model(prm):\\n class ModelError(ModelErrorInterface):\\n     def __init__(self, fw, data):\\n         self.fw, self.data = fw, data\\n-        self.parameter_list = ParameterList()\\n-        self.parameter_list.define(\\\"A\\\")\\n-        self.parameter_list.define(\\\"B\\\")\\n \\n-    def __call__(self):\\n-        return {\\\"dummy_sensor\\\": self.fw(self.parameter_list) - self.data}\\n+    def __call__(self, latent_prm):\\n+        return {\\\"dummy_sensor\\\": self.fw(latent_prm) - self.data}\\n \\n \\n class Test_VB(unittest.TestCase):\\n@@ -95,12 +92,12 @@ def test_joint_evaluate(self):\\n         # For the inference, we combine them and use a 'key' to distinguish\\n         # e.g. \\\"A\\\" from the one model to \\\"A\\\" from the other one.\\n         problem = VariationalBayesProblem()\\n-        problem.add_model_error(me1)\\n-        problem.add_model_error(me2)\\n+        me_key1 = problem.add_model_error(me1)\\n+        me_key2 = problem.add_model_error(me2)\\n \\n-        problem.latent[\\\"B1\\\"].add(me1.parameter_list, \\\"B\\\")\\n-        problem.latent[\\\"B2\\\"].add(me2.parameter_list, \\\"B\\\")\\n-        problem.define_shared_latent_parameter_by_name(\\\"A\\\")\\n+        problem.set_latent_individually(\\\"B1\\\", me_key1, \\\"B\\\")\\n+        problem.set_latent_individually(\\\"B2\\\", me_key2, \\\"B\\\")\\n+        problem.set_latent(\\\"A\\\")\\n         noise_key = problem.add_noise_model(UncorrelatedSingleNoise())\\n \\n         parameter_vec = np.array([1, 2, 4])\\n@@ -121,10 +118,10 @@ def test_joint(self):\\n         key2 = problem.add_model_error(me2)\\n         print(key1, key2)\\n \\n-        problem.latent[\\\"A1\\\"].add(me1.parameter_list, \\\"A\\\")\\n-        problem.latent[\\\"B1\\\"].add(me1.parameter_list, \\\"B\\\")\\n-        problem.latent[\\\"A2\\\"].add(me2.parameter_list, \\\"A\\\")\\n-        problem.latent[\\\"B2\\\"].add(me2.parameter_list, \\\"B\\\")\\n+        problem.set_latent_individually(\\\"A1\\\", key1, \\\"A\\\")\\n+        problem.set_latent_individually(\\\"B1\\\", key1, \\\"B\\\")\\n+        problem.set_latent_individually(\\\"A2\\\", key2, \\\"A\\\")\\n+        problem.set_latent_individually(\\\"B2\\\", key2, \\\"B\\\")\\n \\n         problem.set_normal_prior(\\\"A1\\\", A1 + 0.5, 2)\\n         problem.set_normal_prior(\\\"B1\\\", B1 + 0.5, 2)\\n\",\"diff --git a/tests/test_parameters.py b/tests/test_parameters.py\\nindex df6c5e3..5e7b4da 100644\\n--- a/tests/test_parameters.py\\n+++ b/tests/test_parameters.py\\n@@ -38,6 +38,24 @@ def test_iterate(self):\\n         for p in self.p.names:\\n             print(p)\\n \\n+    def test_overwrite(self):\\n+        other = ParameterList()\\n+        other.define(\\\"pA\\\", 42)\\n+\\n+        p_new = self.p.overwrite_with(other)\\n+        self.assertEqual(p_new[\\\"pA\\\"], 42)\\n+        self.assertEqual(p_new[\\\"pB\\\"], 0)\\n+\\n+        other2 = ParameterList()\\n+        other2.define(\\\"pC\\\", 42)\\n+        with self.assertRaises(Exception):\\n+            p_new = self.p.overwrite_with(other2)\\n+\\n+    def test_overwrite_one(self):\\n+        p_new = self.p.with_value(\\\"pA\\\", 42)\\n+        self.assertEqual(p_new[\\\"pA\\\"], 42)\\n+        self.assertEqual(p_new[\\\"pB\\\"], 0)\\n+\\n \\n if __name__ == \\\"__main__\\\":\\n     unittest.main()\\n\",\"diff --git a/setup.py b/setup.py\\nindex 0b89768..bae1d80 100644\\n--- a/setup.py\\n+++ b/setup.py\\n@@ -18,7 +18,7 @@\\n         \\\"License :: OSI Approved :: MIT License\\\",\\n         \\\"Operating System :: OS Independent\\\",\\n     ],\\n-    install_requires=[\\\"numpy\\\", \\\"scipy\\\"],\\n+    install_requires=[\\\"numpy\\\", \\\"scipy\\\", \\\"tabulate\\\"],\\n     extras_require={  # Optional\\n         \\\"dev\\\": [\\\"black\\\"],\\n         \\\"test\\\": [\\\"coverage\\\", \\\"pytest\\\", \\\"flake8\\\"],\\n\",\"diff --git a/tests/test_inference_problem.py b/tests/test_inference_problem.py\\nindex 9a4146c..3f651ac 100644\\n--- a/tests/test_inference_problem.py\\n+++ b/tests/test_inference_problem.py\\n@@ -1,5 +1,7 @@\\n import numpy as np\\n import unittest\\n+import scipy.optimize\\n+\\n from bayes.vb import Gamma\\n from bayes.parameters import ParameterList\\n from bayes.noise import UncorrelatedSingleNoise\\n@@ -8,7 +10,7 @@\\n \\n def dummy_model_error(prms):\\n     x = np.r_[1, 2]\\n-    return {\\\"dummy_sensor\\\": x * prms[\\\"B\\\"]}\\n+    return {\\\"dummy_sensor\\\": x * prms[\\\"B\\\"] - 20}\\n \\n \\n class TestProblem(unittest.TestCase):\\n@@ -26,7 +28,7 @@ def test_latent_parameters(self):\\n         p.set_latent_individually(\\\"B\\\", me_key)\\n \\n         me = p.evaluate_model_errors([42])\\n-        self.assertListEqual([42, 84], list(me[me_key][\\\"dummy_sensor\\\"]))\\n+        self.assertListEqual([22, 64], list(me[me_key][\\\"dummy_sensor\\\"]))\\n \\n     def test_shared_latent_evaluate(self):\\n         p = InferenceProblem()\\n@@ -40,6 +42,22 @@ def test_shared_latent_evaluate(self):\\n         for key, me in p.model_errors.items():\\n             self.assertListEqual(list(result[key]), list(dummy_model_error({\\\"B\\\": 42})))\\n \\n+    def test_maximum_likelihood(self):\\n+        p = InferenceProblem()\\n+        p.add_model_error(dummy_model_error)\\n+        p.set_latent(\\\"B\\\")\\n+        noise = UncorrelatedSingleNoise()\\n+        noise.parameter_list[\\\"precision\\\"] = 1.\\n+        p.add_noise_model(noise)\\n+        \\n+        def minus_loglike(x):\\n+            return -p.loglike([x])\\n+\\n+        result = scipy.optimize.minimize_scalar(minus_loglike)\\n+        self.assertTrue(result.success) \\n+        self.assertAlmostEqual(result.x, 12)  # 12. Trust me! :P\\n+\\n+\\n \\n class TestVBProblem(unittest.TestCase):\\n     def test_prior(self):\\n\",\"diff --git a/tests/sampling_example.py b/tests/sampling_example.py\\nindex a7ec0b3..20ca40a 100644\\n--- a/tests/sampling_example.py\\n+++ b/tests/sampling_example.py\\n@@ -29,8 +29,7 @@ def __init__(self, name, position):\\n         self.position = position\\n \\n \\n-class MyForwardModel:\\n-    def __call__(self, parameter_list, sensors, time_steps):\\n+def my_forward_model(parameter_list, sensors, time_steps):\\n         \\\"\\\"\\\"\\n         evaluates \\n             fw(x, t) = A * x + B * t\\n@@ -43,22 +42,15 @@ def __call__(self, parameter_list, sensors, time_steps):\\n             result[sensor] = A * sensor.position + B * time_steps\\n         return result\\n \\n-    def parameter_list(self):\\n-        p = ParameterList()\\n-        p.define(\\\"A\\\", None)\\n-        p.define(\\\"B\\\", None)\\n-        return p\\n-\\n \\n class MyModelError(ModelErrorInterface):\\n     def __init__(self, fw, data):\\n         self._fw = fw\\n         self._ts, self._sensor_data = data\\n-        self.parameter_list = fw.parameter_list()\\n \\n-    def __call__(self):\\n+    def __call__(self, parameter_list):\\n         sensors = list(self._sensor_data.keys())\\n-        model_response = self._fw(self.parameter_list, sensors, self._ts)\\n+        model_response = self._fw(parameter_list, sensors, self._ts)\\n         error = {}\\n         for sensor in sensors:\\n             error[sensor] = model_response[sensor] - self._sensor_data[sensor]\\n@@ -77,14 +69,13 @@ def __call__(self):\\n     # Define the sensor\\n     s1, s2, s3 = MySensor(\\\"S1\\\", 0.2), MySensor(\\\"S2\\\", 0.5), MySensor(\\\"S3\\\", 42.0)\\n \\n-    fw = MyForwardModel()\\n-    prm = fw.parameter_list()\\n+    prm = ParameterList()\\n \\n     # set the correct values\\n     A_correct = 42.0\\n     B_correct = 6174.0\\n-    prm[\\\"A\\\"] = A_correct\\n-    prm[\\\"B\\\"] = B_correct\\n+    prm.define(\\\"A\\\", A_correct)\\n+    prm.define(\\\"B\\\", B_correct)\\n \\n     np.random.seed(6174)\\n     noise_sd1 = 0.2\\n@@ -92,7 +83,7 @@ def __call__(self):\\n \\n     def generate_data(N_time_steps, noise_sd):\\n         time_steps = np.linspace(0, 1, N_time_steps)\\n-        model_response = fw(prm, [s1, s2, s3], time_steps)\\n+        model_response = my_forward_model(prm, [s1, s2, s3], time_steps)\\n         sensor_data = {}\\n         for sensor, perfect_data in model_response.items():\\n             sensor_data[sensor] = perfect_data + np.random.normal(\\n@@ -103,17 +94,17 @@ def generate_data(N_time_steps, noise_sd):\\n     data1 = generate_data(101, noise_sd1)\\n     data2 = generate_data(51, noise_sd2)\\n \\n-    me1 = MyModelError(fw, data1)\\n-    me2 = MyModelError(fw, data2)\\n+    me1 = MyModelError(my_forward_model, data1)\\n+    me2 = MyModelError(my_forward_model, data2)\\n \\n     problem = VariationalBayesProblem()\\n     key1 = problem.add_model_error(me1)\\n     key2 = problem.add_model_error(me2)\\n \\n-    problem.latent[\\\"A\\\"].add(me1.parameter_list, \\\"A\\\")\\n-    problem.latent[\\\"A\\\"].add(me2.parameter_list, \\\"A\\\")\\n+    problem.set_latent_individually(\\\"A\\\", key1, \\\"A\\\")\\n+    problem.set_latent_individually(\\\"A\\\", key2, \\\"A\\\")\\n     # or simply\\n-    problem.define_shared_latent_parameter_by_name(\\\"B\\\")\\n+    problem.set_latent(\\\"B\\\")\\n \\n     problem.set_normal_prior(\\\"A\\\", 40.0, 5.0)\\n     problem.set_normal_prior(\\\"B\\\", 6000.0, 300.0)\\n@@ -147,7 +138,7 @@ def generate_data(N_time_steps, noise_sd):\\n     \\\"\\\"\\\"\\n     for noise_key in problem.noise_prior:\\n         noise_parameters = problem.noise_models[noise_key].parameter_list\\n-        problem.latent[noise_key].add(noise_parameters, \\\"precision\\\")\\n+        problem.set_latent_individually(noise_key, noise_key, \\\"precision\\\")\\n \\n     \\\"\\\"\\\"\\n     2)  Wrap problem.loglike for a tool of your choice\"]", "hints_text": ""}
